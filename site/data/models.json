[
  {
    "id": "GDUT-HWD",
    "title": "Automatic detection of hardhats worn by construction personnel: A deep learning approach and benchmark dataset",
    "authors": ["Jixiu Wu", "Nian Cai", "Wenjie Chen", "Huiheng Wang", "Guotian Wang"],
    "abstract": "Hardhats play an essential role in protecting construction individuals from accidents. However, wearing hardhats is not strictly enforced among workers due to all kinds of reasons. To enhance construction sites safety, the majority of existing works monitor the presence and proper use of hardhats through multi-stage data processing, which come with limitations on adaption and generalizability. In this paper, a one-stage system based on convolutional neural network is proposed to automatically monitor whether construction personnel are wearing hardhats and identify the corresponding colors. To facilitate the study, this work constructs a new and publicly available hardhat wearing detection benchmark dataset, which consists of 3174 images covering various on-site conditions. Then, features from different layers with different scales are fused discriminately by the proposed reverse progressive attention to generate a new feature pyramid, which will be fed into the Single Shot Multibox Detector (SSD) to predict the final detection results. The proposed system is trained by an end-to-end scheme. The experimental results demonstrate that the proposed system is effective under all kinds of on-site conditions, which can achieve 83.89% mAP (mean average precision) with the input size 512×512.",
    "year": 2019,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Apache 2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2019.102894",
    "code_url": "https://github.com/wujixiu/helmet-detection",
    "thumbnail": "assets/img/models/GDUT-HWD.png"
  },
  {
    "id": "Construction-Activity-Scenes",
    "title": "Manifesting construction activity scenes via image captioning",
    "authors": ["Huan Liu", "Guangbin Wang", "Ting Huang", "Ping He", "Martin Skitmore", "Xiaochun Luo"],
    "abstract": "This study proposed an automated method for manifesting construction activity scenes by image captioning – an approach rooted in computer vision and natural language generation. A linguistic description schema for manifesting the scenes is developed initially and two unique dedicated image captioning datasets are created for method validation. A general model architecture of image captioning is then instituted by combining an encoder-decoder framework with deep neural networks, followed by three experimental tests involving the selection of model learning strategies and performance evaluation metrics. It is demonstrated the method's performance is comparable with that of state-of-the-art computer vision methods in general. The paper concludes with a discussion of the feasibility of the practical application of the proposed approach at the current technical level.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103334",
    "code_url": "https://github.com/HannahHuanLIU/AEC-image-captioning",
    "thumbnail": "assets/img/models/Construction-Activity-Scenes.png"
  },
  {
    "id": "Kenki-Posi",
    "title": "Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects",
    "authors": ["Runqiu Bao","Ren Komatsu","Renato Miyagusuku","Masaki Chino", "Atsushi Yamashita", "Hajime Asama"],
    "abstract": "At modern construction sites, utilizing GNSS (Global Navigation Satellite System) to measure the real-time location and orientation (i.e. pose) of construction machines and navigate them is very common. However, GNSS is not always available. Replacing GNSS with on-board cameras and visual simultaneous localization and mapping (visual SLAM) to navigate the machines is a cost-effective solution. Nevertheless, at construction sites, multiple construction machines will usually work together and side-by-side, causing large dynamic occlusions in the cameras' view. Standard visual SLAM cannot handle large dynamic occlusions well. In this work, we propose a motion segmentation method to efficiently extract static parts from crowded dynamic scenes to enable robust tracking of camera ego-motion. Our method utilizes semantic information combined with object-level geometric constraints to quickly detect the static parts of the scene. Then, we perform a two-step coarse-to-fine ego-motion tracking with reference to the static parts. This leads to a novel dynamic visual SLAM formation. We test our proposals through a real implementation based on ORB-SLAM2, and datasets we collected from real construction sites. The results show that when standard visual SLAM fails, our method can still retain accurate camera ego-motion tracking in real-time. Comparing to state-of-the-art dynamic visual SLAM methods, ours shows outstanding efficiency and competitive result trajectory accuracy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1080/01691864.2020.1869586",
    "code_url": "https://github.com/RunqiuBao/kenki-positioning-vSLAM",
    "thumbnail": "assets/img/models/Kenki-Posi.gif"
  },
  {
    "id": "CDE",
    "title": "Machine learning using synthetic images for detecting dust emissions on construction sites",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Automated dust monitoring in workplaces helps provide timely alerts to over-exposed workers and effective mitigation measures for proactive dust control. However, the cluttered nature of construction sites poses a practical challenge to obtain enough high-quality images in the real world. The study aims to establish a framework that overcomes the challenges of lacking sufficient imagery data (“data-hungry problem”) for training computer vision algorithms to monitor construction dust. This study develops a synthetic image generation method that incorporates virtual environments of construction dust for producing training samples. Three state-of-the-art object detection algorithms, including Faster-RCNN, you only look once (YOLO) and single shot detection (SSD), are trained using solely synthetic images. Finally, this research provides a comparative analysis of object detection algorithms for real-world dust monitoring regarding the accuracy and computational efficiency. This study creates a construction dust emission (CDE) dataset consisting of 3,860 synthetic dust images as the training dataset and 1,015 real-world images as the testing dataset. The YOLO-v3 model achieves the best performance with a 0.93 F1 score and 31.44 fps among all three object detection models. The experimental results indicate that training dust detection algorithms with only synthetic images can achieve acceptable performance on real-world images. This study provides insights into two questions: (1) how synthetic images could help train dust detection models to overcome data-hungry problems and (2) how well state-of-the-art deep learning algorithms can detect nonrigid construction dust.",
    "year": 2021,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Compliance checking"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1108/SASBE-04-2021-0066",
    "code_url": "https://github.com/ruoxinx/site-dust-detect",
    "thumbnail": "assets/img/models/CDE.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "CPPE",
    "title": "Pose guided anchoring for detecting proper use of personal protective equipment",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Ensuring proper use of personal protective equipment (PPE) is essential for improving workplace safety management. The authors present an extensible pose-guided anchoring framework aimed at multi-class PPE compliance detection. The overall approach harnesses a pose estimator to detect worker body parts as spatial anchors and guide the localization of part attention regions using body-knowledge-based rules considering workers' orientations and object scales. Specifically, “part attention regions” are local image patches expecting PPEs based on their inherent relationships with body parts, e.g., (head, hardhat) and (upper-body, vest). Finally, the shallow CNN-based classifiers can reliably recognize both PPE and non-PPE classes within their corresponding part attention regions. Quantitative evaluations tested on the developed construction personal protective equipment dataset (CPPE) show an overall 0.97 and 0.95 F1-score for hardhat and safety vest detection, respectively. Comparative studies with existing methods also demonstrate the higher detection accuracy and advantageous extensibility of the proposed strategy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2021.103828",
    "code_url": "https://github.com/ruoxinx/PPE-Detection-Pose",
    "thumbnail": "assets/img/models/CPPE.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
   {
    "id": "ConSLAM",
    "title": ["ConSLAM: Construction Dataset for SLAM"],
    "authors": ["Maciej Trzeciak", "Kacper Pluta", "Yasmin Fathy", "Lucio Alcalde", "Stanley Chee", "Antony Bromley", "Ioannis Brilakis", "Pierre Alliez"],
    "abstract": "This paper presents a data set collected periodically on a construction site. The data set aims to evaluate the performance of simultaneous localization and mapping (SLAM) algorithms used by mobile scanners or autonomous robots. It includes ground-truth scans of a construction site collected using a terrestrial laser scanner along with five sequences of spatially registered and time-synchronized images, lidar scans, and inertial data coming from our prototypical handheld scanner. We also recover the ground-truth trajectory of the mobile scanner by registering the sequential lidar scans to the ground-truth scans and show how to use a popular software package to measure the accuracy of SLAM algorithms against our trajectory automatically. To the best of our knowledge, this is the first publicly accessible data set consisting of periodically collected sequential data on a construction site.",
    "year": 2023,
    "modalities": ["Ground RGB, Point Cloud"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Academic Use Only (University of Cambridge)",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-5212",
    "code_url": "https://github.com/mac137/ConSLAM",
    "thumbnail": "assets/img/models/ConSLAM.png",
	"contributor": "CIT Lab",
	"contributor_url": "https://cit.eng.cam.ac.uk/about-cit"
  },
  {
    "id": "CMA",
    "title": "Transformer-based deep learning model and video dataset for unsafe action identification in construction projects",
    "authors": ["Meng Yang", "Chengke Wu", "Yuanjun Guo", "Rui Jiang", "Feixiang Zhou", "Jianlin Zhang", "Zhile Yang"],
    "abstract": "A large proportion of construction accidents are caused by unintentional and unsafe actions and behaviors. It is of significant difficulties and ineffectiveness to monitor unsafe behaviors using conventional manual supervision due to the complex and dynamic working conditions on construction sites. Recently, surveillance videos and computer vision (CV) techniques have been increasingly adopted to automatically identify risky behaviors. However, the challenge remains that spatial and temporal features in video clips cannot be effectively captured and fused by current CV models. To address this challenge, this paper describes a deep learning model named Spatial Temporal Relation Transformer (STR-Transformer), where spatial and temporal features of work behaviors are simultaneously extracted in paralleling video streams and then fused by a specially designed module. To verify the effectiveness of the STR-Transformer, a customized dataset is developed, including seven categories of construction worker behaviors and 1595 video clips. In numerical experiments and case studies, the STR-Transformer achieves an average precision of 88.7%, 4.0% higher than the baseline model. The STR-Transformer enables more accurate and reliable automatic safety surveillance on construction projects, and is expected to reduce accident rates and management costs. Moreover, the performance of STR-Transformer relies on efficient feature integration, which may inspire future studies to identify, extract, and fuse richer features when applying CV-based deep learning models in construction management.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Action recognition"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104703",
    "code_url": "https://github.com/S1mpleyang/ConstructionActionRecognition",
    "thumbnail": "assets/img/models/CMA.png"
  },
  {
    "id": "VisualSiteDiary",
    "title": "VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals",
    "authors": ["Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Mani Golparvar-Fard"],
    "abstract": "This paper presents VisualSiteDiary, a Vision Transformer-based image captioning model which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. As a model for deciphering construction photologs, VisualSiteDiary incorporates pseudo-region features, utilizes high-level knowledge in pretraining, and fine-tunes for diverse captioning styles. To validate VisualSiteDiary, a new image captioning dataset, VSD, is presented. This dataset includes many realistic yet challenging cases commonly observed in commercial building projects. Experimental results using five different metrics demonstrate that VisualSiteDiary provides superior-quality captions compared to the state-of-the-art image captioning models. Excluding the task of object recognition, the presented model also outperformed mPLUG –the state-of-the-art visual-language model– in the image retrieval task by 0.6% in precision and 0.9% in recall, respectively. Detailed discussions illustrate practical examples on how VisualSiteDiary improves the process of creating daily construction reports, paving the way for future developments in the field.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105483",
    "code_url": "https://github.com/joonv2/VisualSiteDiary",
    "thumbnail": "assets/img/models/VisualSiteDiary.gif"
  },
  {
    "id": "Small-Construction-Tools",
    "title": "Utilizing synthetic images to enhance the automated recognition of small-sized construction tools",
    "authors": ["Soeun Han", "Wonjun Park", "Kyumin Jeong", "Taehoon Hong", "Choongwan Koo"],
    "abstract": "Previous studies on vision-based classifiers often overlooked the need for detecting small-sized construction tools. Considering the substantial variations in these tools' size and shape, it is essential to train models using synthetic images that encompass diverse angles and distances. This study aimed to improve the performance of classifiers for small-sized construction tools by leveraging synthetic data. Three classifiers were proposed using YOLOv8 algorithm, varying in data composition: (i) ‘Real-4000’: 4000 authentic images; (ii) ‘Hybrid-4000’: 2000 authentic and 2000 synthetic images; (iii) ‘Hybrid-8000’: 4000 authentic and 4000 synthetic images. To assess practical applicability, a test dataset of 144 samples for each type was collected directly from construction sites. Results revealed that the ‘Hybrid-8000’ model, utilizing synthetic images, excelled at 94.8% of mAP_0.5. This represented a significant 15.2% improvement, affirming its practical applicability. These classifiers hold promise for enhancing safety and advancing real-time automation and robotics in construction.",
    "year": 2024,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105415",
    "code_url": "https://github.com/SenseableSpace/Detection-Small-Construction-Tools-Synthetic-Images",
    "thumbnail": "assets/img/models/Small-Construction-Tools.png",
	"contributor": "Choongwan Koo",
    "contributor_url": "https://github.com/SenseableSpace/Detection-Small-Construction-Tools-Synthetic-Images"
  },
  {
    "id": "OSHA-safety",
    "title": "Deep Learning Enabled Computer Vision Model for Automated Safety Compliance in Construction Environments",
    "authors": ["Amr A. Mohy", "Hesham A. Bassioni", "Elbadr O. Elgendi", "Tarek M. Hassan"],
    "abstract": "Construction site safety demands proactive hazard detection, a challenge traditionally met with reactive measures that are often inadequate. This paper introduces a novel deep learning-based computer vision model designed for automated safety compliance monitoring, addressing critical limitations of existing approaches. The model utilizes a modified one-stage object detection algorithm, uniquely enhanced with Contextual Transformer Networks (CoTs), a Triplet Attention module, Activate or Not (ACON) activation functions, and Content-Aware Reassembly of Features (CARAFE) up-sampling, to significantly improve feature extraction, visual recognition, and contextual understanding in complex construction environments. To support this model development, a new OSHA-data-driven dataset of 55,594 images across 28 safety categories was developed. This dataset encompasses personal protective equipment (PPE), scaffolding, materials, hazards, and worker actions, ensuring comprehensive coverage of key safety domains. The Wise-Intersection over Union (IoU) loss function further refines bounding box regression, enhancing localization accuracy. Evaluations on both a benchmarking dataset and the newly developed dataset demonstrate the model's benchmark-surpassing performance (Precision: 0.89, mAP95: 0.45). This research offers a practically viable, data-driven solution for a critical industry challenge, moving towards a future of zero-accident construction sites.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "http://dx.doi.org/10.36680/j.itcon.2025.057",
    "code_url": "https://github.com/amr21006/PhD_Safety_Management_Object_Detection",
    "thumbnail": "assets/img/models/OSHA-safety.png"
  },
    {
    "id": "repetitive-action",
    "title": "Automatic repetitive action counting for construction worker ergonomic assessment",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Work-related musculoskeletal disorders are the primary cause of nonfatal occupational injuries in the construction industry. Accurate ergonomic assessment is essential to reduce the risk of work-related injuries. Repetitive work significantly contributes to musculoskeletal injuries, and various ergonomic evaluation methods have specific criteria for assessing repetitive actions. However, most existing methods for repetitive motions primarily rely on subjective and time-consuming manual observation. To accurately assess ergonomic risk, an automatic and precise method is required to count repetitive actions in construction work. This poses a challenge due to the unstructured nature of construction actions and their varying frequencies and cycles. This paper aims to overcome these challenges by identifying repetitive unstructured actions using posture self-similarity comparison and predicting construction actions' length using a transformer layer. Experimental results demonstrated that the proposed method achieved a 91.5 % accuracy in identifying repetitive actions. The research results will contribute to the promotion of accurate ergonomics evaluation of automation.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Pose estimation"],
	"applications": ["Ergonomic assessment"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105726",
    "code_url": "https://github.com/Chenxy875/Repetitive-Actions-Counting-Method-for-Construction-Workers-Ergonomic-Assessments",
    "thumbnail": "assets/img/models/repetitive-action.gif",
	"contributor": "yantaolab",
    "contributor_url": "https://github.com/yantaolab"
  },
  {
    "id": "Pictor-v3",
    "title": "Deep Learning for Site Safety: Real-Time Detection of Personal Protective Equipment",
    "authors": ["Nipun D. Nath", "Amir H. Behzadan", "Stephanie G. Paal"],
    "abstract": "The leading causes of construction fatalities include traumatic brain injuries (resulted from fall and electrocution) and collisions (resulted from struck by objects). As a preventive step, the U.S. Occupational Safety and Health Administration (OSHA) requires that contractors enforce and monitor appropriate usage of personal protective equipment (PPE) of workers (e.g., hard hat and vest) at all times. This paper presents three deep learning (DL) models built on You-Only-Look-Once (YOLO) architecture to verify PPE compliance of workers; i.e., if a worker is wearing hard hat, vest, or both, from image/video in real-time. In the first approach, the algorithm detects workers, hats, and vests and then, a machine learning model (e.g., neural network and decision tree) verifies if each detected worker is properly wearing hat or vest. In the second approach, the algorithm simultaneously detects individual workers and verifies PPE compliance with a single convolutional neural network (CNN) framework. In the third approach, the algorithm first detects only the workers in the input image which are then cropped and classified by CNN-based classifiers (i.e., VGG-16, ResNet-50, and Xception) according to the presence of PPE attire. All models are trained on an in-house image dataset that is created using crowd-sourcing and web-mining. The dataset, named Pictor-v3, contains ~1,500 annotated images and ~4,700 instances of workers wearing various combinations of PPE components. It is found that the second approach achieves the best performance, i.e., 72.3% mean average precision (mAP), in real-world settings, and can process 11 frames per second (FPS) on a laptop computer which makes it suitable for real-time detection, as well as a good candidate for running on light-weight mobile devices. The closest alternative in terms of performance (67.93% mAP) is the third approach where VGG-16, ResNet-50, and Xception classifiers are assembled in a Bayesian framework. However, the first approach is the fastest among all and can process 13 FPS with 63.1% mAP. The crowed-sourced Pictor-v3 dataset and all trained models are publicly available to support the design and testing of other innovative applications for monitoring safety compliance, and advancing future research in automation in construction.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103085",
    "code_url": "https://github.com/ciber-lab/pictor-ppe",
    "thumbnail": "assets/img/models/Pictor-v3.gif",
	"contributor": "ciber-lab",
    "contributor_url": "https://github.com/ciber-lab"
  },
    {
    "id": "ABECIS",
    "title": "Developing a Free and Open-Source Semi-Automated Building Exterior Crack Inspection Software for Construction and Facility Managers",
    "authors": ["Pi Ko", "Samuel A. Prieto", "Borja García de Soto"],
	"abstract":"Inspection of cracks is an important process for properly monitoring and maintaining a building. However, manual crack inspection is time-consuming, inconsistent, and dangerous (e.g., in tall buildings). Due to the development of open-source AI technologies, the increase in available Unmanned Aerial Vehicles (UAVs) and the availability of smartphone cameras, it has become possible to automate the building crack inspection process. This study presents the development of an easy-to-use, free and open-source Automated Building Exterior Crack Inspection Software (ABECIS) for construction and facility managers, using state-of-the-art segmentation algorithms to identify concrete cracks and generate a quantitative and qualitative report. ABECIS was tested using images collected from a UAV and smartphone cameras in real-world conditions and a controlled laboratory environment. From the raw output of the algorithm, the median Intersection over Unions (IoU) for the test experiments are (1) 0.686 for indoor crack detection experiment in a controlled lab environment using a commercial drone, (2) 0.186 for indoor crack detection at a construction site using a smartphone and (3) 0.958 for outdoor crack detection on university campus using a commercial drone. These IoU results can be improved significantly to over 0.8 when a human operator selectively removes the false positives. In general, ABECIS performs best for outdoor drone images, and combining the algorithm predictions with human verification/intervention offers very accurate crack detection results. The software is available publicly and can be downloaded for out-of-the-box use.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection","Semantic segmentation"],
	"applications": ["Quality control"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1109/ACCESS.2023.3296793",
    "code_url": "https://github.com/SMART-NYUAD/ABECIS",
    "thumbnail": "assets/img/models/ABECIS.png"
  },
    {
    "id": "MGCAR",
    "title": "Multi-granular crew activity recognition for construction monitoring",
    "authors": ["Cheng Yun Tsai", "Mik Wanul Khosiin", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "The labor force is vital to construction projects, but traditional manual methods for productivity analysis are time-consuming and error-prone. Recent advancements in computer vision and deep learning offer automated solutions, yet most studies focus on low-level pose recognition, neglecting the collaborative dynamics of construction sites. This paper introduces a multi-granular crew activity recognition framework that identifies individual actions, groups collaborating workers, and links them to specific tasks. Using graph-based representations and self-attention mechanisms, the model integrates spatial and contextual information for accurate recognition. Experiments on a dataset covering rebar, formwork, and concrete operations show an overall F1 Score of 70.31%. Results highlight the importance of balancing visual features and spatial proximity for optimal performance. This framework offers an efficient solution for construction site monitoring and lays groundwork for future research on temporal modeling and human-object interaction analysis.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Crew activity recognition"],
	"applications": ["Productivity monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106428",
    "code_url": "https://github.com/JimmyTsai-Yun/MGCAR",
    "thumbnail": "assets/img/models/MGCAR.png",
	"contributor": "Construction API Lab",
    "contributor_url": "https://con-api.team/"
  },
  {
    "id": "AID-AI-Infraction-Detection",
    "title": "Exploring a Multimodal Conversational Agent for Construction Site Safety: A Low-Code Approach to Hazard Detection and Compliance Assessment",
    "authors": ["Giancarlo de Marco", "Elias Niederwieser", "Dietmar Siegele"],
    "abstract": "This paper discusses the viability of using a low-code multimodal large language model agent with computer vision functionality to support occupational safety and health evaluations on construction sites. The central hypothesis aims to verify that these systems can provide reliable answers, as evaluated against a ground truth review, including the identification of high-risk dangers. A conversational agent was given the task of finding hazards and checking for national legislative compliance within a dataset of 100 real-world construction photos. The comparison of the agent’s results to the ground truth provides insight into current limitations. The primary issues identified were inconsistent taxonomies, inadequate causal reasoning, and insufficient contextual consideration, all of which adversely impacted performance—particularly when analyzing low-resolution images. The metrics supporting the conclusion synthesize that this tool is a valuable augmentation technology, enhancing safety evaluations while still requiring human supervision to ensure reliability.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Vision–language reasoning"],
	"applications": ["Safety monitoring"],
    "license": "FI-NCAL",
    "paper_url": "https://doi.org/10.3390/buildings15183352",
    "code_url": "https://github.com/fraunhofer-italia/AID-AI-Infraction-Detection",
    "thumbnail": "assets/img/models/AID-AI-Infraction-Detection.png"
  },
    {
    "id": "ETHcavation",
    "title": "ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments",
    "authors": ["Lorenzo Terenzi", "Julian Nubert", "Pol Eyschen", "Pascal Roth", "Simin Fei", "Edo Jelavic", "Marco Hutter"],
	"abstract": "Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset and code for training and deployment are publicly available to support future research.",
    "year": 2025,
    "modalities": ["Ground RGB, 3D Point Cloud"],
    "tasks": ["Semantic segmentation", "Object tracking"],
	"applications": ["Site mapping and navigation"],
    "license": "Unspecified",
    "paper_url": "https://arxiv.org/abs/2410.04250",
    "code_url": "https://github.com/leggedrobotics/rsl_panoptic_mapping",
    "thumbnail": "assets/img/models/ETHcavation.gif",
	"added_date": "2025-09-28",
	"contributor": "Robotic Systems Lab",
    "contributor_url": "https://rsl.ethz.ch/"
  },
  {
    "id": "Buildee",
    "title": "Buildee: A 3D Simulation Framework for Scene Exploration and Reconstruction with Understanding",
    "authors": ["Clementin Boittiaux", "Vincent Lepetit"],
    "abstract": "We introduce Buildee, a 3D simulation framework designed to benchmark scene exploration, 3D reconstruction, and semantic segmentation tasks in both static and dynamic environments. Built as a Python module on top of Blender, Buildee leverages its advanced rendering capabilities to generate realistic RGB, depth, and semantic data while enabling 2D / 3D point tracking and occlusion checking. Additionally, we provide a procedural generator for construction site environments and baseline methods for key computer vision tasks. Through Buildee, we establish a standardized platform for evaluating scene understanding algorithms in realistic settings. Our code is publicly available at https://github.com/clementinboittiaux/buildee.",
    "year": 2025,
    "modalities": ["Ground RGB, depth, synthetic"],
    "tasks": ["Scene generation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://openreview.net/pdf?id=1LmsiOaMTy",
    "code_url": "https://github.com/clementinboittiaux/buildee",
    "thumbnail": "assets/img/models/Buildee.gif",
	"added_date": "2025-09-28",
  "contributor": "Laboratoire d'Informatique Gaspard-Monge",
    "contributor_url": "https://ligm.univ-eiffel.fr/"
  },
    {
    "id": "Open",
    "title": "Facilitating Construction Scene Understanding Knowledge Sharing and Reuse via Lifelong Site Object Detection",
    "authors": ["Ruoxin Xiong", "Yuansheng Zhu", "Yanyu Wang", "Pengkun Liu", "Pingbo Tang"],
	"abstract": "Automatically recognizing diverse construction resources (e.g., workers and equipment) from construction scenes supports efficient and intelligent workplace management. Previous studies have focused on identifying fixed object categories in specific contexts, but they have difficulties in accumulating existing knowledge while extending the model for handling additional classes in changing applications. This work proposes a novel lifelong construction resource detection framework for continuously learning from dynamic changing contexts without catastrophically forgetting previous knowledge. In particular, we contribute: (1) an OpenConstruction Dataset with 31 unique object categories, integrating three large datasets for validating lifelong object detection algorithms; (2) an OpenConstruction Taxonomy, unifying heterogeneous label space from various scenarios; and (3) an informativeness-based lifelong object detector that leverages very limited examples from previous learning tasks and adds new data progressively. We train and evaluate the proposed method on the OpenConstruction Dataset in sequential data streams and show mAP improvements on the overall task. Code is available at https://github.com/YUZ128pitt/OpenConstruction.",
    "year": 2022,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1007/978-3-031-25082-8_15",
    "code_url": "https://github.com/YUZ128pitt/OpenConstruction",
    "thumbnail": "assets/img/models/Open.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "Worker-Safety-Twin",
    "title": "Advancing construction site workforce safety monitoring through BIM and computer vision integration",
    "authors": ["Almo Senja Kulinan", "Minsoo Park", "Pa Pa Win Aung", "Gichun Cha", "Seunghee Park"],
    "abstract": "Ensuring a safe work environment is crucial for construction projects. It is essential that workforce monitoring is both efficient and non-intrusive to the ongoing construction activities. This paper introduces a method that integrates building information modeling (BIM) and computer vision to monitor workforce safety hazards at construction sites in real time. Despite the rising adoption of BIM and computer vision individually within the construction sector, the potential of their integrated application as a cohesive system for workforce safety monitoring remains unexplored. While BIM provides rich 3D semantic information about the construction site, computer vision captures real-time field data. The system was tested using a realistic construction simulation, and the accuracy of the position estimate was evaluated in a real-world interior environment, yielding a mean error distance (MED) of 13.2 cm. Overall, the findings have substantial significance for the construction industry to help minimize accidents and enhance overall worker safety.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.105227",
    "code_url": "https://github.com/almosenja/Worker-Safety-Twin",
    "thumbnail": "assets/img/models/Worker-Safety-Twin.gif"
  },
  {
    "id": "compliance-checking",
    "title": "Automated rule-based safety inspection and compliance checking of temporary guardrail systems in construction",
    "authors": ["K.W. Johansen", "J. Teizer", "C. Schultz"],
    "abstract": "The construction industry records more hazards compared to any other sector. Protective equipment, such as guardrail systems, is essential for protecting workers from deadly falls but may quickly become incompliant after installation. Yet, many construction projects do not have the resources to dedicate personnel to perform the inspection as frequently as needed. Therefore, this paper proposes an automated rule-based inspection and compliance-checking system that can assist the responsible personnel in detecting faulty guardrails in live work environments. The classification approach utilizes safety design and mimics the steps of human guardrailing compliance assessment, which enforces simplicity and transparency, allowing the human domain expert to remain in control. Even under scarce data availability, this first-of-a-kind classification approach is reliable and scalable and successfully classifies 21 predefined and 9 validation scenarios of guardrail systems for fall protection.",
    "year": 2024,
    "modalities": ["Aerial RGB"],
    "tasks": ["Knowledge reasoning"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105849",
    "code_url": "https://github.com/kakke14/Automated-inspection-and-compliance-checking",
    "thumbnail": "assets/img/models/compliance-checking.png",
	"contributor": "Construction Automation and Information Technologies Lab",
	"contributor_url": "https://teizer.com/team/"
  },
  {
    "id": "URRE-Net",
    "title": "An unsupervised low-light image enhancement method for improving V-SLAM localization in uneven low-light construction sites",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Construction robots have been increasingly adopted in construction projects to improve productivity, reduce risk, and speed up work cycles. Visual Simultaneous Localization and Mapping (V-SLAM) technology is widely used in construction robots due to its lightweight weight, cost-effectiveness, and provision of semantic information. On real construction sites, lighting conditions are often challenging due to factors such as uneven lighting intensity, inadequate exposure, or robots in backlit positions (e.g. roughcast houses without artificial lighting, underground car parks), which cause images captured under these conditions to exhibit low signal-to-noise ratio, uneven lighting, color distortion, noise, and other problems. These challenges make the extraction and matching of feature points in V-SLAM difficult, resulting in significant positioning errors or the inability to generate positioning trajectories for construction robotics. Although existing methods for enhancing low-light images can certainly improve image brightness, they still cannot effectively address the localization challenges brought about by low-light construction scenes with uneven illumination and noise. To address the interference of uneven and changing lighting conditions on V-SLAM localization in construction sites, this paper proposes the Unsupervised Reflectance Retinex and Noise model (URRN-Net) to enhance low-light construction images. By using a CNN model based on the Retinex theory to decompose the illumination characteristics of images, we achieve effective brightness restoration of uneven low-light images by utilizing unsupervised loss functions. URRE-Net can reduce the root mean square localization error by >65% compared to low-light images in the ORB-SLAM3 method, and the maximum error is reduced by >71% in on-site experiments. The proposed URRE-Net can be integrated with existing V-SLAM algorithms to provide more robust localization services for low-light construction site applications such as building operations (e.g., interior wall spraying robotic) or construction management tasks (e.g., automatic tunnel inspection).",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105404",
    "code_url": "https://github.com/Chenxy875/URRE-Net",
    "thumbnail": "assets/img/models/URRE-Net.gif",
	"contributor": "yantaolab",
    "contributor_url": "https://github.com/yantaolab"
  },
  {
    "id": "LMs_ConstructionSafety",
    "title": "Performance comparison of retrieval-augmented generation and fine-tuned large language models for construction safety management knowledge retrieval",
    "authors": ["Jungwon Lee", "Seungjun Ahn", "Daeho Kim", "Dongkyun Kim"],
    "abstract": "Construction safety standards are in unstructured formats like text and images, complicating their effective use in daily tasks. This paper compares the performance of Retrieval-Augmented Generation (RAG) and fine-tuned Large Language Model (LLM) for the construction safety knowledge retrieval. The RAG model was created by integrating GPT-4 with a knowledge graph derived from construction safety guidelines, while the fine-tuned LLM was fine-tuned using a question-answering dataset derived from the same guidelines. These models' performance is tested through case studies, using accident synopses as a query to generate preventive measurements. The responses were assessed using metrics, including cosine similarity, Euclidean distance, BLEU, and ROUGE scores. It was found that both models outperformed GPT-4, with the RAG model improving by 21.5 % and the fine-tuned LLM by 26 %. The findings highlight the relative strengths and weaknesses of the RAG and fine-tuned LLM approaches in terms of applicability and reliability for safety management.",
    "year": 2024,
    "modalities": ["Document"],
    "tasks": ["Knowledge reasoning"],
	"applications": ["Compliance checking"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105846",
    "code_url": "https://github.com/juuuungwon/LMs_ConstructionSafety",
    "thumbnail": "assets/img/models/LMs_ConstructionSafety.png"
  },
  {
    "id": "ego",
    "title": "Egocentric camera-based method for detecting static hazardous objects on construction sites",
    "authors": ["Ziming Liu", "Jiuyi Xu", "Christine Wun Ki Suen", "Meida Chen", "Zhengbo Zou", "Yangming Shi"],
    "abstract": "The construction site is a hazardous workplace, accounting for more than 20 % of worker fatalities compared to other industries in the United States. Predominant causes of these fatalities are slips, trips, and falls (STFs). Therefore, identifying hazardous objects on construction sites that could lead to STFs is crucial for enhancing construction safety. Previous studies using fixed-position cameras often miss observations of obstructed or hidden objects. This paper proposes an alternative approach using safety helmets with lightweight wide-angle cameras and leveraging open-vocabulary object detection (OVOD) methods to identify hazardous objects on construction sites that could lead to STFs. In addition, an egocentric view dataset specifically for construction sites was created and released for benchmarking purposes. Research results indicated a 79.0 % weighted F1-score in classifying static hazardous objects on construction sites. This proposed system has the potential to enhance construction safety and provide a valuable dataset for future construction safety research.",
    "year": 2025,
    "modalities": ["Egocentric RGB"],
    "tasks": ["object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106048",
    "code_url": "https://github.com/JiuyiX/Tripping-Hazard-Detection-based-on-OVOD",
    "thumbnail": "assets/img/models/ego.png",
	"contributor": "Yangming Research Group",
    "contributor_url": "https://shiyangming.com/"
  },
  {
    "id": "YOLO-FAS",
    "title": "YOLO-FAS: A lightweight model for detecting rebar intersections location and tying status",
    "authors": ["Hao Duan", "Mingming Yu", "Tengfeng Ai", "Mengmeng Zhu", "Haili Jiang", "Shuai Guo"],
    "abstract": "Current deep learning (DL) algorithms for detecting complex rebar mesh intersection points rely heavily on large amounts of training data to ensure recognition accuracy and generalization capabilities. However, deploying such algorithms on small mobile robots is challenging due to limited computational resources, making it difficult to achieve both real-time performance and high precision. To address this issue, this paper proposes a lightweight YOLO-FAS model specifically designed for rebar intersections detection on tying robots. This model is based on YOLOv5s and optimizes the FastNet structure by introducing the PGConv module, enhances feature fusion with the AFPN (Asymptotic feature pyramid network) module, and improves localization accuracy using the EIOU loss function. Experimental results demonstrate that the model's parameters and computational load are reduced by 60.6% and 62%, respectively. Furthermore, through BN (Batch Normalization) channel pruning and QAT (Quantization Aware Training), the model is compressed, and inference is accelerated using TensorRT. The improved YOLO-FAS model achieves a FPS (frames per second) increase of 84.1% in FP32 mode and 33.2% in INT8 mode. Finally, after real-world deployment testing, the system's average memory usage is reduced by 0.77GB, and the accuracy of recognition of intersection points reaches 98.21%, representing an improvement of 3.04% over YOLOv5s. The results indicate that this method effectively achieves model lightweighting while ensuring efficient and accurate detection of rebar intersection points, demonstrating robust performance and promising application prospects.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "GNU 3.0",
    "paper_url": "https://doi.org/10.1016/j.neucom.2025.129485",
    "code_url": "https://github.com/5204338/yolov5_node",
    "thumbnail": "assets/img/models/YOLO-FAS.gif"
  },
  {
    "id": "MTKF",
    "title": "A unified object and keypoint detection framework for Personal Protective Equipment use",
    "authors": ["Bin Yang", "Hongru Xiao", "Binghan Zhang"],
    "abstract": "Accurately detecting whether workers wear Personal Protective Equipment (PPE) in real time plays an important role in safety management. Previous studies mainly used multiple models jointly or only object detection for wearing relationship judgments. This makes it difficult to provide real-time, accurate detection of security relationships. Therefore, this paper proposes safe-wearing detection rules and a novel multi-targets and keypoints detection framework (MTKF), which is capable of accomplishing multiple classes of targets and keypoints detection simultaneously in one-stage, to get more accurate results. In order to improve the performance in the PPE and worker keypoints detection in challenging construction scenes, the detection head transformation strategy, mix group shuffle attention (MGSA) module, and the improved dual and cross-class suppression algorithm (DC-NMS) are proposed. The experimental results are implemented on one established dataset (Joint dataset) and two public datasets (SHWD and COCO), which conduct a comprehensive evaluation in multiple dimensions. Compared to the baseline model, our method improves the mAP by 2.6%–7.1%, reduces the number of parameters by at least 70%, and is able to achieve an inference speed of 155 fps.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU 3.0",
    "paper_url": "https://doi.org/10.1016/j.dibe.2024.100559",
    "code_url": "https://github.com/TJSG337/MTKF",
    "thumbnail": "assets/img/models/MTKF.png"
  },
  {
    "id": "SOC-YOLO",
    "title": "Small and overlapping worker detection at construction sites",
    "authors": ["Minsoo Park", "Dai Quoc Tran", "Jinyeong Bak", "Seunghee Park"],
    "abstract": "Although there has been study on worker detection using computer vision (CV) for the safety of construction sites, it is still challenging to identify employees who are obstructed or have poor vision. To solve these problems, we propose a method of small and overlapping target (worker) detection at a complex construction site named SOC-YOLO. The method is based on YOLOv5 and utilizes distance intersection over union (DIoU) non-maximum suppression (NMS), incorporating weighted triplet attention, expansion feature-level, and Soft-pool. Workers can be captured with overlap, particularly in large-scale construction sites, using the DIoU-based loss function, and NMS contributed to accuracy improvement. Next, we propose a weighted-triplet attention mechanism that can extract feature information from space more effectively and channel attention when learning object detection networks, using a simple average approach based on the same weight between the existing triplet attention. Next, we propose a model that adds additional predictive heads and residual connections to address the poor detection accuracy of workers photographed over long distances. A low-level feature map containing more information regarding small targets is used by extending the feature level. Finally, Softpool-spatial pyramid pooling fast (Softpool-SPPF) is proposed to solve the problem of inconsistent input image sizes. Softpool-SPPF performs an spatial pyramid pooling (SPP) function while preserving more functional information for accurate small target detection. Experiments were conducted using published worker detection datasets and handmade datasets, and the results showed increase from 81.26% to 84.63% average precision (AP) for small objects, from 67.52% to 73.88% mAP for minute objects, from 74.56% to77.57% for overlapping objects. The proposed method is expected to be useful for safety monitoring by applying it to the construction site worker tracking model.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU 3.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.104856",
    "code_url": "https://github.com/pms5343/SOC-YOLO",
    "thumbnail": "assets/img/models/SOC-YOLO.png"
  },
  {
    "id": "SORD",
    "title": "Deep learning without human labeling for on-site rebar instance segmentation using synthetic BIM data and domain adaptation",
    "authors": ["Tsung-Wei Huang", "Yi-Hsiang Chen", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "On-site rebar inspection is crucial for structural safety but remains labor-intensive and time-consuming. While deep learning presents a promising solution, existing research often relies on limited real-world labeled data. This paper introduces a framework to train a deep learning model for on-site rebar instance segmentation without human labeling. Synthetic data are generated from BIM models, creating a Synthetic On-site Rebar Dataset (SORD) with 25,287 labeled images. Domain adaptation is incorporated to bridge the gap between synthetic and real-world non-labeled data. This approach eliminates the need for human labeling. It significantly enhances model performance, achieving a threefold improvement in Average Precision (AP) metrics compared to models trained on limited real-world data. Additionally, the proposed method demonstrates superior performance across various on-site rebar images collected online, underscoring its generalizability and practical applications.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Semantic segmentation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105953",
    "code_url": "https://github.com/HuangBugWei/DA-MaskRCNN",
    "thumbnail": "assets/img/models/SORD.png",
	"contributor": "Construction API Lab",
    "contributor_url": "https://con-api.team/"
  },
  {
    "id": "FLE-YOLO",
    "title": "FLE-YOLO: A Faster, Lighter, and More Efficient Strategy for Autonomous Tower Crane Hook Detection",
    "authors": ["Xin Hu", "Xiyu Wang", "Yashu Chang", "Jian Xiao", "Hongliang Cheng"],
    "abstract": "To address the complexities of crane hook operating environments, the challenges faced by large-scale object detection algorithms on edge devices, and issues such as frame rate mismatch causing image delays, this paper proposes a faster, lighter, and more efficient object detection algorithm called FLE-YOLO. Firstly, the FasterNet is used as the backbone for feature extraction, and the Triplet Attention mechanism is integrated to effectively emphasize target information while maintaining network lightweightness effectively. Additionally, the Slim-neck module is introduced in the neck connection layer, utilizing a lightweight convolutional network GSconv to further streamline the network structure without compromising recognition accuracy. Lastly, the Dyhead module is employed in the head section to unify multiple attention operations, improve the ability to resist interference from small objects and complex backgrounds. Experimental evaluations on public datasets VOC2012 and COCO2017 demonstrate the effectiveness of our proposed algorithm in terms of lightweight design and detection accuracy. Experimental evaluations were also conducted using images of crane hooks captured under complex operating conditions. The results demonstrate that compared to the original algorithm, the proposed approach achieves a reduction in computational complexity to 19.4 GFLOPs, an increase in FPS to 142.857 f/s, and the precision reached 97.3%. Additionally, the AP50 reaches 98.3%, reflecting 0.6% improvement. Ultimately, the testing carried out at the construction site successfully facilitated the identification and tracking of hooks, thereby ensuring the safety and efficiency of tower crane operations.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.3390/app15105364",
    "code_url": "https://github.com/sugar-fifty-doge/FLE-YOLO",
    "thumbnail": "assets/img/models/FLE-YOLO.png"
  },
  {
    "id": "VR-Point-Cloud-Analysis",
    "title": "Fusion of Thermal Point Cloud Series of Buildings for Inspection in Virtual Reality",
    "authors": ["Emiliano Pérez", "Pilar Merchán", "Alejandro Espacio", "Santiago Salamanca"],
    "abstract": "Point cloud acquisition systems now enable the capture of geometric models enriched with additional attribute data, providing a deeper semantic understanding of the measured environments. However, visualizing complex spatiotemporal point clouds remains computationally challenging. This paper presents a fusion methodology that aggregates points from different instants into unified clouds with reduced redundancy while preserving time-varying information. The static 3D structure is condensed using a voxel approach, while temporal attributes are propagated across the merged data. The resulting point cloud is optimized and rendered interactively in a virtual reality (VR) application. This platform allows for intuitive exploration, visualization, and analysis of the merged clouds. Users can examine thermographic properties using color maps and study graphical temperature trends. The potential of VR for insightful interrogation of point clouds enriched with multiple properties is highlighted by the system.",
    "year": 2024,
    "modalities": ["3D point cloud"],
    "tasks": ["Point cloud visualization"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.3390/buildings14072127",
    "code_url": "https://github.com/3dcovim/VR-Point-Cloud-Analysis",
    "thumbnail": "assets/img/models/VR-Point-Cloud-Analysis.png",
	"contributor": "3D Co-ViM Lab",
    "contributor_url": "https://3dcovim.cms.unex.es/team/"
  },
  {
  "id": "structgan-v1",
  "title": "Automated Structural Design of Shear Wall Residential Buildings using Generative Adversarial Networks",
  "authors": ["Wenjie Liao", "Xinzheng Lu", "Yuli Huang", "Zhe Zheng", "Yuanqing Lin"],
  "abstract": "Artificial intelligence is reshaping building design processes to be smarter and automated. Considering the increasingly wide application of shear wall systems in high-rise buildings and envisioning the massive benefit of automated structural design, this paper proposes a generative adversarial network (GAN)-based shear wall design method, which learns from existing shear wall design documents and then performs structural design intelligently and swiftly. To this end, structural design datasets were prepared via abstraction, semanticization, classification, and parameterization in terms of building height and seismic design category. The GAN model improved its shear wall design proficiency through adversarial training supported by data and hyper-parametric analytics. The performance of the trained GAN model was appraised against the metrics based on the confusion matrix and the intersection-over-union approach. Finally, case studies were conducted to evaluate the applicability, effectiveness, and appropriateness of the innovative GAN-based structural design method, indicating significant speed-up and comparable quality.Artificial intelligence is reshaping building design processes to be smarter and automated. Considering the increasingly wide application of shear wall systems in high-rise buildings and envisioning the massive benefit of automated structural design, this paper proposes a generative adversarial network (GAN)-based shear wall design method, which learns from existing shear wall design documents and then performs structural design intelligently and swiftly. To this end, structural design datasets were prepared via abstraction, semanticization, classification, and parameterization in terms of building height and seismic design category. The GAN model improved its shear wall design proficiency through adversarial training supported by data and hyper-parametric analytics. The performance of the trained GAN model was appraised against the metrics based on the confusion matrix and the intersection-over-union approach. Finally, case studies were conducted to evaluate the applicability, effectiveness, and appropriateness of the innovative GAN-based structural design method, indicating significant speed-up and comparable quality.",
  "year": 2021,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Image-to-image translation", "Layout synthesis"],
  "applications": ["Automated structural design", "Shear wall layout generation"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.autcon.2021.103931",
  "code_url": "https://github.com/wenjie-liao/StructGAN_v1",
  "thumbnail": "assets/img/models/structgan-v1.png",
  "contributor": "Xinzheng Lu",
  "contributor_url": "https://www.civil.tsinghua.edu.cn/ceen/info/1063/1221.htm"
},
{
  "id": "structgan-phy",
  "title": "Intelligent structural design of shear wall residence using physics-enhanced generative adversarial networks",
  "authors": ["Xinzheng Lu", "Wenjie Liao", "Yu Zhang", "Yuli Huang"],
  "abstract": "Intelligent structural design using generative adversarial networks (GANs) is a revolutionary design approach for building structures. Despite its far-reaching capability, the data quantity and quality may have limited the performance of such a data-driven network. This study proposes to enhance the objectiveness of training processes by innovatively introducing a surrogate model, Physics Estimator, that informs the generator by appraising the physical behavior of the generated design. Dual loss functions evaluated by a traditional data-driven discriminator and the Physics Estimator collaboratively foster the physics-enhanced GAN architecture. We further develop a structural mechanics model to train and optimize the inherent accuracy of the Physics Estimator. The comparative study suggests that the proposed physics-enhanced GAN can generate structural designs from architectural drawings and specified design conditions 44% better than a data-driven design method and 90 times faster than a competent engineer.",
  "year": 2022,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Image-to-image translation", "Layout synthesis"],
  "applications": ["Automated structural design"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1002/eqe.3632",
  "code_url": "https://github.com/wenjie-liao/StructGAN-PHY",
  "thumbnail": "assets/img/models/structgan-phy.png",
  "contributor": "Xinzheng Lu",
  "contributor_url": "https://www.civil.tsinghua.edu.cn/ceen/info/1063/1221.htm"
},
{
  "id": "structgan-txt-txtimg2img",
  "title": "Intelligent generative structural design method for shear wall building based on “fused-text-image-to-image” generative adversarial networks",
  "authors": ["Wenjie Liao", "Yuli Huang", "Zhe Zheng", "Xinzheng Lu"],
  "abstract": "Like the way engineers designing buildings, competent generative design methods try to understand the prescriptive requirement in text and architectural sketches, apply engineering principles and develop the structural design. However, this requirement may be challenging to existing methods because they are not good at simultaneously taking text and image input and then generating designs. This study proposed an innovative design approach, TxtImg2Img, to overcome the difficulties. Based on generative adversarial networks architecture, the generator is proposed to encode, extract and fuse texts and images, and generate new design images; the discriminator is developed to judge real and fake images and texts. Consequently, TxtImg2Img is advantageous in extracting features from the multimodal text and image data, fusing the features using the Hadamard product, and generating designs to satisfy the text-image requirements after learning from a limited number of design samples. Specifically, TxtImg2Img can generate structural design images without distortion, and the corresponding structural design meets the mechanical requirements, after being trained by dozens of words and hundreds of image data. The case studies confirm performance improvement of up to 21% and that the proposed approach presents a promising breakthrough for intelligent construction.",
  "year": 2022,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Multimodal conditional generation", "Image-to-image translation", "Layout synthesis"],
  "applications": ["Automated structural design", "Shear wall layout generation"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.eswa.2022.118530",
  "code_url": "https://github.com/wenjie-liao/StructGAN-TXT-TxtImg2Img",
  "thumbnail": "assets/img/models/structgan-txt-txtimg2img.png",
  "contributor": "Xinzheng Lu",
  "contributor_url": "https://www.civil.tsinghua.edu.cn/ceen/info/1063/1221.htm"
},
{
  "id": "image2triplets",
  "title": "Image2Triplets: A computer vision-based explicit relationship extraction framework for updating construction activity knowledge graphs",
  "authors": ["Zaolin Pan", "Cheng Su", "Yichuan Deng", "Jack C. P. Cheng"],
  "abstract": "Knowledge graph (KG) is an effective tool for knowledge management, particularly in the architecture, engineering and construction (AEC) industry, where knowledge is fragmented and complicated. However, research on KG updates in the industry is scarce, with most current research focusing on text-based KG updates. Considering the superiority of visual data over textual data in terms of accuracy and timeliness, the potential of computer vision technology for explicit relationship extraction in KG updates is yet to be explored. This paper combines zero-shot human-object interaction detection techniques with general KGs to propose a novel framework called Image2Triplets that can extract explicit visual relationships from images to update the construction activity KG. Comprehensive experiments on the images of architectural decoration processes have been performed to validate the proposed framework. The results and insights will contribute new knowledge and evidence to human-object interaction detection, KG update and construction informatics from the theoretical perspective.",
  "modalities": ["Ground RGB"],
  "year": 2022, 
  "tasks": ["Human–object interaction detection"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.compind.2022.103610",
  "code_url": "https://github.com/CrossStyle/Image2Triplets",
  "thumbnail": "assets/img/models/image2triplets.png"
},
{
  "id": "mr-visual-warning",
  "title": "Real-time mixed reality-based visual warning for construction workforce safety",
  "authors": ["Shaoze Wu", "Lei Hou", "Guomin (Kevin) Zhang", "Haosen Chen"],
  "abstract": "Spatial locations of personnel, equipment, and materials are constantly changing as construction projects progress. The dynamic nature of the construction industry affects workers' performance of identifying hazards. Even though a great deal of effort has been made to improve construction safety, the construction industry still witnesses a high accident rate. In order to complement the existing body of knowledge relating to construction safety, this paper integrates Digital Twin (DT), Deep Learning (DL), and Mixed Reality (MR) technologies into a newly developed real-time visual warning system, which enables construction workers to proactively determine their safety status and avoid accidents. Next, system tests were conducted under three quasi-on-site scenarios, and the feasibility was proven in terms of synchronising construction activities over a large area and visually representing hazard information to its users. These evidenced merits of the development testing scenarios can improve workers' risk assessment accuracy, reinforce workers' safety behaviour, and provide a new perspective for construction safety managers to analyse construction safety status.",
  "year": 2022,
  "modalities": ["Video", "BIM Models"],
  "tasks": ["Object detection", "Object tracking"],
  "applications": ["Safety monitoring"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104252",
  "code_url": "https://github.com/Mercer-S/MR-visual-warning",
  "thumbnail": "assets/img/models/mr-visual-warning.png"
},
{
  "id": "synthetic-truss-bridges",
  "title": "Automated production of synthetic point clouds of truss bridges for semantic and instance segmentation using deep learning models",
  "authors": ["Daniel Lamas", "Andrés Justo", "Mario Soilán", "Belén Riveiro"],
  "abstract":"The cost of obtaining large volumes of bridge data with technologies like laser scanners hinders the training of deep learning models. To address this, this paper introduces a new method for creating synthetic point clouds of truss bridges and demonstrates the effectiveness of a deep learning approach for semantic and instance segmentation of these point clouds. The method generates point clouds by specifying the dimensions and components of the bridge, resulting in high variability in the generated dataset. A deep learning model is trained using the generated point clouds, which is an adapted version of JSNet. The accuracy of the results surpasses previous heuristic methods. The proposed methodology has significant implications for the development of automated inspection and monitoring systems for truss bridges. Furthermore, the success of the deep learning approach suggests its potential for semantic and instance segmentation of complex point clouds beyond truss bridges.",
  "year": 2024,
  "modalities": ["3D point cloud", "Synthetic"],
  "tasks": ["Point cloud generation"],
  "applications": ["Structural Condition Monitoring"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.105176",
  "code_url": "https://github.com/GeoTechUVigo/synthetic_truss_bridges",
  "thumbnail": "assets/img/models/synthetic-truss-bridges.png"
},
{
  "id": "truss-bridge-pointcloud-segmentation",
  "title": "Instance and semantic segmentation of point clouds of large metallic truss bridges",
  "authors": ["Daniel Lamas", "Andrés Justo", "Mario Soilán", "Manuel Cabaleiro", "Belén Riveiro"],
  "abstract": "Several methods have been developed for the semantic segmentation of reinforced concrete bridges, however, there is a gap for truss bridges. Therefore, in this study a state-of-the-art methodology for the instance and semantic segmentation of point clouds of truss bridges for modelling purposes is presented, which, to the best of the authors' knowledge, is the first such methodology. This algorithm segments each truss element and classifies them as a chord, diagonal, vertical post, interior lateral brace, bottom lateral brace, or strut. The algorithm consists of a sequence of methods, including principal component analysis or clustering, that analyse each point and its neighbours in the point cloud. Case studies show that by adjusting only six manually measured parameters, the algorithm can automatically segment a truss bridge point cloud.",
  "year": 2023,
  "modalities": ["3D point cloud"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Structural Condition Monitoring"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.104659",
  "code_url": "https://github.com/GeoTechUVigo/truss_bridge_pointcloud_segmentation",
  "thumbnail": "assets/img/models/truss-bridge-pointcloud-segmentation.gif"
},
{
  "id": "floorplangan",
  "title": "FloorplanGAN: Vector residential floorplan adversarial generation",
  "authors": ["Ziniu Luo", "Weixin Huang"],
  "abstract": "An architectural floorplan is a class of drawings that reflects the layout of rooms. The difference between a floorplan and a natural image and its dual features as both a vector graphic and a raster image makes it difficult to be generated by conventional deep neural generative models. We propose an adversarial generative framework that combines vector generation and raster discrimination for residential floorplan generation tasks. The floorplan is first generated in vector format with room areas as constraints and then discriminated in raster format visually using convolutional layers. A Differentiable Renderer connects the gap between the Vector Generator and Raster Discriminator. A self-attention mechanism is utilized to capture the interrelations of rooms in each floorplan. Experiments were conducted to demonstrate the feasibility of the proposed FloorplanGAN. In addition, we evaluated the effectiveness of generation based on diverse objective metrics and a user study. The code is available here: https://github.com/luozn15/FloorplanGAN.",
  "year": 2022,
  "modalities": ["Raster floorplan images"],
  "tasks": ["Image-to-image translation"],
  "applications": ["Floorplan generation"],
  "license": "MIT",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104470",
  "code_url": "https://github.com/luozn15/FloorplanGAN",
  "thumbnail": "assets/img/models/floorplangan.png"
},
{
  "id": "cloud2bim",
  "title": "Open-source automatic pipeline for efficient conversion of large-scale point clouds to IFC format",
  "authors": ["Slávek Zbirovský", "Václav Nežerka"],
  "abstract": "Building Information Model (BIM) creation usually relies on laborious manual transformation of the unstructured point cloud data provided by laser scans or photogrammetry. This paper presents Cloud2BIM, an open-source software tool designed to automate the conversion of point clouds into BIM models compliant with the Industry Foundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for wall and slab segmentation, opening detection, and room zoning based on real wall surfaces, resulting in a comprehensive and fully automated workflow. Unlike existing tools, it avoids computationally- and calibration-intensive techniques such as RANSAC, supports non-orthogonal geometries, and provides unprecedented processing speed, achieving results up to seven times faster than fastest competing solutions. Systematic validation using benchmark datasets confirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for generating accurate BIM models, capable of converting extensive point cloud datasets for entire buildings into IFC format with minimal user input.",
  "year": 2025,
  "modalities": ["3D point cloud"],
  "tasks": ["Scan-to-BIM"],
  "applications": ["Site understanding"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2025.106303",
  "code_url": "https://github.com/VaclavNezerka/Cloud2BIM",
  "thumbnail": "assets/img/models/cloud2bim.png"
},
{
  "id": "bfa-yolo",
  "title": "BFA-YOLO: A balanced multiscale object detection network for building façade elements detection",
  "authors": ["Yangguang Chen", "Tong Wang", "Guanzhou Chen", "Kun Zhu", "Xiaoliang Tan", "Jiaqi Wang", "Wenchao Guo", "Qing Wang", "Xiaolong Luo", "Xiaodong Zhang"],
  "abstract": "The detection of façade elements on buildings, such as doors, windows, balconies, air conditioning units, billboards, and glass curtain walls, is a critical step in automating the creation of Building Information Modeling (BIM). However, this field faces significant challenges, including the uneven distribution of façade elements, the presence of small objects, and substantial background noise, which hamper detection accuracy. To address these issues, we developed the BFA-YOLO model and the BFA-3D dataset in this study. The BFA-YOLO model is an advanced architecture designed specifically for analyzing multi-view images of façade elements. It integrates three novel components: the Feature Balanced Spindle Module (FBSM) that tackles the issue of uneven object distribution; the Target Dynamic Alignment Task Detection Head (TDATH) that enhances the detection of small objects; and the Position Memory Enhanced Self-Attention Mechanism (PMESA), aimed at reducing the impact of background noise. These elements collectively enable BFA-YOLO to effectively address each challenge, thereby improving model robustness and detection precision. The BFA-3D dataset offers multi-view images with precise annotations across a wide range of façade element categories. This dataset is developed to address the limitations present in existing façade detection datasets, which often feature a single perspective and insufficient category coverage. Through comparative analysis, BFA-YOLO demonstrated improvements of 1.8% and 2.9% in mAP50 on the BFA-3D dataset and the public Façade-WHU dataset, respectively, when compared to the baseline YOLOv8 model. These results highlight the superior performance of BFA-YOLO in façade element detection and the advancement of intelligent BIM technologies. The dataset and code are available at https://github.com/CVEO/BFA-YOLO.",
  "year": 2025,
  "modalities": ["Aerial RGB"],
  "tasks": ["Object detection"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.aei.2025.103289",
  "code_url": "https://github.com/CVEO/BFA-YOLO",
  "thumbnail": "assets/img/models/bfa-yolo.png"
},
{
  "id": "MLSTRUCT-FP_benchmarks",
  "title": "Large-scale multi-unit floor plan dataset for architectural plan analysis and recognition",
  "authors": ["Pablo N. Pizarro", "Nancy Hitschfeld", "Ivan Sipiran"],
  "abstract": "Among automatic floor plan analysis, data-driven methods have become increasingly popular in recent years because of their superior accuracy and generalizability compared to traditional approaches while processing rasterized floor plans. However, the scarcity of public raster datasets with various styles and sufficient quantity hinders the development of new models, as current ones only contain a single apartment or house, limiting the analysis of large-scale plans usually designed in architectural and structural offices. In order to address that issue, this paper presents a multi-unit floor plan dataset comprising 954 high-resolution images of residential buildings with annotated walls and slabs as polygons, enabling large-scale plan analysis. Additionally, this study implements an automatic wall vectorization method that uses a learning discriminative-based semantic segmentation U-Net model to retrieve wall objects, followed by a deep-learning model that predicts the segmented primitives, providing a baseline for future comparison of automatic wall segmentation results.",
  "year": 2023,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Plan recognition"],
  "license": "MIT",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.105132",
  "code_url": "https://github.com/MLSTRUCT/MLSTRUCT-FP_benchmarks",
  "thumbnail": "assets/img/models/MLSTRUCT-FP_benchmarks.png",
  "contributor": "MLSTRUCT Research Group",
  "contributor_url": "https://github.com/MLSTRUCT"
},
{
  "id": "tbbrdet",
  "title": "Deep learning approaches to building rooftop thermal bridge detection from aerial images",
    "authors": ["Zoe Mayer", "James Kahn", "Yu Hou", "Markus Götz", "Rebekka Volk", "Frank Schultmann"],
  "abstract": "Thermal bridges are weak points of building envelopes that can lead to energy losses, collection of moisture, and formation of mould in the building fabric. To detect thermal bridges of large building stocks, drones with thermographic cameras can be used. As the manual analysis of comprehensive image datasets is very time-consuming, we investigate deep learning approaches for its automation. For this, we focus on thermal bridges on building rooftops recorded in panorama drone images from our updated dataset of Thermal Bridges on Building Rooftops (TBBRv2), containing 926 images with 6,927 annotations. The images include RGB, thermal, and height information. We compare state-of-the-art models with and without pretraining from five different neural network architectures: MaskRCNN R50, Swin-T transformer, TridentNet, FSAF, and a MaskRCNN R18 baseline. We find promising results, especially for pretrained models, scoring an Average Recall above for detecting large thermal bridges with a pretrained Swin-T Transformer model.",
  "year": 2023,
  "modalities": ["Thermal images"],
  "tasks": ["Object detection"],
  "applications": ["Building energy analysis"],
  "license": "BSD 3-Clause",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104690",
  "code_url": "https://github.com/Helmholtz-AI-Energy/TBBRDet",
  "thumbnail": "assets/img/models/TBBRDet.gif"
},
{
  "id": "OccFaçade",
  "title": "OccFaçade: Enabling precise building façade parsing in large urban scenes with occlusion",
    "authors": ["Yongjun Zhang", "Dongdong Yue", "Xinyi Liu", "Siyuan Zou", "Weiwei Fan", "Zihang Liu"],
  "abstract": "Building façade parsing is to recognize the building façade image into different categories of individuals including walls, doors, windows, balconies, etc. However, obstructions such as trees present a significant challenge to conducting façade parsing. In this paper, we designed OccFaçade to achieve high-precision parsing of occluded building façades in large urban scenes. OccFaçade primarily incorporates two modules, Multi-layer Dilated Convolution Module (MD-Module) and Multi-scale Row-Column Convolution Module (MRC-Module), to capture repeated texture in local and row-column directions. This aims to leverage repetitive textures to address occlusion challenges in building façade parsing. Besides, we introduce our building façade dataset MeshFaçade from the Mesh data generated by drone imagery to study the occlusion problem of missing textures. The experimental results demonstrate that OccFaçade achieves state-of-the-art performance with mIOU of 85.01%, 84.09%, 72.95%, and 88.83% on the ENPC2014 dataset, ECP dataset, RueMonge2014 dataset, and our MeshFaçade dataset, respectively. The code and data are available at https://github.com/yueyisui/OccFacade.",
  "year": 2024,
  "modalities": ["Ground RGB"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1080/01431161.2024.2391589",
  "code_url": "https://github.com/yueyisui/OccFacade",
  "thumbnail": "assets/img/models/OccFaçade.png",
  "contributor": "skyearth team",
  "contributor_url": "https://en.skyearth.org/team/"
},
{
    "id": "MultiWorker3DPose",
    "title": "A synthetic data-enhanced method for automated 3D pose recognition of construction workers",
    "authors": ["Yonglin Fu", "Weisheng Lu", "Zhiming Dong", "Yihai Fang"],
    "abstract": "Automated 3D pose recognition of construction workers is instrumental to analyzing their occupational safety and health, productivity and other jobsite behaviors. Existing studies in this field have been confined to high-quality training datasets collected from real-life construction jobsites, potentially triggering ethical, privacy, and cost concerns. Inspired by the success of synthetic data in other fields, this research proposes a synthetic data-enhanced method for automated 3D pose recognition of construction workers. It generates a synthetic dataset to supplement a real-life dataset for model training, presents a monocular vision-based model for recognizing multiple workers’ 3D poses, and then validates the model performance. Experiments verify that this model jointly trained with synthetic and real data outperforms a model trained on real data alone. The data enrichment approach explored in this study offers reliable data quality at less expense than real data-focused approaches. This research therefore lays a foundation for a series of studies to enhance workers’ occupational safety and health and productivity.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Pose estimation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.eswa.2025.128768",
    "code_url": "https://github.com/fyongl6/MultiWorker3DPose",
    "thumbnail": "assets/img/models/MultiWorker3DPose.png"
  },
{
    "id": "SkeySpot",
    "title": " Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry",
    "authors": ["Dhruv Dosi", "Rohit Meena", "Param Rajpura", "Yogesh Kumar Meena"],
    "abstract": "Legacy floor plans, often preserved only as scanned documents, remain essential resources for architecture, urban planning, and facility management in the construction industry. However, the lack of machine-readable floor plans render large-scale interpretation both time-consuming and error-prone. Automated symbol spotting offers a scalable solution by enabling the identification of service key symbols directly from floor plans, supporting workflows such as cost estimation, infrastructure maintenance, and regulatory compliance. This work introduces a labelled Digitised Electrical Layout Plans (DELP) dataset comprising 45 scanned electrical layout plans annotated with 2,450 instances across 34 distinct service key classes. A systematic evaluation framework is proposed using pretrained object detection models for DELP dataset. Among the models benchmarked, YOLOv8 achieves the highest performance with a mean Average Precision (mAP) of 82.5%. Using YOLOv8, we develop SkeySpot, a lightweight, open-source toolkit for real-time detection, classification, and quantification of electrical symbols. SkeySpot produces structured, standardised outputs that can be scaled up for interoperable building information workflows, ultimately enabling compatibility across downstream applications and regulatory platforms. By lowering dependency on proprietary CAD systems and reducing manual annotation effort, this approach makes the digitisation of electrical layouts more accessible to small and medium-sized enterprises (SMEs) in the construction industry, while supporting broader goals of standardisation, interoperability, and sustainability in the built environment.",
    "year": 2025,
    "modalities": ["Rasterized CAD images"],
    "tasks": ["Object detection"],
	"applications": ["Plan recognition"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.48550/arXiv.2508.10449",
    "code_url": "https://github.com/HAIx-Lab/SkeySpot",
    "thumbnail": "assets/img/models/SkeySpot.png"
  },
{
    "id": "Vitruvio",
    "title": "Vitruvio: Conditional variational autoencoder to generate building meshes via single perspective sketches",
    "authors": ["Alberto Tono", "Heyaojing Huang", "Ashwin Agrawal", "Martin Fischer"],
    "abstract": "At the beginning of a project, architects convey design ideas via quick 2D diagrams, front views, floor plans, and sketches. Consequently, many stakeholders have difficulty visualizing the 3D representation of the building mass, leading to varied interpretations thus inhibiting a shared understanding of the design. To alleviate the challenge, this paper proposes a deep learning-based method, Vitruvio, for creating a 3D model from a single perspective sketch. This method allows designers to automatically generate 3D representations in real-time based on their initial sketches and thus communicate effectively and intuitively to the client. Vitruvio adapts the Occupancy Network to perform single view reconstruction (SVR), a technique for creating 3D representations from a single image. Vitruvio achieves: (1) an 18% increase in the reconstruction accuracy and (2) a 26% reduction in the inference time compared to the Occupancy Network on one thousand buildings provided by the New York municipality. This research investigates the effect that the building orientation has on the reconstruction quality, discovering that Vitruvio can capture fine-grain details in complex buildings when their native orientation is preserved during training, as opposed to the SVR's standard practice that aligns every building to its canonical pose. The code is available here https://github.com/CDInstitute/Vitruvio.",
    "year": 2024,
    "modalities": ["2D sketch", "3D mesh"],
    "tasks": ["3D reconstruction"],
	"applications": ["3D building mesh generation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105498",
    "code_url": "https://github.com/CDInstitute/Vitruvio",
    "thumbnail": "assets/img/models/Vitruvio.png"
  },
  {
    "id": "BCon",
    "title": "ControlNet-based domain adaptation for synthetic construction images via graphical simulation and generative AI",
    "authors": ["Sina Davari", "Daeho Kim", "Ali Tohidifar"],
    "abstract": "Data scarcity in construction hinders deep neural network training for computer vision applications. While synthetic data generators provide annotated images, they lack realism, creating a reality gap that leads to suboptimal real-world performance. This paper introduces BCon, a framework that integrates BlendCon, a construction data generation engine, with ControlNet, a generative architecture featuring conditioning controls, to enhance the realism and diversity of synthetic images while preserving annotations. Through hyperparameter tuning and post-processing, a dataset of 25,600 enhanced images is created. Quantitative evaluations demonstrate significant improvements in realism metrics: DreamSim (+9.5%), VIEScore (+114.3%), CLIPScore (+14.7%), and FID-5k (+22.6%), indicating closer alignment with real images. Moreover, YOLOv10 models trained on enhanced images achieve an AP50–95 of 0.66 on worker detection, outperforming those trained on original synthetic data by 7.9% and slightly surpassing models trained on equivalently sized real data. This framework offers cost-effective, high-quality dataset generation for visual AI applications in construction.",
    "year": 2025,
    "modalities": ["Synthetic"],
    "tasks": ["Image synthesis"],
	"applications": ["Site understanding"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106562",
    "code_url": "https://github.com/SinaDavari/bcon",
    "thumbnail": "assets/img/models/BCon.png",
	"added_date": "2025-10-5"
  },
  {
    "id": "client-to-bim",
    "title": "Transformer-based framework for mapping client requirements to BIM",
    "authors": ["Syed Haseeb Shah", "Saddiq Ur Rehman", "Inhan Kim", "Kyung-Eun Hwang"],
    "abstract": "Translating heterogeneous, client-authored textual requirements into constructible, information-rich models constitutes a primary impediment to digital transformation in early design phases. Legacy workflows demand high frequency client architect iteration, manual decoding of narrative requirements, and bespoke parametric modeling, introducing latency and inconsistency. This paper introduces an end-to-end automation pipeline that couples advanced Natural Language Processing (NLP) with Building Information Modeling (BIM) to dynamically interpret design intent from user inputs and instantiate corresponding BIM assemblies. A semantic translation layer maps parsed entities to a curated BIM model repository and propagates constraints into the authoring environment. On a multi project evaluation set the framework achieved 92 % mapping accuracy between client inputs and instantiated BIM elements. Embedding this capability enhances requirement traceability, clarifies intent for stakeholders, and enables scalable data driven design analytics. This contribution operationalizes AI assisted construction automation by unifying NLP and BIM within a single extensible workflow.",
    "year": 2025,
    "modalities": ["BIM"],
    "tasks": ["Text-to-BIM"],
	"applications": ["Design brief automation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106601",
    "code_url": "https://github.com/ITalab/client-to-bim",
    "thumbnail": "assets/img/models/client-to-bim.png",
	"added_date": "2025-10-15",
	"contributor": "Haseeb",
	"contributor_url": "https://github.com/QuantumNovice"
  },
  {
    "id": "ResBIM",
    "title": "Fully automated synthetic BIM dataset generation using a deep learning-based framework",
    "authors": ["Xing Liang", "Nobuyoshi Yabuki", "Tomohiro Fukuda"],
    "abstract": "Building information models (BIMs) are essential for efficient building operation, yet most existing buildings only have two-dimensional (2D) drawings, leading to increased interest in 2D-to-BIM reconstruction. To address the data scarcity hindering automated BIM reconstruction and evaluation, this paper presents a deep learning-based fully automated framework for BIM dataset generation. The approach uses image processing to define polygonal boundaries, applies neural networks to generate geometric layouts, and augments semantic information with predefined data for BIM generation via software application programming interfaces (APIs). The resulting Residential unit BIM (ResBIM) is a synthetic dataset comprising over 1000 paired BIMs (RVT format) and their corresponding 2D floor plans automatically annotated via a toolbox, filling a critical gap in BIM data availability. This work provides a scalable automated BIM reconstruction solution and establishes the foundation for future AI-driven BIM automation research.",
    "year": 2026,
    "modalities": ["Rasterized CAD images"],
    "tasks": ["2D-to-BIM reconstruction"],
	"applications": ["Conceptual design"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106584",
    "code_url": "https://github.com/RogerLiang0725/ResBIM",
    "thumbnail": "assets/img/models/ResBIM.png",
	"added_date": "2025-10-20",
	"contributor": "XingLiang",
    "contributor_url": "https://github.com/RogerLiang0725"
  },
  {
    "id": "masonry-cc",
    "title": "Automatic segmentation of 3D point clouds of rubble masonry walls, and its application to building surveying, repair and maintenance",
    "authors": ["Enrique Valero", "Frédéric Bosché", "Alan Forster"],
    "abstract": "Changing climatic conditions are contributing to faster deterioration of building fabric. Increasing number of heavy rainfall events can particularly affect historic and Cultural Heritage (CH) buildings. These evolving and uncertain circumstances demand more frequent survey of building fabric to ensure satisfactory repair and maintenance. However, traditional fabric surveys have been shown to lack efficiency, accuracy and objectivity, hindering essential repair operations. The recent development of reality capture technologies, together with the development of algorithms to effectively process the acquired data, offers the promise of transformation of surveying methods. This paper presents an original algorithm for automatic segmentation of individual masonry units and mortar regions in digitised rubble stone constructions, using geometrical and colour data acquired by Terrestrial Laser Scanning (TLS) devices. The algorithm is based on the 2D Continuous Wavelet Transform (CWT), and uniquely it does not require the wall to be flat or plumb. This characteristic is important because historic structures, in particular, commonly present non-negligible levels of bow, waviness and out-of-verticality. The method is validated through experiments undertaken using data from two relevant and highly significant Scottish CH buildings. The value of such segmentation to building surveying and maintenance regimes is also further demonstrated with application in automated and accurate measurement of mortar recess and pinning. Overall, the results demonstrate the value of the automatic segmentation of masonry units towards more comprehensive and accurate surveys.",
    "year": 2018,
    "modalities": ["3D point cloud"],
    "tasks": ["Point cloud segmentation"],
	"applications": ["Historic digital survey"],
    "license": "GNU 2.1",
    "paper_url": "https://doi.org/10.1016/j.autcon.2018.08.018",
    "code_url": "https://github.com/CyberbuildLab/masonry-cc",
    "thumbnail": "assets/img/models/masonry-cc.png",
	"added_date": "2025-11-2",
	"contributor": "Cyberbuild Lab",
    "contributor_url": "https://github.com/CyberbuildLab"
  },
    {
    "id": "4PlCS",
    "title": "4-Plane congruent sets for automatic registration of as-is 3D point clouds with 3D BIM models",
    "authors": ["Martín Bueno", "Frédéric Bosché", "Higinio González-Jorge", "Joaquín Martínez-Sánchez", "Pedro Arias"],
    "abstract": "Construction quality and progress control are demanding, yet critical construction activities. Building Information Models and as-built scanned data can be used in Scan-vs-BIM processes to effectively and comprehensively support these activities. This however requires accurate registration of scanned point clouds with 3D (BIM) models. Automating such registration remains a challenge in the context of the built environment, because as-built can be incomplete and/or contain data from non-model objects, and construction buildings and other structures often present symmetries and self-similarities that are very challenging to registration.  In this paper, we present a novel automatic coarse registration method that is an adaptation of the ‘4 Points Congruent Set’ algorithm to the use of planes; we call it the ‘4-Plane Congruent Set’ (4-PlCS) algorithm. The approach is further integrated in a software system that delivers not one but a ranked list of the most likely transformations, so to allow the user to quickly select the correct transformation, if need be. Two variants of the method are also considered, in particular one in the case when the vertical axis is known a priori; we call that method the 4.5-PlCS method. The proposed algorithm is tested using five different datasets, including three simulated and two real-life ones. The results show the effectiveness of the proposed method, where the correct transformation always ranks very high (in our experiments, first or second), and is extremely close to the ground-truth transformation. Experimental comparison of the proposed approach with a standard, more intuitive approach based on finding 3-plane congruent sets shows the discriminatory power of 4-plane bases over 3-plane bases, albeit at no clear benefits in terms of computational time. The experimental results for the 4.5-PlCS method show that it delivers a non-negligible reduction in computational time (approx. 20%), but at no additional benefit in terms of effectiveness in finding the correct transformation.",
    "year": 2018,
    "modalities": ["3D Point Cloud"],
    "tasks": ["3D registration", "BIM alignment"],
	"applications": ["Progress monitoring", "As-Built BIM generation"],
    "license": "Apache 2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2018.01.014",
    "code_url": "https://github.com/CyberbuildLab/4PlCS",
    "thumbnail": "assets/img/models/4PlCS.png",
	"added_date": "2025-11-2",
	"contributor": "Cyberbuild Lab",
    "contributor_url": "https://github.com/CyberbuildLab"
  },
    {
    "id": "BE-OLS",
    "title": "Development of a Built Environment Ontology Lookup Service (BE-OLS) with a New Ontology Evaluation",
    "authors": ["Arghavan Akbarieh", "Iryna Osadcha", "Karim Farghaly", "Frédéric Bosché", "Rene Puust"],
    "abstract": "Recent years have seen a surge in development of formal ontologies within the domain of built environment. Yet, experts like novices face difficulties in locating relevant ontologies, hampering the use of ontologies. Besides, these ontologies demonstrate a range of development maturity. This highlights a gap: the need for a continuously updated repository to help users in discovering, evaluating and (re)using ontologies. This paper presents the Ontology Lookup Service for the Built Environment: BE-OLS, which allows to easily find and consult existing ontologies. It also enables researchers to find gaps or redundancies to help bring some order to the current state of the field.",
    "year": 2025,
    "modalities": ["Ontology"],
    "tasks": ["Ontology Search"],
	"applications": ["Knowledge management"],
    "license": "Unspecified",
    "paper_url": "http://www.doi.org/10.35490/EC3.2025.353",
    "code_url": "https://github.com/CyberbuildLab/BE-OLS",
    "thumbnail": "assets/img/models/BE-OLS.png",
	"added_date": "2025-11-2",
	"contributor": "Cyberbuild Lab",
    "contributor_url": "https://github.com/CyberbuildLab"
  },
    {
    "id": "pcd-bim-matching-confidence",
    "title": "Evaluating confidence in geometric matching between 3D point clouds and BIM models by integrating coverage, distance, and distribution metrics",
    "authors": ["Ahmet Bahaddin Ersöz", "Frédéric Bosché"],
    "abstract": "Accurate and objective assessment of the matching of a Building Information Model (BIM) with 3D point cloud data (PCD) is critical to Scan-to-BIM and Scan-vs-BIM workflows. However, existing methods for PCD-BIM matching evaluation do not fully and robustly account for geometric accuracy and spatial completeness. This paper introduces a statistically-grounded method that combines three indices that complementarily assess matching Coverage, Distribution, and Distance. The proposed method also accounts for inter-element occlusions when calculating each element’s theoretically visible surface, which increases the metrics’ reliability. Validation is conducted across 46 PCD-BIM pairs, encompassing 4000+ elements from ISPRS, CV4AEC, BIMNET and custom datasets, as well as a residential building case study comparing manual and automated BIM model reconstructions, and demonstrating the applicability of the method to any type of element. Results show practical value for both Scan-to-BIM and Scan-vs-BIM practice and enable quantitative assessment of benchmark dataset quality via the proposed indices.",
    "year": 2026,
    "modalities": ["3D point cloud"],
    "tasks": ["PCD-BIM matching"],
	"applications": ["Scan-to-BIM"],
    "license": "Apache 2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106636",
    "code_url": "https://github.com/CyberbuildLab/pcd-bim-matching-confidence",
    "thumbnail": "assets/img/models/pcd-bim-matching-confidence.png",
	"added_date": "2025-11-2",
	"contributor": "Cyberbuild Lab",
    "contributor_url": "https://github.com/CyberbuildLab"
  },
{
    "id": "BMKG-DCoT",
    "title": "Knowledge graph-driven bridge maintenance decision-making via integrating large language models and chain-of-thought reasoning",
    "authors": ["Yuchen Wang", "Wen Xiong", "Yanjie Zhu", "C.S. Cai"],
    "abstract": "Bridge maintenance knowledge graphs (BMKGs) contain rich historical records of defects and corresponding maintenance actions, yet few studies exploit this data to guide decisions for new defects. Traditional case-based reasoning (CBR) methods can surface similar historical cases but suffer from high retrieval costs and struggle with ambiguous scenarios. To address these limitations, this paper introduces BMKG-DCoT, which integrates large language models (LLMs) with chain-of-thought (CoT) reasoning to mimic maintenance expert decision workflows. BMKG-DCoT follows a three-phase iterative process—Inference-Planning (formulating reasoning steps and retrieval plans), Interaction-Decision (targeted data extraction from the BMKG), and Observation-Judgment (evaluating and refining inferences). On some standard and ambiguous maintenance queries, BMKG-DCoT achieves F1 scores of 0.9529 and 0.9130, respectively, outperforming baselines. Explainability analyses further confirm that its reasoning paths are transparent and interpretable, underscoring its promise as an AI-driven decision-support tool for bridge maintenance.",
    "year": 2026,
    "modalities": ["Knowledge graph"],
    "tasks": ["Information Retrieval", "Question Answering"],
	"applications": ["Structural Condition Monitoring"],
    "license": "AGPL 3.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106632",
    "code_url": "https://github.com/YuchenWang1/BMKG-DCoT",
    "thumbnail": "assets/img/models/BMKG-DCoT.png",
	"added_date": "2025-11-03",
	"contributor": "Yuchen Wang",
    "contributor_url": "https://github.com/YuchenWang1"
  },
  {
    "id": "BIM_updating",
    "title": "Semi-automated localised updating for as-built BIM of piping systems using point cloud data",
    "authors": ["Yu Zhang", "Long Chen", "Qiuchen Lu", "Yang Zou", "Xiaer Xiahou", "Simon Sølvsten", "Craig Hancock"],
    "abstract": "As-designed building information models (BIM) often diverge from as-built conditions, limiting their reliability during the operation and maintenance (O&M). Current research focuses on change detection but lacks a systematic workflow for reliable updates, especially for piping systems with frequent changes and complex geometries. The paper addresses how to establish a semi-automated, end-to-end workflow for localised updating as-designed BIM of piping systems from point cloud data. The workflow applies PointNet++ for segmentation, followed by iterative closest point, random sample consensus, and region-growing for geometry extraction. The proposed BIM updating taxonomy and dedicated pre-judgment updating requirements (PUR) and spatial and topological relationships up-dating (STRU) algorithms identify update requirements and automate parametric updates. Validation through case studies demonstrates the workflow's ability to accurately perform localised updates, reducing the manual workload by approximately 70 %. This practical, scalable solution strengthens O&M by maintaining accurate as-built models and inspires future automated BIM updating research.",
    "year": 2026,
    "modalities": ["3D Point Cloud"],
    "tasks": ["Point cloud segmentation"],
	"applications": ["Change detection", "As-Built BIM generation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106609",
    "code_url": "https://github.com/RainGo111/BIM_updating",
    "thumbnail": "assets/img/models/BIM_updating.png",
	"added_date": "2025-11-4",
	"contributor": "Yu Zhang",
    "contributor_url": "https://github.com/RainGo111"
  },
  {
    "id": "DoriaNET",
    "title": "Deep learning for post-hurricane aerial damage assessment of buildings",
    "authors": ["Chih-Shen Cheng", "Amir H. Behzadan", "Arash Noshadravan"],
    "abstract": "This study aims to improve post-disaster preliminary damage assessment (PDA) using artificial intelligence (AI) and unmanned aerial vehicle (UAV) imagery. In particular, a stacked convolutional neural network (CNN) architecture is introduced and trained on an in-house visual dataset from Hurricane Dorian. To account for the ordinality of damage level classes, the cross-entropy classification loss function is replaced with the square of earth mover's distance (EMD2) loss. The trained model achieves 65.6% building localization precision and 61% (90% considering ±1 class deviation from ground-truth) classification accuracy. It also exhibits a positive accuracy–confidence correlation, which is valuable for model assessment in situations where ground-truth information is not readily available. Finally, the outcome of damage assessment is compared with the literature by examining the relationship between building size and number of stories, and severity of induced disaster damage.",
    "year": 2021,
    "modalities": ["Aerial RGB"],
    "tasks": ["Object detection", "Semantic segmentation"],
	"applications": ["Structural Condition Monitoring"],
    "license": "ODC-BY",
    "paper_url": "https://doi.org/10.1111/mice.12658",
    "code_url": "https://doi.org/10.17603/ds2-gqvg-qx37",
    "thumbnail": "assets/img/models/DoriaNET.png",
	"added_date": "2025-11-4",
	"contributor": "ciber-lab",
	"contributor_url": "https://github.com/ciber-lab"
  },
  {
  "id": "TLSynth",
  "title": "TLSynth: A Novel Blender Add-On for Real-Time Point Cloud Generation from 3D Models",
  "authors": ["Emiliano Pérez", "Adolfo Sánchez-Hermosell", "Pilar Merchán"],
  "abstract": "Point clouds are a crucial element in the process of scanning and reconstructing 3D environments, such as buildings or heritage sites. They allow for the creation of 3D models that can be used in a wide range of applications. In some cases, however, only the 3D model of an environment is available, and it is necessary to obtain point clouds with the same characteristics as those captured by a laser scanner. For instance, point clouds may be required for surveys, performance optimization, site scan planning, or validation of point cloud processing algorithms. This paper presents a new terrestrial laser scanner (TLS) simulator, designed as a Blender add-on, that produces synthetic point clouds from 3D models in real time. The simulator allows users to adjust a set of parameters to replicate real-world scanning conditions, such as noise generation, ensuring the synthetic point clouds closely mirror those produced by actual laser scanners. The target meshes may be derived from either a real-world scan or 3D designs created using design software. By replicating the spatial distributions and attributes of real laser scanner outputs and supporting real-time generation, the simulator serves as a valuable tool for scan planning and the development of synthetic point cloud repositories, advancing research and practical applications in 3D computer vision.",
  "year": 2025,
  "modalities": ["3D Point Cloud"],
  "tasks": ["Synthetic point cloud generation", "Scan planning"],
  "applications": ["3D reconstruction"],
  "license": "CC BY 4.0",
  "paper_url": "https://doi.org/10.3390/rs17030421",
  "code_url": "https://github.com/3dcovim/TLSynth",
  "thumbnail": "assets/img/models/TLSynth.png",
  "added_date": "2025-11-11",
  "contributor": "3D Co-ViM Lab",
  "contributor_url": "https://3dcovim.cms.unex.es/team/"
},
    {
    "id": "LOD3",
    "title": "Generating LOD3 building models from structure-from-motion and semantic segmentation",
    "authors": ["Bryan German Pantoja Rosero", "Radhakrishna Achanta", "Mateusz Kozinski", "Pascal Fua", "Fernando Perez-Cruz", "Katrin Beyer"],
	"abstract": "This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["3D reconstruction", "Semantic Segmentation", "Structure from motion"],
	"applications": ["LOD3 Building Model Generation"],
    "license": "GNU 3.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104430",
    "code_url": "https://github.com/eesd-epfl/LOD3_buildings",
    "thumbnail": "assets/img/models/LOD3.png",
	"added_date": "2025-11-13",
	"contributor": "DISC Lab",
    "contributor_url": "https://disclab.pantoja-rosero.com/"
  },
    {
    "id": "DamageSegmentationXR",
    "title": "Integrating extended reality and AI-based damage segmentation for near real-time, traceable bridge inspections",
    "authors": ["B.G. Pantoja-Rosero", "S. Salamone"],
    "abstract": "This paper presents a framework for interactive and traceable visual bridge inspections by integrating AI-driven image processing models with extended reality. The framework addresses the subjectivity, labor intensity, and documentation challenges of traditional visual inspections. It employs YOLO11-seg models on a mixed reality device to perform multi-class damage instance detection and segmentation in near real-time, identifying cracks, spalling, rust, efflorescence, and exposed rebar. Unlike existing methods, this approach supports automated operation and local processing without cloud computing reliance. AI predictions are embedded in the 3D extended reality environment, enhancing efficiency and accuracy while enabling inspectors to visualize and interact with damage data, improving decision-making and traceability throughout the bridge life-cycle. Beyond model’s accuracy, inference time was evaluated to verify near real-time feasibility. Tests in three real-world scenarios demonstrated practical applicability. Future work will incorporate damage characterization and decision-support tools to advance digital inspection practices and foster efficient, resilient bridge monitoring.",
    "year": 2022,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection", "Semantic segmentation", "Extended reality"],
	"applications": ["Structural Condition Monitoring"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106567",
    "code_url": "https://github.com/bgpantojar/DamageSegmentationXR",
    "thumbnail": "assets/img/models/DamageSegmentationXR.png",
	"added_date": "2025-11-13",
	"contributor": "DISC Lab",
    "contributor_url": "https://disclab.pantoja-rosero.com/"
  },
  {
  "id": "dadt-buildings-model",
  "title": "Damage-augmented digital twins towards the automated inspection of buildings",
  "authors": [
    "Bryan German Pantoja-Rosero",
    "Radhakrishna Achanta",
	"Katrin Beyer"
  ],
  "abstract":"Current procedures for the rapid inspection of buildings and infrastructure are subjective, time-consuming, and cumbersome to document, necessitating new technologies to automate the process and eliminate these shortcomings. Fortunately, recent developments in imaging devices and artificial intelligence, such as computer vision, provide the necessary tools for this, though they are not yet integrated into infrastructure applications. In this paper, we propose an end-to-end pipeline that generates damage-augmented digital twins for buildings at LOD3, including geometrical information as well as data pertaining to damage condition and its characterization. Our framework incorporates multiple-view images to (1) create a level of detail model, (2) segment damage information, and (3) characterize damage. The core of the method is the structure from motion, which is used to reconstruct the building scene, and machine-learning models that segment and characterize damage. In contrast to current practices, our method does not require manual intervention, generates lightweight models, and can be applied to a wide range of assets. The results generated with our pipeline represent a significant step towards an automated infrastructure damage assessment. We intend to expand our work in the future to include real-time applications and applications to other types of infrastructure. Codes and data sets are publicly available (https://github.com/eesd-epfl/DADT_buildings and https://doi.org/10.5281/zenodo.7767478).",
  "year": 2023,
  "modalities": ["Ground RGB", "3D Point Cloud", "LOD3 Meshes"],
  "tasks": [
    "Semantic Segmentation",
    "3D Reconstruction"
  ],
  "applications": [
    "Structural Condition Monitoring",
    "Digital Twin Enrichment"
  ],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.104842",
  "code_url": "https://github.com/eesd-epfl/DADT_buildings",
  "thumbnail": "assets/img/models/dadt-buildings-model.png",
  "added_date": "2025-11-14",
  "contributor": "DISC Lab",
   "contributor_url": "https://disclab.pantoja-rosero.com/"
},
{
  "id": "crack_kinematics",
  "title": "Determining crack kinematics from imaged crack patterns",
  "authors": ["B.G. Pantoja-Rosero", "K.R.M. dos Santos", "R. Achanta", "A. Rezaie", "K. Beyer"],
  "abstract":"Determining the relationship between the cause of damage and the subsequent structural behavior of infrastructure systems requires an accurate characterization of the propagation of cracks, which represents the evolution of the damage state. When no information about the cause of damage is available, kinematic approaches can be used to describe the motion of crack contours. Current image-based approaches to derive crack kinematics use digital image correlation (DIC) on a set of sequential images as the crack propagates. However, DIC is invasive in that the structure surfaces must be painted with random speckle patterns, limiting its use primarily to controlled experiments. In this paper, we propose a novel image-based methodology for computing crack opening in Mode I or Mode II. As an input, this method takes a binary image from a semantic segmentation of an image of a crack pattern. This binary image is used to detect the opposite edges along the crack, which are then registered using an optimization algorithm based on the Euclidean transformation model and non-linear least squares. As a final output, this method produces displacement maps in the tangential and normal directions to the crack skeleton. To demonstrate its performance, we validate our methodology first with synthetic crack patterns and then with real crack patterns. Because this methodology for determining crack openings requires only simple data (just a binary crack pattern image), it is straightforward, robust, and adaptable, thus contributing to the development of structural image-based damage assessments. The computational codes and datasets are available to the public for future research and benchmarking on ",
  "year": 2022,
  "modalities": ["Ground RGB"],
  "tasks": ["Semantic segmentation", "Image-based kinematics"],
  "applications": ["Structural Condition Monitoring", "Crack kinematics"],
  "license": "GPL-3.0", 
  "paper_url": "https://doi.org/10.1016/j.conbuildmat.2022.128054",
  "code_url": "https://github.com/eesd-epfl/crack_kinematics",
  "thumbnail": "assets/img/models/crack_kinematics.png",
  "added_date": "2025-11-15",
  "contributor": "DISC Lab",
  "contributor_url": "https://disclab.pantoja-rosero.com/"
},
{
  "id": "topo_crack_detection",
  "title": "TOPO-Loss for continuity-preserving crack detection using deep learning",
  "authors": ["B.G. Pantoja-Rosero", "D. Oner", "M. Kozinski", "R. Achanta", "P. Fua", "F. Perez-Cruz", "K. Beyer"],
  "abstract":"We present a method for segmenting cracks in images of masonry buildings damaged by earthquakes. Existing methods of crack detection fail to preserve the continuity of cracks, and their performance deteriorates with imprecise training labels. We address these problems by adapting an approach previously proposed for reconstructing roads in aerial images, in which a Convolutional Neural Network is trained with a loss function specifically designed to encourage the continuity of thin structures and to accommodate imprecise annotations. We evaluate combinations of three loss functions (the Mean Squared Error, the Dice loss and the new connectivity-oriented loss) on two datasets using TernausNet, a deep network shown to attain state-of-the-art accuracy in crack detection. We herein show that combining these three losses significantly improves the topology of the predictions quantitatively and qualitatively. We also propose a new continuity metric, named Cracks Per Patch (CPP), and share a new dataset of images of earthquake-affected urban scenes accompanied by crack annotations. The dataset and implementations are publicly available for future studies and benchmarking (https://github.com/eesd-epfl/topo_crack_detection and https://doi.org/10.5281/zenodo.6769028).",
  "year": 2022,
  "modalities": ["Ground RGB"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Structural Condition Monitoring"],
  "license": "GPL-3.0", 
  "paper_url": "https://doi.org/10.1016/j.conbuildmat.2022.128264",
  "code_url": "https://github.com/eesd-epfl/topo_crack_detection",
  "thumbnail": "assets/img/models/topo_crack_detection.png",
  "added_date": "2025-11-16",
   "contributor": "DISC Lab",
  "contributor_url": "https://disclab.pantoja-rosero.com/"
},
{
  "id": "FEM_buildings",
  "title": "Automated image-based generation of finite element models for masonry buildings",
  "authors": ["Bryan German Pantoja-Rosero", "Radhakrishna Achanta", "Katrin Beyer"],
  "abstract": "To predict the response of masonry buildings to various types of loads, engineers use finite element models, specifically solid-element and macro-element models. For predicting masonry responses to seismic events in particular, equivalent frame models—a subcategory of macro-element models—are a common choice because of their low computational cost. However, an existing bottleneck in modeling pipelines is generating the geometry of the model, which is currently a slow and laborious process that is done manually using computer-aided design tools. In this paper, we address this by automating the modelling process using recent advancements in computer vision and machine learning. We present an image-based end-to-end pipeline that automatically generates finite element meshes for solid-element and equivalent-frame models of the outer walls of free-standing historical masonry buildings. As the input, our framework requires RGB images of the buildings that are processed using structure-from-motion algorithms, which create 3D geometries, and convolutional neural networks, which segment the openings and their corners. These layers are then combined to generate level of detail models. We tested our pipeline on structures with irregular surface geometries and opening layouts. While generating the solid element mesh from the level of detail model is straightforward, generating equivalent frame models required algorithms for segmenting the façade and the meshing. Experts in the field analyzed the generated equivalent frame models and determined them to be useful for numerical modeling. These finite element geometries will be invaluable for future predictions of the seismic response of damaged and undamaged buildings. The codes and dataset are publicly available for future studies and benchmarking (https://github.com/eesd-epfl/FEM_buildings and https://doi.org/10.5281/zenodo.8094306).",
  "year": 2023,
  "modalities": ["Ground RGB"],
  "tasks": ["Finite element model generation", "Structure from motion"],
  "applications": ["Structural Condition Monitoring"],
  "license": "GPL-3.0", 
  "paper_url": "https://doi.org/10.1007/s10518-023-01726-7",
  "code_url": "https://github.com/eesd-epfl/FEM_buildings",
  "thumbnail": "assets/img/models/FEM_buildings.png",
  "added_date": "2025-11-17",
  "contributor": "DISC Lab",
  "contributor_url": "https://disclab.pantoja-rosero.com/"
},
{
  "id": "stone_masonry_GDT",
  "title": "Image-based geometric digital twinning for stone masonry elements",
  "authors": ["Bryan German Pantoja-Rosero", "Savvas Saloustros", "Radhakrishna Achanta", "Katrin Beyer"],
  "abstract":"We present an image-based pipeline for generating geometrical digital twins (GDTs) of stone masonry elements with detail down to the stone level. For this purpose, we acquire RGB images of the individual stones and of the wall during the construction phase. In our framework, we use structure from motion (SfM) to first generate 3D source and destination models, which are then registered to form the GDT through non-linear least squares and 2D point feature correspondences detected on the SfM images. This method contrasts with traditional techniques that register point clouds using 3D point descriptors. Because of the robustness of image feature descriptors, we found that using 2D instead of 3D point features facilitates the automation of the GDT generation. To benchmark our algorithm, we compared the results through an Euclidean–distance-based proposed metric with a known 3D textured model from which images were synthetically generated. We show the robustness and feasibility of our method for full size elements, wherein GDTs were generated for dry-stone and stone-mortar systems. This study allows researchers to produce accurate representations of the 3D geometry of walls built for experimental research, reducing therefore uncertainties related to the stone size, shape and arrangement to a minimum when comparing 3D numerical simulations of these walls to experimental results. Codes and data sets are publicly available (https://github.com/eesd-epfl/stone_masonry_GDT and https://doi.org/10.5281/zenodo.7266587).",
  "year": 2023,
  "modalities": ["Ground RGB"],
  "tasks": ["3D registration", "Structure from motion"],
  "applications": ["Digital twin generation"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104632",
  "code_url": "https://github.com/eesd-epfl/stone_masonry_GDT",
  "thumbnail": "assets/img/models/stone_masonry_GDT.gif",
  "added_date": "2025-11-17",
  "contributor": "DISC Lab",
  "contributor_url": "https://disclab.pantoja-rosero.com/"
},
  {
    "id": "VideoCAD",
    "title": "VideoCADFormer: A Model for Learning Long-Horizon 3D CAD UI Interactions from Video",
    "authors": ["Brandon Man", "Ghadi Nehme", "Md Ferdous Alam", "Faez Ahmed"],
	"abstract":"Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt to model UI interactions for precision engineering tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order-of-magnitude increase in complexity for real-world engineering UI tasks, with time horizons up to 20x longer than those in other datasets. We show two important downstream applications of VideoCAD: (1) learning UI interactions from professional 3D CAD tools for precision tasks and (2) a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models (LLMs) on spatial reasoning and video understanding. To learn the UI interactions, we propose VideoCADFormer, a state-of-the-art model for learning CAD interactions directly from video, which outperforms existing behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.",
    "year": 2025,
    "modalities": ["Video clips"],
    "tasks": ["CAD Generation",
	"Behavior cloning","Video QA"],
    "applications": ["Computer-Aided Design", "Video-based UI understanding"],
    "license": "Unspecified",
    "paper_url": "https://arxiv.org/abs/2505.24838",
    "code_url": "https://github.com/ghadinehme/VideoCAD",
    "thumbnail": "assets/img/models/VideoCAD.gif",
    "added_date": "2025-11-26",
  "contributor": "DeCoDE",
  "contributor_url": "https://decode.mit.edu/"
  },
  {
    "id": "KG-LLM-inspection-work-packaging",
    "title": "A KG-LLM-Enabled Inspection Work Packaging Approach in Module Prefabrication",
    "authors": ["Qing Dong", "Xiao Li", "Guohao Liu", "Chengke Wu", "Taicong Chen"],
  "abstract":"Quality inspection during the production of modular construction (MC) is critical to the success of MC projects because defects such as geometric errors can severely impact installation. Therefore, the effective organization and management of quality inspection is essential. Although work packaging has proven to be an effective method for project planning, automatically and accurately generating inspection work packages remains a challenge, particularly in mass production. To this end, this study aims to develop a knowledge graph-large language model (KG-LLM)-enabled inspection work packaging approach in MC production. First, a KG is constructed to represent inspection knowledge, encompassing inspection tasks, target MC products, inspectors, inspection tools, inspection frequencies, and corresponding production processes, which can enrich LLM in product2inspection mapping and can help identify features of inspection tasks for inspection2package clustering. Second, a prompt-based product2inspection mapping approach leveraging LLM is developed. Next, inspection2package clustering is completed through hierarchical agglomerative clustering, with parameters optimized by particle swarm optimization (PSO). Finally, the effectiveness of the proposed approach is validated through controlled experiments on an MC case project. This study demonstrates how leveraging KG and LLMs can enhance collaboration in quality inspection during module prefabrication and highlights the potential of LLMs in the construction domain.",
    "year": 2025,
    "modalities": ["Text"],
    "tasks": ["knowledge graph construction", "Semantic mapping"],
    "applications": ["Work Package Generation", "Quality control"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-6595",
    "code_url": "https://github.com/CI3LAB/KG-LLM-inspection-work-packaging",
    "thumbnail": "assets/img/models/KG-LLM-inspection-work-packaging.png",
    "added_date": "2025-12-05",
  "contributor": "CI3Lab",
  "contributor_url": "https://cicubelab.civil.hku.hk/"
  },
  {
    "id": "EnergyPlus-MCP",
    "title": "EnergyPlus-MCP: A model-context-protocol server for AI-driven building energy modeling",
    "authors": ["Han Li", "Yujie Xu", "Tianzhen Hong"],
  "abstract":"Traditional building energy modeling with the EnergyPlus building performance simulation engine requires domain expertise, programming skills, and intensive manual efforts limiting its effective adoption. This paper introduces EnergyPlus-MCP, the first open-source Model Context Protocol (MCP) server specifically designed for EnergyPlus simulation workflows, establishing a new foundational infrastructure for AI-driven building energy modeling. The MCP server implements a layered architecture with 35 specialized tools spanning model management, editing and analysis, HVAC and other systems configuration inspection, and simulation execution, enabling Large Language Models to interact with EnergyPlus through conversational interfaces. The server addresses critical workflow barriers by automating model validation, streamlining energy efficiency measures modification, and providing intelligent output management with interactive visualization. Through practical demonstrations using a multi-zone building retrofit analysis, we show how the EnergyPlus-MCP server significantly reduces manual efforts while maintaining full simulation rigor. By providing accessible natural language interfaces to sophisticated building energy analysis, this approach enables scalable deployment of simulation expertise across public and private organizations, educational institutions, and research teams, fundamentally transforming traditional building energy modeling practices.",
    "year": 2025,
    "modalities": ["Text"],
    "tasks": ["Model Context Protocol"],
    "applications": ["Building Energy Analysis"],
    "license": "Modified BSD",
    "paper_url": "https://doi.org/10.1016/j.softx.2025.102367",
    "code_url": "https://github.com/LBNL-ETA/EnergyPlus-MCP",
    "thumbnail": "assets/img/models/EnergyPlus-MCP.png",
    "added_date": "2025-12-08",
  "contributor": "Lawrence Berkeley National Laboratory",
  "contributor_url": "https://www.lbl.gov/"
  },
  {
    "id": "DUSt3R",
    "title": "DUSt3R: Geometric 3D Vision Made Easy",
    "authors": ["Shuzhe Wang", "Vincent Leroy", "Yohann Cabon", "Boris Chidlovskii", "Jerome Revaud"],
  "abstract":"Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.",    
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["3D reconstruction"],
    "applications": ["Building Model Generation"],
    "license": "CC BY-NC-SA 4.0",
    "paper_url": "https://arxiv.org/abs/2312.14132",
    "code_url": "https://github.com/naver/dust3r",
    "thumbnail": "assets/img/models/DUSt3R.png",
    "added_date": "2025-12-09"
  },
  {
    "id": "VoxCity",
    "title": "VoxCity: Grid-based semantic 3D city model generation, and urban environment simulation",
    "authors": ["Kunihiko Fujiwara","Ryuta Tsurumi","Tomoki Kiyono","Zicheng Fan","Xiucheng Liang","Binyu Lei","Winston Yap","Koichi Ito","Filip Biljecki"],
  "abstract":"Three-dimensional urban environment simulation is a powerful tool for informed urban planning. However, the intensive manual effort required to prepare input 3D city models has hindered its widespread adoption. To address this challenge, we present VoxCity, an open-source Python package that provides a one-stop solution for grid-based 3D city model generation and urban environment simulation for cities worldwide. VoxCity’s ‘generator’ subpackage automatically downloads building heights, tree canopy heights, land cover, and terrain elevation within a specified target area, and voxelizes buildings, trees, land cover, and terrain to generate an integrated voxel city model. The ‘simulator’ subpackage enables users to conduct environmental simulations, including solar radiation and view index analyses. Users can export the generated models using several file formats compatible with external software, such as ENVI-met (INX), Blender, and Rhino (OBJ). We generated 3D city models for eight global cities, and demonstrated the calculation of solar irradiance, sky view index, and green view index. We also showcased microclimate simulation and 3D rendering visualization through ENVI-met and Rhino, respectively, through the file export function. Additionally, we reviewed openly available geospatial data to create guidelines to help users choose appropriate data sources depending on their target areas and purposes. VoxCity can significantly reduce the effort and time required for 3D city model preparation and promote the utilization of urban environment simulations. This contributes to more informed urban and architectural design that considers environmental impacts, and in turn, fosters sustainable and livable cities. VoxCity is released openly at https://github.com/kunifujiwara/VoxCity",
    "year": 2026,
    "modalities": ["Point cloud", "3D model"],
    "tasks": ["3D reconstruction", "3D rendering"],
    "applications": ["City Model Generation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.compenvurbsys.2025.102366",
    "code_url": "https://github.com/kunifujiwara/VoxCity",
    "thumbnail": "https://raw.githubusercontent.com/kunifujiwara/VoxCity/main/images/concept.png",
    "added_date": "2026-01-07",
    "contributor": "Urban Analytics Lab",
  "contributor_url": "https://ual.sg/"
  },
    {
    "id": "Text2BIM",
    "title": "Text2BIM: Generating Building Models Using a Large Language Model-Based Multiagent Framework",
    "authors": ["Changyu Du", "Sebastian Esser", "Stavros Nousias", "André Borrmann"],
	"abstract":"The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (architecture, engineering, and construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multiagent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool’s APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that the proposed approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. The code is open source.",
    "year": 2025,
    "modalities": ["Text", "BIM"],
    "tasks": ["Text-to-BIM"],
    "applications": ["Conceptual design"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-6386",
    "code_url": "https://github.com/dcy0577/Text2BIM",
    "thumbnail": "assets/img/models/Text2BIM.gif",
    "added_date": "2026-01-10",
    "contributor": "Changyu Du",
  "contributor_url": "https://github.com/dcy0577"
  },
   {
    "id": "BIMgent",
    "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents",
    "authors": ["Zihan Deng", "Changyu Du", "Stavros Nousias", "André Borrmann"],
	"abstract": "Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios. Code available at: https://github.com/ZihanDDD/BIMgent",
    "year": 2025,
    "modalities": ["Text", "RGB", "BIM"],
    "tasks": [
    "Autonomous BIM Authoring",
    "Text-to-BIM",
    "Floorplan-to-BIM",
    "GUI-Based Workflow Planning"
	],
    "applications": ["Conceptual design"],
    "license": "GPL-3.0",
    "paper_url": "https://arxiv.org/abs/2506.07217",
    "code_url": "https://github.com/ZihanDDD/BIMgent",
    "thumbnail": "assets/img/models/BIMgent.gif",
    "added_date": "2026-01-10",
    "contributor": "Zihan Deng",
  "contributor_url": "https://github.com/ZihanDDD"
  },
     {
    "id": "BIM-Command-Recommendation",
    "title": "Predictive modeling: BIM command recommendation based on large-scale usage logs",
    "authors": ["Changyu Du", "Zihan Deng","Stavros Nousias", "André Borrmann"],
	"abstract": "The adoption of Building Information Modeling (BIM) and model-based design within the Architecture, Engineering, and Construction (AEC) industry has been hindered by the perception that using BIM authoring tools demands more effort than conventional 2D drafting. To enhance design efficiency, this paper proposes a BIM command recommendation framework that predicts the optimal next actions in real-time based on users’ historical interactions. We propose a comprehensive filtering and enhancement method for large-scale raw BIM log data and introduce a novel command recommendation model. Our model builds upon the state-of-the-art Transformer backbones originally developed for large language models (LLMs), incorporating a custom feature fusion module, dedicated loss function, and targeted learning strategy. In a case study, the proposed method is applied to over 32 billion rows of real-world log data collected globally from the BIM authoring software Vectorworks. Experimental results demonstrate that our method can learn universal and generalizable modeling patterns from anonymous user interaction sequences across different countries, disciplines, and projects. When generating recommendations for the next command, our approach achieves a Recall@10 of approximately 84%. The code is available at: https://github.com/dcy0577/BIM-Command-Recommendation.git.",
    "year": 2025,
    "modalities": ["BIM Interaction Logs"],
	"tasks": [
	  "BIM Command Recommendation",
	  "Next-Action Prediction",
	  "Sequential recommendation"
	],
    "applications": ["BIM authoring assistance"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.aei.2025.103574",
    "code_url": "https://github.com/dcy0577/BIM-Command-Recommendation",
    "thumbnail": "assets/img/models/BIM-Command-Recommendation.gif",
    "added_date": "2026-01-10",
    "contributor": "Changyu Du",
  "contributor_url": "https://github.com/dcy0577"
  },
    {
    "id": "openBIMdisk",
    "title": "Open BIM exchange on Blockchain 3.0 virtual disk: A traceable semantic differential transaction approach",
    "authors": ["Lingming Kong", "Rui Zhao", "Chimay J. Anumba", "Weisheng Lu", "Fan Xue"],
	"abstract": "Building information modeling (BIM) has become prevalent in construction engineering management. However, the efficiency of traditional file-based BIM exchange between multidisciplinary teams remains low due to the transmission of redundant data from mostly unchanged BIM objects. Additionally, the traceability of changes between BIM files is limited. This paper proposes a traceable semantic differential transaction (tSDT) approach for open BIM exchange, which minimizes data redundancy and enables semantic-level traceability of BIM changes. Furthermore, openBIMdisk implements the tSDT and provides a Blockchain 3.0 virtual disk to support efficient, traceable, and secure BIM exchanges across multiple blockchain services. A pilot study of a modular construction project demonstrated the effectiveness of tSDT and open-BIMdisk. Experimental results indicated that tSDT achieved minimal BIM redundancy for storing and restoring all BIM changes, using a sheer 0.007% of disk space on average. openBIMdisk facilitated BIM version management and object-level semantic traceability with a response time of 5.3 ms. The contributions of this paper are twofold. First, tSDT offers a novel and efficient approach for semantic BIM change traceability. Second, openBIMdisk provides BIM practitioners with a Blockchain 3.0 application featuring intuitive and user-friendly interfaces for BIM exchange.",
    "year": 2025,
    "modalities": [
	"BIM",
    "Transaction Logs"
	],
	"tasks": [
	"Semantic BIM Change Detection"
	],
    "applications": [    
    "Blockchain-Enabled BIM Management",
    "Design Change Auditing",
    "Cross-Platform BIM Data Exchange"
	],
    "license": "Apache 2.0",
    "paper_url": "https://doi.org/10.1007/s42524-024-4006-x",
    "code_url": "https://github.com/KarleKong/openBIMdisk",
    "thumbnail": "assets/img/models/openBIMdisk.png",
    "added_date": "2026-01-10",
    "contributor": "Lingming Kong",
	"contributor_url": "https://github.com/KarleKong"
  },
    {
    "id": "SpaRSE-BIM",
    "title": "SpaRSE-BIM: Classification of IFC-based geometry via sparse convolutional neural networks",
    "authors": ["Christoph Emunds", "Nicolas Pauen", "Veronika Richter", "Jérôme Frisch", "Christoph van Treeck"],
  "abstract": "Information exchange between Building Information Modeling (BIM) tools is challenging, since many applications use their own native data formats. The Industry Foundation Classes (IFC) schema, an open data exchange format for BIM, does not capture the full semantic meaning needed for direct use by different BIM tools and can be prone to information loss due to reduction, simplification, translation and interpretation of the data. Current practice often treats the imported model as a reference and requires a user to remodel the building using the respective application’s native elements. Many BIM object properties are defined by its classification. Inconsistencies in the mapping between native BIM elements and IFC, e.g. due to unsupported export functionality or manual error, can lead to problems when using the model in a downstream application. Recent works demonstrate that neural networks offer a promising possibility to alleviate this issue via classification of the objects contained in a BIM model and suggesting those corrections to the user. However, the computational overhead of these deep learning models, either due to necessary pre-processing of the data or runtime performance of the model, makes it difficult for them to be used in plug-ins or middleware for BIM tools. This work proposes SpaRSE-BIM, a neural network model based on sparse convolutions for the classification of IFC-based geometry and semantic enrichment of BIM models. Experiments are performed on two IFC entity classification benchmark datasets. The results demonstrate that SpaRSE-BIM is significantly more efficient at inference time compared to previous approaches, while maintaining state-of-the-art accuracy. Further experiments explore the applicability of IFC entity classification datasets to the domain of Scan-to-BIM. It can be shown that the feature space of SpaRSE-BIM learns to discern objects in a semantically meaningful way, even in cases where fine-grained subtype information for IFC objects is not available during training.",
    "year": 2022,
    "modalities": ["BIM"],
  "tasks": ["BIM Object Classification"],
    "applications": ["Scan-to-BIM"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.aei.2022.101641",
    "code_url": "https://github.com/RWTH-E3D/ifcnet-models",
    "thumbnail": "assets/img/models/SpaRSE-BIM.png",
    "added_date": "2026-01-11",
    "contributor": "Christoph Emunds",
  "contributor_url": "https://github.com/cemunds"
  },
      {
    "id": "MultiLabel_SewerDefect_SSL",
    "title": "Self-supervised learning for multi-label sewer defect classification",
    "authors": ["Tugba Yildizli", "Tianlong Jia", "Jeroen Langeveld", "Riccardo Taormina"],
	"abstract": "Automated sewer defect detection has advanced through deep learning, particularly supervised methods using CCTV images, but based on large annotated datasets. This paper proposes a semi-supervised learning (SSL) approach to reduce labeling demands. The method comprises self-supervised pre-training on unlabeled images using SwAV (Swapping Assignments between multiple Views) followed by fine-tuning for multi-label classification. Experiments on the Sewer-ML dataset demonstrate that the SSL approach, trained on only 35k labeled images, achieves an F1-score of 69.11%, and F2CIW of 54.22%, surpassing the fully supervised baseline trained from scratch on 1.04 million images. Increasing the unlabeled pre-training data further enhances performance, while ImageNet initialization consistently outperforms training from scratch. Self-supervised learning also helps mitigate the effects of mislabeled data, which is observed to be present even in the Sewer-ML ground truth. Overall, self-supervised learning provides an accurate, scalable, and cost-effective alternative to fully supervised approaches, particularly in data-scarce or imperfectly labeled scenarios.",
    "year": 2026,
    "modalities": ["Ground RGB"],
  "tasks": ["Sewer defect classification"],
    "applications": ["Asset management"],
    "license": "CC BY-NC-SA 4.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106751",
    "code_url": "https://github.com/tubayildizli/MultiLabel_SewerDefect_SSL",
    "thumbnail": "assets/img/models/MultiLabel_SewerDefect_SSL.png",
    "added_date": "2026-01-17",
    "contributor": "Tugba Yildizli",
    "contributor_url": "https://www.tudelft.nl/en/staff/t.yildizli/"
  },
  {
  "id": "BDAChat",
  "title": "BDAChat: Temporal Vision-Language Model for Object-Level Building Damage Assessment",
  "authors": [
    "Yong Wang",
    "Jiawei Cui",
    "Changhai Zhai",
    "Xigui Tao",
    "Yuhao Li"
  ],
  "abstract": "The rapid assessment of constructed facilities after extreme events is a knowledge-intensive task critical for effective emergency management. However, methodologies for automated, object-level damage assessment at scale remain underdeveloped, often lacking fine-grained interpretability or scalability. This paper introduces a framework that integrates instance segmentation with temporal Vision Language Model (VLM), which is empowered with visual damage reasoning capabilities through fine-tuning on domain-specific knowledge, for the automated and interpretable assessment of structural assets from satellite imagery. Our three-stage approach synergizes: high-precision segmentation via a modified Segment Anything Model (SAM); spatiotemporal data pairing to isolate asset-specific changes; and BDAChat, the first temporal VLM fine-tuned for object-level damage assessment. Unlike traditional black-box models, BDAChat provides both high-accuracy damage classification and causal interpretations, serving as an intelligent damage inference system. The framework’s effectiveness and scalability are validated through the Lahaina wildfire and hurricane Ian case study. This modular framework automates and accelerates the object-level building damage assessment process, demonstrating significant potential for real-time building damage evaluation and resilient infrastructure planning. The code and dataset are available at https://github.com/WangYong921/BDAChat.",
  "year": 2026,
  "modalities": [
    "Satellite RGB"
  ],
  "tasks": [
    "Disaster recognition",
    "Damage classification",
    "Visual question answering"
  ],
  "applications": [
    "Post-disaster damage assessment"
  ],
  "license": "MIT",
  "paper_url": "https://doi.org/10.1016/j.aei.2026.104320",
  "code_url": "https://github.com/WangYong921/BDAChat",
  "thumbnail": "assets/img/models/BDAChat.png",
  "added_date": "2026-01-20",
  "contributor": "Urban Infrastructure Safety and Resilience Research Center",
  "contributor_url": "https://homepage.hit.edu.cn/zhaichanghai?lang=en"
}

]
