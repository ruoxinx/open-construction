[
  {
    "id": "GDUT-HWD",
    "title": "Automatic detection of hardhats worn by construction personnel: A deep learning approach and benchmark dataset",
    "authors": ["Jixiu Wu", "Nian Cai", "Wenjie Chen", "Huiheng Wang", "Guotian Wang"],
    "abstract": "Hardhats play an essential role in protecting construction individuals from accidents. However, wearing hardhats is not strictly enforced among workers due to all kinds of reasons. To enhance construction sites safety, the majority of existing works monitor the presence and proper use of hardhats through multi-stage data processing, which come with limitations on adaption and generalizability. In this paper, a one-stage system based on convolutional neural network is proposed to automatically monitor whether construction personnel are wearing hardhats and identify the corresponding colors. To facilitate the study, this work constructs a new and publicly available hardhat wearing detection benchmark dataset, which consists of 3174 images covering various on-site conditions. Then, features from different layers with different scales are fused discriminately by the proposed reverse progressive attention to generate a new feature pyramid, which will be fed into the Single Shot Multibox Detector (SSD) to predict the final detection results. The proposed system is trained by an end-to-end scheme. The experimental results demonstrate that the proposed system is effective under all kinds of on-site conditions, which can achieve 83.89% mAP (mean average precision) with the input size 512×512.",
    "year": 2019,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Apache-2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2019.102894",
    "code_url": "https://github.com/wujixiu/helmet-detection",
    "thumbnail": "assets/img/models/GDUT-HWD.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Construction-Activity-Scenes",
    "title": "Manifesting construction activity scenes via image captioning",
    "authors": ["Huan Liu", "Guangbin Wang", "Ting Huang", "Ping He", "Martin Skitmore", "Xiaochun Luo"],
    "abstract": "This study proposed an automated method for manifesting construction activity scenes by image captioning – an approach rooted in computer vision and natural language generation. A linguistic description schema for manifesting the scenes is developed initially and two unique dedicated image captioning datasets are created for method validation. A general model architecture of image captioning is then instituted by combining an encoder-decoder framework with deep neural networks, followed by three experimental tests involving the selection of model learning strategies and performance evaluation metrics. It is demonstrated the method's performance is comparable with that of state-of-the-art computer vision methods in general. The paper concludes with a discussion of the feasibility of the practical application of the proposed approach at the current technical level.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103334",
    "code_url": "https://github.com/HannahHuanLIU/AEC-image-captioning",
    "thumbnail": "assets/img/models/Construction-Activity-Scenes.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Kenki-Posi",
    "title": "Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects",
    "authors": ["Runqiu Bao","Ren Komatsu","Renato Miyagusuku","Masaki Chino", "Atsushi Yamashita", "Hajime Asama"],
    "abstract": "At modern construction sites, utilizing GNSS (Global Navigation Satellite System) to measure the real-time location and orientation (i.e. pose) of construction machines and navigate them is very common. However, GNSS is not always available. Replacing GNSS with on-board cameras and visual simultaneous localization and mapping (visual SLAM) to navigate the machines is a cost-effective solution. Nevertheless, at construction sites, multiple construction machines will usually work together and side-by-side, causing large dynamic occlusions in the cameras' view. Standard visual SLAM cannot handle large dynamic occlusions well. In this work, we propose a motion segmentation method to efficiently extract static parts from crowded dynamic scenes to enable robust tracking of camera ego-motion. Our method utilizes semantic information combined with object-level geometric constraints to quickly detect the static parts of the scene. Then, we perform a two-step coarse-to-fine ego-motion tracking with reference to the static parts. This leads to a novel dynamic visual SLAM formation. We test our proposals through a real implementation based on ORB-SLAM2, and datasets we collected from real construction sites. The results show that when standard visual SLAM fails, our method can still retain accurate camera ego-motion tracking in real-time. Comparing to state-of-the-art dynamic visual SLAM methods, ours shows outstanding efficiency and competitive result trajectory accuracy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site navigation"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1080/01691864.2020.1869586",
    "code_url": "https://github.com/RunqiuBao/kenki-positioning-vSLAM",
    "thumbnail": "assets/img/models/Kenki-Posi.png",
	"added_date": "2025-09-26"
  }
]
