[
  {
    "id": "GDUT-HWD",
    "title": "Automatic detection of hardhats worn by construction personnel: A deep learning approach and benchmark dataset",
    "authors": ["Jixiu Wu", "Nian Cai", "Wenjie Chen", "Huiheng Wang", "Guotian Wang"],
    "abstract": "Hardhats play an essential role in protecting construction individuals from accidents. However, wearing hardhats is not strictly enforced among workers due to all kinds of reasons. To enhance construction sites safety, the majority of existing works monitor the presence and proper use of hardhats through multi-stage data processing, which come with limitations on adaption and generalizability. In this paper, a one-stage system based on convolutional neural network is proposed to automatically monitor whether construction personnel are wearing hardhats and identify the corresponding colors. To facilitate the study, this work constructs a new and publicly available hardhat wearing detection benchmark dataset, which consists of 3174 images covering various on-site conditions. Then, features from different layers with different scales are fused discriminately by the proposed reverse progressive attention to generate a new feature pyramid, which will be fed into the Single Shot Multibox Detector (SSD) to predict the final detection results. The proposed system is trained by an end-to-end scheme. The experimental results demonstrate that the proposed system is effective under all kinds of on-site conditions, which can achieve 83.89% mAP (mean average precision) with the input size 512×512.",
    "year": 2019,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Apache-2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2019.102894",
    "code_url": "https://github.com/wujixiu/helmet-detection",
    "thumbnail": "assets/img/models/GDUT-HWD.png"
  },
  {
    "id": "Construction-Activity-Scenes",
    "title": "Manifesting construction activity scenes via image captioning",
    "authors": ["Huan Liu", "Guangbin Wang", "Ting Huang", "Ping He", "Martin Skitmore", "Xiaochun Luo"],
    "abstract": "This study proposed an automated method for manifesting construction activity scenes by image captioning – an approach rooted in computer vision and natural language generation. A linguistic description schema for manifesting the scenes is developed initially and two unique dedicated image captioning datasets are created for method validation. A general model architecture of image captioning is then instituted by combining an encoder-decoder framework with deep neural networks, followed by three experimental tests involving the selection of model learning strategies and performance evaluation metrics. It is demonstrated the method's performance is comparable with that of state-of-the-art computer vision methods in general. The paper concludes with a discussion of the feasibility of the practical application of the proposed approach at the current technical level.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103334",
    "code_url": "https://github.com/HannahHuanLIU/AEC-image-captioning",
    "thumbnail": "assets/img/models/Construction-Activity-Scenes.png"
  },
  {
    "id": "Kenki-Posi",
    "title": "Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects",
    "authors": ["Runqiu Bao","Ren Komatsu","Renato Miyagusuku","Masaki Chino", "Atsushi Yamashita", "Hajime Asama"],
    "abstract": "At modern construction sites, utilizing GNSS (Global Navigation Satellite System) to measure the real-time location and orientation (i.e. pose) of construction machines and navigate them is very common. However, GNSS is not always available. Replacing GNSS with on-board cameras and visual simultaneous localization and mapping (visual SLAM) to navigate the machines is a cost-effective solution. Nevertheless, at construction sites, multiple construction machines will usually work together and side-by-side, causing large dynamic occlusions in the cameras' view. Standard visual SLAM cannot handle large dynamic occlusions well. In this work, we propose a motion segmentation method to efficiently extract static parts from crowded dynamic scenes to enable robust tracking of camera ego-motion. Our method utilizes semantic information combined with object-level geometric constraints to quickly detect the static parts of the scene. Then, we perform a two-step coarse-to-fine ego-motion tracking with reference to the static parts. This leads to a novel dynamic visual SLAM formation. We test our proposals through a real implementation based on ORB-SLAM2, and datasets we collected from real construction sites. The results show that when standard visual SLAM fails, our method can still retain accurate camera ego-motion tracking in real-time. Comparing to state-of-the-art dynamic visual SLAM methods, ours shows outstanding efficiency and competitive result trajectory accuracy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1080/01691864.2020.1869586",
    "code_url": "https://github.com/RunqiuBao/kenki-positioning-vSLAM",
    "thumbnail": "assets/img/models/Kenki-Posi.gif"
  },
  {
    "id": "CDE",
    "title": "Machine learning using synthetic images for detecting dust emissions on construction sites",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Automated dust monitoring in workplaces helps provide timely alerts to over-exposed workers and effective mitigation measures for proactive dust control. However, the cluttered nature of construction sites poses a practical challenge to obtain enough high-quality images in the real world. The study aims to establish a framework that overcomes the challenges of lacking sufficient imagery data (“data-hungry problem”) for training computer vision algorithms to monitor construction dust. This study develops a synthetic image generation method that incorporates virtual environments of construction dust for producing training samples. Three state-of-the-art object detection algorithms, including Faster-RCNN, you only look once (YOLO) and single shot detection (SSD), are trained using solely synthetic images. Finally, this research provides a comparative analysis of object detection algorithms for real-world dust monitoring regarding the accuracy and computational efficiency. This study creates a construction dust emission (CDE) dataset consisting of 3,860 synthetic dust images as the training dataset and 1,015 real-world images as the testing dataset. The YOLO-v3 model achieves the best performance with a 0.93 F1 score and 31.44 fps among all three object detection models. The experimental results indicate that training dust detection algorithms with only synthetic images can achieve acceptable performance on real-world images. This study provides insights into two questions: (1) how synthetic images could help train dust detection models to overcome data-hungry problems and (2) how well state-of-the-art deep learning algorithms can detect nonrigid construction dust.",
    "year": 2021,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Compliance checking"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1108/SASBE-04-2021-0066",
    "code_url": "https://github.com/ruoxinx/site-dust-detect",
    "thumbnail": "assets/img/models/CDE.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "CPPE",
    "title": "Pose guided anchoring for detecting proper use of personal protective equipment",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Ensuring proper use of personal protective equipment (PPE) is essential for improving workplace safety management. The authors present an extensible pose-guided anchoring framework aimed at multi-class PPE compliance detection. The overall approach harnesses a pose estimator to detect worker body parts as spatial anchors and guide the localization of part attention regions using body-knowledge-based rules considering workers' orientations and object scales. Specifically, “part attention regions” are local image patches expecting PPEs based on their inherent relationships with body parts, e.g., (head, hardhat) and (upper-body, vest). Finally, the shallow CNN-based classifiers can reliably recognize both PPE and non-PPE classes within their corresponding part attention regions. Quantitative evaluations tested on the developed construction personal protective equipment dataset (CPPE) show an overall 0.97 and 0.95 F1-score for hardhat and safety vest detection, respectively. Comparative studies with existing methods also demonstrate the higher detection accuracy and advantageous extensibility of the proposed strategy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2021.103828",
    "code_url": "https://github.com/ruoxinx/PPE-Detection-Pose",
    "thumbnail": "assets/img/models/CPPE.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
   {
    "id": "ConSLAM",
    "title": "ConSLAM: Construction Dataset for SLAM",
    "authors": ["ConSLAM: Construction Dataset for SLAM"],
    "abstract": "This paper presents a data set collected periodically on a construction site. The data set aims to evaluate the performance of simultaneous localization and mapping (SLAM) algorithms used by mobile scanners or autonomous robots. It includes ground-truth scans of a construction site collected using a terrestrial laser scanner along with five sequences of spatially registered and time-synchronized images, lidar scans, and inertial data coming from our prototypical handheld scanner. We also recover the ground-truth trajectory of the mobile scanner by registering the sequential lidar scans to the ground-truth scans and show how to use a popular software package to measure the accuracy of SLAM algorithms against our trajectory automatically. To the best of our knowledge, this is the first publicly accessible data set consisting of periodically collected sequential data on a construction site.",
    "year": 2023,
    "modalities": ["Ground RGB, Point Cloud"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Academic Use Only",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-5212",
    "code_url": "https://github.com/mac137/ConSLAM",
    "thumbnail": "assets/img/models/ConSLAM.png"
  },
  {
    "id": "CMA",
    "title": "Transformer-based deep learning model and video dataset for unsafe action identification in construction projects",
    "authors": ["Meng Yang", "Chengke Wu", "Yuanjun Guo", "Rui Jiang", "Feixiang Zhou", "Jianlin Zhang", "Zhile Yang"],
    "abstract": "A large proportion of construction accidents are caused by unintentional and unsafe actions and behaviors. It is of significant difficulties and ineffectiveness to monitor unsafe behaviors using conventional manual supervision due to the complex and dynamic working conditions on construction sites. Recently, surveillance videos and computer vision (CV) techniques have been increasingly adopted to automatically identify risky behaviors. However, the challenge remains that spatial and temporal features in video clips cannot be effectively captured and fused by current CV models. To address this challenge, this paper describes a deep learning model named Spatial Temporal Relation Transformer (STR-Transformer), where spatial and temporal features of work behaviors are simultaneously extracted in paralleling video streams and then fused by a specially designed module. To verify the effectiveness of the STR-Transformer, a customized dataset is developed, including seven categories of construction worker behaviors and 1595 video clips. In numerical experiments and case studies, the STR-Transformer achieves an average precision of 88.7%, 4.0% higher than the baseline model. The STR-Transformer enables more accurate and reliable automatic safety surveillance on construction projects, and is expected to reduce accident rates and management costs. Moreover, the performance of STR-Transformer relies on efficient feature integration, which may inspire future studies to identify, extract, and fuse richer features when applying CV-based deep learning models in construction management.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Action recognition"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104703",
    "code_url": "https://github.com/S1mpleyang/ConstructionActionRecognition",
    "thumbnail": "assets/img/models/CMA.png"
  },
  {
    "id": "VisualSiteDiary",
    "title": "VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals",
    "authors": ["Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Mani Golparvar-Fard"],
    "abstract": "This paper presents VisualSiteDiary, a Vision Transformer-based image captioning model which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. As a model for deciphering construction photologs, VisualSiteDiary incorporates pseudo-region features, utilizes high-level knowledge in pretraining, and fine-tunes for diverse captioning styles. To validate VisualSiteDiary, a new image captioning dataset, VSD, is presented. This dataset includes many realistic yet challenging cases commonly observed in commercial building projects. Experimental results using five different metrics demonstrate that VisualSiteDiary provides superior-quality captions compared to the state-of-the-art image captioning models. Excluding the task of object recognition, the presented model also outperformed mPLUG –the state-of-the-art visual-language model– in the image retrieval task by 0.6% in precision and 0.9% in recall, respectively. Detailed discussions illustrate practical examples on how VisualSiteDiary improves the process of creating daily construction reports, paving the way for future developments in the field.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105483",
    "code_url": "https://github.com/joonv2/VisualSiteDiary",
    "thumbnail": "assets/img/models/VisualSiteDiary.gif"
  },
  {
    "id": "Small-Construction-Tools",
    "title": "Utilizing synthetic images to enhance the automated recognition of small-sized construction tools",
    "authors": ["Soeun Han", "Wonjun Park", "Kyumin Jeong", "Taehoon Hong", "Choongwan Koo"],
    "abstract": "Previous studies on vision-based classifiers often overlooked the need for detecting small-sized construction tools. Considering the substantial variations in these tools' size and shape, it is essential to train models using synthetic images that encompass diverse angles and distances. This study aimed to improve the performance of classifiers for small-sized construction tools by leveraging synthetic data. Three classifiers were proposed using YOLOv8 algorithm, varying in data composition: (i) ‘Real-4000’: 4000 authentic images; (ii) ‘Hybrid-4000’: 2000 authentic and 2000 synthetic images; (iii) ‘Hybrid-8000’: 4000 authentic and 4000 synthetic images. To assess practical applicability, a test dataset of 144 samples for each type was collected directly from construction sites. Results revealed that the ‘Hybrid-8000’ model, utilizing synthetic images, excelled at 94.8% of mAP_0.5. This represented a significant 15.2% improvement, affirming its practical applicability. These classifiers hold promise for enhancing safety and advancing real-time automation and robotics in construction.",
    "year": 2024,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105415",
    "code_url": "https://github.com/SenseableSpace/Detection-Small-Construction-Tools-Synthetic-Images",
    "thumbnail": "assets/img/models/Small-Construction-Tools.png"
  },
  {
    "id": "OSHA-safety",
    "title": "Deep Learning Enabled Computer Vision Model for Automated Safety Compliance in Construction Environments",
    "authors": ["Amr A. Mohy", "Hesham A. Bassioni", "Elbadr O. Elgendi", "Tarek M. Hassan"],
    "abstract": "Construction site safety demands proactive hazard detection, a challenge traditionally met with reactive measures that are often inadequate. This paper introduces a novel deep learning-based computer vision model designed for automated safety compliance monitoring, addressing critical limitations of existing approaches. The model utilizes a modified one-stage object detection algorithm, uniquely enhanced with Contextual Transformer Networks (CoTs), a Triplet Attention module, Activate or Not (ACON) activation functions, and Content-Aware Reassembly of Features (CARAFE) up-sampling, to significantly improve feature extraction, visual recognition, and contextual understanding in complex construction environments. To support this model development, a new OSHA-data-driven dataset of 55,594 images across 28 safety categories was developed. This dataset encompasses personal protective equipment (PPE), scaffolding, materials, hazards, and worker actions, ensuring comprehensive coverage of key safety domains. The Wise-Intersection over Union (IoU) loss function further refines bounding box regression, enhancing localization accuracy. Evaluations on both a benchmarking dataset and the newly developed dataset demonstrate the model's benchmark-surpassing performance (Precision: 0.89, mAP95: 0.45). This research offers a practically viable, data-driven solution for a critical industry challenge, moving towards a future of zero-accident construction sites.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "http://dx.doi.org/10.36680/j.itcon.2025.057",
    "code_url": "https://github.com/amr21006/PhD_Safety_Management_Object_Detection",
    "thumbnail": "assets/img/models/OSHA-safety.png"
  },
    {
    "id": "repetitive-action",
    "title": "Automatic repetitive action counting for construction worker ergonomic assessment",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Work-related musculoskeletal disorders are the primary cause of nonfatal occupational injuries in the construction industry. Accurate ergonomic assessment is essential to reduce the risk of work-related injuries. Repetitive work significantly contributes to musculoskeletal injuries, and various ergonomic evaluation methods have specific criteria for assessing repetitive actions. However, most existing methods for repetitive motions primarily rely on subjective and time-consuming manual observation. To accurately assess ergonomic risk, an automatic and precise method is required to count repetitive actions in construction work. This poses a challenge due to the unstructured nature of construction actions and their varying frequencies and cycles. This paper aims to overcome these challenges by identifying repetitive unstructured actions using posture self-similarity comparison and predicting construction actions' length using a transformer layer. Experimental results demonstrated that the proposed method achieved a 91.5 % accuracy in identifying repetitive actions. The research results will contribute to the promotion of accurate ergonomics evaluation of automation.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Pose estimation"],
	"applications": ["Ergonomic assessment"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105726",
    "code_url": "https://github.com/Chenxy875/Repetitive-Actions-Counting-Method-for-Construction-Workers-Ergonomic-Assessments",
    "thumbnail": "assets/img/models/repetitive-action.gif"
  },
  {
    "id": "Pictor-v3",
    "title": "Deep Learning for Site Safety: Real-Time Detection of Personal Protective Equipment",
    "authors": ["Nipun D. Nath", "Amir H. Behzadan", "Stephanie G. Paal"],
    "abstract": "The leading causes of construction fatalities include traumatic brain injuries (resulted from fall and electrocution) and collisions (resulted from struck by objects). As a preventive step, the U.S. Occupational Safety and Health Administration (OSHA) requires that contractors enforce and monitor appropriate usage of personal protective equipment (PPE) of workers (e.g., hard hat and vest) at all times. This paper presents three deep learning (DL) models built on You-Only-Look-Once (YOLO) architecture to verify PPE compliance of workers; i.e., if a worker is wearing hard hat, vest, or both, from image/video in real-time. In the first approach, the algorithm detects workers, hats, and vests and then, a machine learning model (e.g., neural network and decision tree) verifies if each detected worker is properly wearing hat or vest. In the second approach, the algorithm simultaneously detects individual workers and verifies PPE compliance with a single convolutional neural network (CNN) framework. In the third approach, the algorithm first detects only the workers in the input image which are then cropped and classified by CNN-based classifiers (i.e., VGG-16, ResNet-50, and Xception) according to the presence of PPE attire. All models are trained on an in-house image dataset that is created using crowd-sourcing and web-mining. The dataset, named Pictor-v3, contains ~1,500 annotated images and ~4,700 instances of workers wearing various combinations of PPE components. It is found that the second approach achieves the best performance, i.e., 72.3% mean average precision (mAP), in real-world settings, and can process 11 frames per second (FPS) on a laptop computer which makes it suitable for real-time detection, as well as a good candidate for running on light-weight mobile devices. The closest alternative in terms of performance (67.93% mAP) is the third approach where VGG-16, ResNet-50, and Xception classifiers are assembled in a Bayesian framework. However, the first approach is the fastest among all and can process 13 FPS with 63.1% mAP. The crowed-sourced Pictor-v3 dataset and all trained models are publicly available to support the design and testing of other innovative applications for monitoring safety compliance, and advancing future research in automation in construction.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103085",
    "code_url": "https://github.com/ciber-lab/pictor-ppe",
    "thumbnail": "assets/img/models/Pictor-v3.gif"
  },
    {
    "id": "ABECIS",
    "title": "Developing a Free and Open-Source Semi-Automated Building Exterior Crack Inspection Software for Construction and Facility Managers",
    "authors": ["Nipun D. Nath", "Amir H. Behzadan", "Stephanie G. Paal"],
	"abstract":"Inspection of cracks is an important process for properly monitoring and maintaining a building. However, manual crack inspection is time-consuming, inconsistent, and dangerous (e.g., in tall buildings). Due to the development of open-source AI technologies, the increase in available Unmanned Aerial Vehicles (UAVs) and the availability of smartphone cameras, it has become possible to automate the building crack inspection process. This study presents the development of an easy-to-use, free and open-source Automated Building Exterior Crack Inspection Software (ABECIS) for construction and facility managers, using state-of-the-art segmentation algorithms to identify concrete cracks and generate a quantitative and qualitative report. ABECIS was tested using images collected from a UAV and smartphone cameras in real-world conditions and a controlled laboratory environment. From the raw output of the algorithm, the median Intersection over Unions (IoU) for the test experiments are (1) 0.686 for indoor crack detection experiment in a controlled lab environment using a commercial drone, (2) 0.186 for indoor crack detection at a construction site using a smartphone and (3) 0.958 for outdoor crack detection on university campus using a commercial drone. These IoU results can be improved significantly to over 0.8 when a human operator selectively removes the false positives. In general, ABECIS performs best for outdoor drone images, and combining the algorithm predictions with human verification/intervention offers very accurate crack detection results. The software is available publicly and can be downloaded for out-of-the-box use.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection","Semantic segmentation"],
	"applications": ["Quality control"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1109/ACCESS.2023.3296793",
    "code_url": "https://github.com/SMART-NYUAD/ABECIS",
    "thumbnail": "assets/img/models/ABECIS.png"
  },
    {
    "id": "MGCAR",
    "title": "Multi-granular crew activity recognition for construction monitoring",
    "authors": ["Cheng Yun Tsai", "Mik Wanul Khosiin", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "The labor force is vital to construction projects, but traditional manual methods for productivity analysis are time-consuming and error-prone. Recent advancements in computer vision and deep learning offer automated solutions, yet most studies focus on low-level pose recognition, neglecting the collaborative dynamics of construction sites. This paper introduces a multi-granular crew activity recognition framework that identifies individual actions, groups collaborating workers, and links them to specific tasks. Using graph-based representations and self-attention mechanisms, the model integrates spatial and contextual information for accurate recognition. Experiments on a dataset covering rebar, formwork, and concrete operations show an overall F1 Score of 70.31%. Results highlight the importance of balancing visual features and spatial proximity for optimal performance. This framework offers an efficient solution for construction site monitoring and lays groundwork for future research on temporal modeling and human-object interaction analysis.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Crew activity recognition"],
	"applications": ["Productivity monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106428",
    "code_url": "https://github.com/JimmyTsai-Yun/MGCAR",
    "thumbnail": "assets/img/models/MGCAR.png"
  },
  {
    "id": "AID-AI-Infraction-Detection",
    "title": "Exploring a Multimodal Conversational Agent for Construction Site Safety: A Low-Code Approach to Hazard Detection and Compliance Assessment",
    "authors": ["Giancarlo de Marco", "Elias Niederwieser", "Dietmar Siegele"],
    "abstract": "This paper discusses the viability of using a low-code multimodal large language model agent with computer vision functionality to support occupational safety and health evaluations on construction sites. The central hypothesis aims to verify that these systems can provide reliable answers, as evaluated against a ground truth review, including the identification of high-risk dangers. A conversational agent was given the task of finding hazards and checking for national legislative compliance within a dataset of 100 real-world construction photos. The comparison of the agent’s results to the ground truth provides insight into current limitations. The primary issues identified were inconsistent taxonomies, inadequate causal reasoning, and insufficient contextual consideration, all of which adversely impacted performance—particularly when analyzing low-resolution images. The metrics supporting the conclusion synthesize that this tool is a valuable augmentation technology, enhancing safety evaluations while still requiring human supervision to ensure reliability.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Vision–language reasoning"],
	"applications": ["Safety monitoring"],
    "license": "Non-Commercial Use Only",
    "paper_url": "https://doi.org/10.3390/buildings15183352",
    "code_url": "https://github.com/fraunhofer-italia/AID-AI-Infraction-Detection",
    "thumbnail": "assets/img/models/AID-AI-Infraction-Detection.png"
  },
    {
    "id": "ETHcavation",
    "title": "ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments",
    "authors": ["Lorenzo Terenzi", "Julian Nubert", "Pol Eyschen", "Pascal Roth", "Simin Fei", "Edo Jelavic", "Marco Hutter"],
	"abstract": "Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset and code for training and deployment are publicly available to support future research.",
    "year": 2025,
    "modalities": ["Ground RGB, 3D Point Cloud"],
    "tasks": ["Semantic segmentation, Object tracking"],
	"applications": ["Site mapping and navigation"],
    "license": "Unspecified",
    "paper_url": "https://arxiv.org/abs/2410.04250",
    "code_url": "https://github.com/leggedrobotics/rsl_panoptic_mapping",
    "thumbnail": "assets/img/models/ETHcavation.gif",
	"added_date": "2025-09-28"
  },
  {
    "id": "Buildee",
    "title": "Buildee: A 3D Simulation Framework for Scene Exploration and Reconstruction with Understanding",
    "authors": ["Clementin Boittiaux", "Vincent Lepetit"],
    "abstract": "We introduce Buildee, a 3D simulation framework designed to benchmark scene exploration, 3D reconstruction, and semantic segmentation tasks in both static and dynamic environments. Built as a Python module on top of Blender, Buildee leverages its advanced rendering capabilities to generate realistic RGB, depth, and semantic data while enabling 2D / 3D point tracking and occlusion checking. Additionally, we provide a procedural generator for construction site environments and baseline methods for key computer vision tasks. Through Buildee, we establish a standardized platform for evaluating scene understanding algorithms in realistic settings. Our code is publicly available at https://github.com/clementinboittiaux/buildee.",
    "year": 2025,
    "modalities": ["Ground RGB, depth, synthetic"],
    "tasks": ["Scene generation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://openreview.net/pdf?id=1LmsiOaMTy",
    "code_url": "https://github.com/clementinboittiaux/buildee",
    "thumbnail": "assets/img/models/Buildee.gif",
	"added_date": "2025-09-28"
  },
   {
    "id": "LOD3",
    "title": "Generating LOD3 building models from structure-from-motion and semantic segmentation",
    "authors": ["Bryan German Pantoja Rosero", "Radhakrishna Achanta", "Mateusz Kozinski", "Pascal Fua", "Fernando Perez-Cruz", "Katrin Beyer"],
	"abstract": "This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["3D reconstruction"],
	"applications": ["Site understanding"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104430",
    "code_url": "https://github.com/eesd-epfl/LOD3_buildings",
    "thumbnail": "assets/img/models/LOD3.png"
  },
    {
    "id": "Open",
    "title": "Facilitating Construction Scene Understanding Knowledge Sharing and Reuse via Lifelong Site Object Detection",
    "authors": ["Ruoxin Xiong", "Yuansheng Zhu", "Yanyu Wang", "Pengkun Liu", "Pingbo Tang"],
	"abstract": "Automatically recognizing diverse construction resources (e.g., workers and equipment) from construction scenes supports efficient and intelligent workplace management. Previous studies have focused on identifying fixed object categories in specific contexts, but they have difficulties in accumulating existing knowledge while extending the model for handling additional classes in changing applications. This work proposes a novel lifelong construction resource detection framework for continuously learning from dynamic changing contexts without catastrophically forgetting previous knowledge. In particular, we contribute: (1) an OpenConstruction Dataset with 31 unique object categories, integrating three large datasets for validating lifelong object detection algorithms; (2) an OpenConstruction Taxonomy, unifying heterogeneous label space from various scenarios; and (3) an informativeness-based lifelong object detector that leverages very limited examples from previous learning tasks and adds new data progressively. We train and evaluate the proposed method on the OpenConstruction Dataset in sequential data streams and show mAP improvements on the overall task. Code is available at https://github.com/YUZ128pitt/OpenConstruction.",
    "year": 2022,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1007/978-3-031-25082-8_15",
    "code_url": "https://github.com/YUZ128pitt/OpenConstruction",
    "thumbnail": "assets/img/models/Open.png",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "Worker-Safety-Twin",
    "title": "Advancing construction site workforce safety monitoring through BIM and computer vision integration",
    "authors": ["Almo Senja Kulinan", "Minsoo Park", "Pa Pa Win Aung", "Gichun Cha", "Seunghee Park"],
    "abstract": "Ensuring a safe work environment is crucial for construction projects. It is essential that workforce monitoring is both efficient and non-intrusive to the ongoing construction activities. This paper introduces a method that integrates building information modeling (BIM) and computer vision to monitor workforce safety hazards at construction sites in real time. Despite the rising adoption of BIM and computer vision individually within the construction sector, the potential of their integrated application as a cohesive system for workforce safety monitoring remains unexplored. While BIM provides rich 3D semantic information about the construction site, computer vision captures real-time field data. The system was tested using a realistic construction simulation, and the accuracy of the position estimate was evaluated in a real-world interior environment, yielding a mean error distance (MED) of 13.2 cm. Overall, the findings have substantial significance for the construction industry to help minimize accidents and enhance overall worker safety.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.105227",
    "code_url": "https://github.com/almosenja/Worker-Safety-Twin",
    "thumbnail": "assets/img/models/Worker-Safety-Twin.gif"
  },
  {
    "id": "compliance-checking",
    "title": "Automated rule-based safety inspection and compliance checking of temporary guardrail systems in construction",
    "authors": ["K.W. Johansen", "J. Teizer", "C. Schultz"],
    "abstract": "The construction industry records more hazards compared to any other sector. Protective equipment, such as guardrail systems, is essential for protecting workers from deadly falls but may quickly become incompliant after installation. Yet, many construction projects do not have the resources to dedicate personnel to perform the inspection as frequently as needed. Therefore, this paper proposes an automated rule-based inspection and compliance-checking system that can assist the responsible personnel in detecting faulty guardrails in live work environments. The classification approach utilizes safety design and mimics the steps of human guardrailing compliance assessment, which enforces simplicity and transparency, allowing the human domain expert to remain in control. Even under scarce data availability, this first-of-a-kind classification approach is reliable and scalable and successfully classifies 21 predefined and 9 validation scenarios of guardrail systems for fall protection.",
    "year": 2024,
    "modalities": ["Aerial RGB"],
    "tasks": ["Knowledge reasoning"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105849",
    "code_url": "https://github.com/kakke14/Automated-inspection-and-compliance-checking",
    "thumbnail": "assets/img/models/compliance-checking.png"
  },
  {
    "id": "URRE-Net",
    "title": "An unsupervised low-light image enhancement method for improving V-SLAM localization in uneven low-light construction sites",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Construction robots have been increasingly adopted in construction projects to improve productivity, reduce risk, and speed up work cycles. Visual Simultaneous Localization and Mapping (V-SLAM) technology is widely used in construction robots due to its lightweight weight, cost-effectiveness, and provision of semantic information. On real construction sites, lighting conditions are often challenging due to factors such as uneven lighting intensity, inadequate exposure, or robots in backlit positions (e.g. roughcast houses without artificial lighting, underground car parks), which cause images captured under these conditions to exhibit low signal-to-noise ratio, uneven lighting, color distortion, noise, and other problems. These challenges make the extraction and matching of feature points in V-SLAM difficult, resulting in significant positioning errors or the inability to generate positioning trajectories for construction robotics. Although existing methods for enhancing low-light images can certainly improve image brightness, they still cannot effectively address the localization challenges brought about by low-light construction scenes with uneven illumination and noise. To address the interference of uneven and changing lighting conditions on V-SLAM localization in construction sites, this paper proposes the Unsupervised Reflectance Retinex and Noise model (URRN-Net) to enhance low-light construction images. By using a CNN model based on the Retinex theory to decompose the illumination characteristics of images, we achieve effective brightness restoration of uneven low-light images by utilizing unsupervised loss functions. URRE-Net can reduce the root mean square localization error by >65% compared to low-light images in the ORB-SLAM3 method, and the maximum error is reduced by >71% in on-site experiments. The proposed URRE-Net can be integrated with existing V-SLAM algorithms to provide more robust localization services for low-light construction site applications such as building operations (e.g., interior wall spraying robotic) or construction management tasks (e.g., automatic tunnel inspection).",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105404",
    "code_url": "https://github.com/Chenxy875/URRE-Net",
    "thumbnail": "assets/img/models/URRE-Net.gif"
  },
  {
    "id": "LMs_ConstructionSafety",
    "title": "Performance comparison of retrieval-augmented generation and fine-tuned large language models for construction safety management knowledge retrieval",
    "authors": ["Jungwon Lee", "Seungjun Ahn", "Daeho Kim", "Dongkyun Kim"],
    "abstract": "Construction safety standards are in unstructured formats like text and images, complicating their effective use in daily tasks. This paper compares the performance of Retrieval-Augmented Generation (RAG) and fine-tuned Large Language Model (LLM) for the construction safety knowledge retrieval. The RAG model was created by integrating GPT-4 with a knowledge graph derived from construction safety guidelines, while the fine-tuned LLM was fine-tuned using a question-answering dataset derived from the same guidelines. These models' performance is tested through case studies, using accident synopses as a query to generate preventive measurements. The responses were assessed using metrics, including cosine similarity, Euclidean distance, BLEU, and ROUGE scores. It was found that both models outperformed GPT-4, with the RAG model improving by 21.5 % and the fine-tuned LLM by 26 %. The findings highlight the relative strengths and weaknesses of the RAG and fine-tuned LLM approaches in terms of applicability and reliability for safety management.",
    "year": 2024,
    "modalities": ["Document"],
    "tasks": ["Knowledge reasoning"],
	"applications": ["Compliance checking"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105846",
    "code_url": "https://github.com/juuuungwon/LMs_ConstructionSafety",
    "thumbnail": "assets/img/models/LMs_ConstructionSafety.png"
  },
  {
    "id": "ego",
    "title": "Egocentric camera-based method for detecting static hazardous objects on construction sites",
    "authors": ["Ziming Liu", "Jiuyi Xu", "Christine Wun Ki Suen", "Meida Chen", "Zhengbo Zou", "Yangming Shi"],
    "abstract": "The construction site is a hazardous workplace, accounting for more than 20 % of worker fatalities compared to other industries in the United States. Predominant causes of these fatalities are slips, trips, and falls (STFs). Therefore, identifying hazardous objects on construction sites that could lead to STFs is crucial for enhancing construction safety. Previous studies using fixed-position cameras often miss observations of obstructed or hidden objects. This paper proposes an alternative approach using safety helmets with lightweight wide-angle cameras and leveraging open-vocabulary object detection (OVOD) methods to identify hazardous objects on construction sites that could lead to STFs. In addition, an egocentric view dataset specifically for construction sites was created and released for benchmarking purposes. Research results indicated a 79.0 % weighted F1-score in classifying static hazardous objects on construction sites. This proposed system has the potential to enhance construction safety and provide a valuable dataset for future construction safety research.",
    "year": 2025,
    "modalities": ["Egocentric RGB"],
    "tasks": ["object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106048",
    "code_url": "https://github.com/JiuyiX/Tripping-Hazard-Detection-based-on-OVOD",
    "thumbnail": "assets/img/models/ego.png"
  },
  {
    "id": "YOLO-FAS",
    "title": "YOLO-FAS: A lightweight model for detecting rebar intersections location and tying status",
    "authors": ["Hao Duan", "Mingming Yu", "Tengfeng Ai", "Mengmeng Zhu", "Haili Jiang", "Shuai Guo"],
    "abstract": "Current deep learning (DL) algorithms for detecting complex rebar mesh intersection points rely heavily on large amounts of training data to ensure recognition accuracy and generalization capabilities. However, deploying such algorithms on small mobile robots is challenging due to limited computational resources, making it difficult to achieve both real-time performance and high precision. To address this issue, this paper proposes a lightweight YOLO-FAS model specifically designed for rebar intersections detection on tying robots. This model is based on YOLOv5s and optimizes the FastNet structure by introducing the PGConv module, enhances feature fusion with the AFPN (Asymptotic feature pyramid network) module, and improves localization accuracy using the EIOU loss function. Experimental results demonstrate that the model's parameters and computational load are reduced by 60.6% and 62%, respectively. Furthermore, through BN (Batch Normalization) channel pruning and QAT (Quantization Aware Training), the model is compressed, and inference is accelerated using TensorRT. The improved YOLO-FAS model achieves a FPS (frames per second) increase of 84.1% in FP32 mode and 33.2% in INT8 mode. Finally, after real-world deployment testing, the system's average memory usage is reduced by 0.77GB, and the accuracy of recognition of intersection points reaches 98.21%, representing an improvement of 3.04% over YOLOv5s. The results indicate that this method effectively achieves model lightweighting while ensuring efficient and accurate detection of rebar intersection points, demonstrating robust performance and promising application prospects.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.neucom.2025.129485",
    "code_url": "https://github.com/5204338/yolov5_node",
    "thumbnail": "assets/img/models/YOLO-FAS.gif"
  },
  {
    "id": "MTKF",
    "title": "A unified object and keypoint detection framework for Personal Protective Equipment use",
    "authors": ["Bin Yang", "Hongru Xiao", "Binghan Zhang"],
    "abstract": "Accurately detecting whether workers wear Personal Protective Equipment (PPE) in real time plays an important role in safety management. Previous studies mainly used multiple models jointly or only object detection for wearing relationship judgments. This makes it difficult to provide real-time, accurate detection of security relationships. Therefore, this paper proposes safe-wearing detection rules and a novel multi-targets and keypoints detection framework (MTKF), which is capable of accomplishing multiple classes of targets and keypoints detection simultaneously in one-stage, to get more accurate results. In order to improve the performance in the PPE and worker keypoints detection in challenging construction scenes, the detection head transformation strategy, mix group shuffle attention (MGSA) module, and the improved dual and cross-class suppression algorithm (DC-NMS) are proposed. The experimental results are implemented on one established dataset (Joint dataset) and two public datasets (SHWD and COCO), which conduct a comprehensive evaluation in multiple dimensions. Compared to the baseline model, our method improves the mAP by 2.6%–7.1%, reduces the number of parameters by at least 70%, and is able to achieve an inference speed of 155 fps.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.dibe.2024.100559",
    "code_url": "https://github.com/TJSG337/MTKF",
    "thumbnail": "assets/img/models/MTKF.png"
  },
  {
    "id": "SOC-YOLO",
    "title": "Small and overlapping worker detection at construction sites",
    "authors": ["Minsoo Park", "Dai Quoc Tran", "Jinyeong Bak", "Seunghee Park"],
    "abstract": "Although there has been study on worker detection using computer vision (CV) for the safety of construction sites, it is still challenging to identify employees who are obstructed or have poor vision. To solve these problems, we propose a method of small and overlapping target (worker) detection at a complex construction site named SOC-YOLO. The method is based on YOLOv5 and utilizes distance intersection over union (DIoU) non-maximum suppression (NMS), incorporating weighted triplet attention, expansion feature-level, and Soft-pool. Workers can be captured with overlap, particularly in large-scale construction sites, using the DIoU-based loss function, and NMS contributed to accuracy improvement. Next, we propose a weighted-triplet attention mechanism that can extract feature information from space more effectively and channel attention when learning object detection networks, using a simple average approach based on the same weight between the existing triplet attention. Next, we propose a model that adds additional predictive heads and residual connections to address the poor detection accuracy of workers photographed over long distances. A low-level feature map containing more information regarding small targets is used by extending the feature level. Finally, Softpool-spatial pyramid pooling fast (Softpool-SPPF) is proposed to solve the problem of inconsistent input image sizes. Softpool-SPPF performs an spatial pyramid pooling (SPP) function while preserving more functional information for accurate small target detection. Experiments were conducted using published worker detection datasets and handmade datasets, and the results showed increase from 81.26% to 84.63% average precision (AP) for small objects, from 67.52% to 73.88% mAP for minute objects, from 74.56% to77.57% for overlapping objects. The proposed method is expected to be useful for safety monitoring by applying it to the construction site worker tracking model.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.104856",
    "code_url": "https://github.com/pms5343/SOC-YOLO",
    "thumbnail": "assets/img/models/SOC-YOLO.png"
  },
  {
    "id": "SORD",
    "title": "Deep learning without human labeling for on-site rebar instance segmentation using synthetic BIM data and domain adaptation",
    "authors": ["Tsung-Wei Huang", "Yi-Hsiang Chen", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "On-site rebar inspection is crucial for structural safety but remains labor-intensive and time-consuming. While deep learning presents a promising solution, existing research often relies on limited real-world labeled data. This paper introduces a framework to train a deep learning model for on-site rebar instance segmentation without human labeling. Synthetic data are generated from BIM models, creating a Synthetic On-site Rebar Dataset (SORD) with 25,287 labeled images. Domain adaptation is incorporated to bridge the gap between synthetic and real-world non-labeled data. This approach eliminates the need for human labeling. It significantly enhances model performance, achieving a threefold improvement in Average Precision (AP) metrics compared to models trained on limited real-world data. Additionally, the proposed method demonstrates superior performance across various on-site rebar images collected online, underscoring its generalizability and practical applications.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Semantic segmentation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105953",
    "code_url": "https://github.com/HuangBugWei/DA-MaskRCNN",
    "thumbnail": "assets/img/models/SORD.png"
  },
  {
    "id": "FLE-YOLO",
    "title": "FLE-YOLO: A Faster, Lighter, and More Efficient Strategy for Autonomous Tower Crane Hook Detection",
    "authors": ["Xin Hu", "Xiyu Wang", "Yashu Chang", "Jian Xiao", "Hongliang Cheng"],
    "abstract": "To address the complexities of crane hook operating environments, the challenges faced by large-scale object detection algorithms on edge devices, and issues such as frame rate mismatch causing image delays, this paper proposes a faster, lighter, and more efficient object detection algorithm called FLE-YOLO. Firstly, the FasterNet is used as the backbone for feature extraction, and the Triplet Attention mechanism is integrated to effectively emphasize target information while maintaining network lightweightness effectively. Additionally, the Slim-neck module is introduced in the neck connection layer, utilizing a lightweight convolutional network GSconv to further streamline the network structure without compromising recognition accuracy. Lastly, the Dyhead module is employed in the head section to unify multiple attention operations, improve the ability to resist interference from small objects and complex backgrounds. Experimental evaluations on public datasets VOC2012 and COCO2017 demonstrate the effectiveness of our proposed algorithm in terms of lightweight design and detection accuracy. Experimental evaluations were also conducted using images of crane hooks captured under complex operating conditions. The results demonstrate that compared to the original algorithm, the proposed approach achieves a reduction in computational complexity to 19.4 GFLOPs, an increase in FPS to 142.857 f/s, and the precision reached 97.3%. Additionally, the AP50 reaches 98.3%, reflecting 0.6% improvement. Ultimately, the testing carried out at the construction site successfully facilitated the identification and tracking of hooks, thereby ensuring the safety and efficiency of tower crane operations.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.3390/app15105364",
    "code_url": "https://github.com/sugar-fifty-doge/FLE-YOLO",
    "thumbnail": "assets/img/models/FLE-YOLO.png"
  },
  {
    "id": "VR-Point-Cloud-Analysis",
    "title": "Fusion of Thermal Point Cloud Series of Buildings for Inspection in Virtual Reality",
    "authors": ["Emiliano Pérez", "Pilar Merchán", "Alejandro Espacio", "Santiago Salamanca"],
    "abstract": "Point cloud acquisition systems now enable the capture of geometric models enriched with additional attribute data, providing a deeper semantic understanding of the measured environments. However, visualizing complex spatiotemporal point clouds remains computationally challenging. This paper presents a fusion methodology that aggregates points from different instants into unified clouds with reduced redundancy while preserving time-varying information. The static 3D structure is condensed using a voxel approach, while temporal attributes are propagated across the merged data. The resulting point cloud is optimized and rendered interactively in a virtual reality (VR) application. This platform allows for intuitive exploration, visualization, and analysis of the merged clouds. Users can examine thermographic properties using color maps and study graphical temperature trends. The potential of VR for insightful interrogation of point clouds enriched with multiple properties is highlighted by the system.",
    "year": 2024,
    "modalities": ["3D point cloud"],
    "tasks": ["Point cloud visualization"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.3390/buildings14072127",
    "code_url": "https://github.com/3dcovim/VR-Point-Cloud-Analysis",
    "thumbnail": "assets/img/models/VR-Point-Cloud-Analysis.png"
  },
  {
  "id": "structgan-v1",
  "title": "Automated Structural Design of Shear Wall Residential Buildings using Generative Adversarial Networks",
  "authors": ["Wenjie Liao", "Xinzheng Lu", "Yuli Huang", "Zhe Zheng", "Yuanqing Lin"],
  "abstract": "Artificial intelligence is reshaping building design processes to be smarter and automated. Considering the increasingly wide application of shear wall systems in high-rise buildings and envisioning the massive benefit of automated structural design, this paper proposes a generative adversarial network (GAN)-based shear wall design method, which learns from existing shear wall design documents and then performs structural design intelligently and swiftly. To this end, structural design datasets were prepared via abstraction, semanticization, classification, and parameterization in terms of building height and seismic design category. The GAN model improved its shear wall design proficiency through adversarial training supported by data and hyper-parametric analytics. The performance of the trained GAN model was appraised against the metrics based on the confusion matrix and the intersection-over-union approach. Finally, case studies were conducted to evaluate the applicability, effectiveness, and appropriateness of the innovative GAN-based structural design method, indicating significant speed-up and comparable quality.Artificial intelligence is reshaping building design processes to be smarter and automated. Considering the increasingly wide application of shear wall systems in high-rise buildings and envisioning the massive benefit of automated structural design, this paper proposes a generative adversarial network (GAN)-based shear wall design method, which learns from existing shear wall design documents and then performs structural design intelligently and swiftly. To this end, structural design datasets were prepared via abstraction, semanticization, classification, and parameterization in terms of building height and seismic design category. The GAN model improved its shear wall design proficiency through adversarial training supported by data and hyper-parametric analytics. The performance of the trained GAN model was appraised against the metrics based on the confusion matrix and the intersection-over-union approach. Finally, case studies were conducted to evaluate the applicability, effectiveness, and appropriateness of the innovative GAN-based structural design method, indicating significant speed-up and comparable quality.",
  "year": 2021,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Image-to-image translation", "Layout synthesis"],
  "applications": ["Automated structural design", "Shear wall layout generation"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.autcon.2021.103931",
  "code_url": "https://github.com/wenjie-liao/StructGAN_v1",
  "thumbnail": "assets/img/models/structgan-v1.png"
},
{
  "id": "structgan-phy",
  "title": "Intelligent structural design of shear wall residence using physics-enhanced generative adversarial networks",
  "authors": ["Xinzheng Lu", "Wenjie Liao", "Yu Zhang", "Yuli Huang"],
  "abstract": "Intelligent structural design using generative adversarial networks (GANs) is a revolutionary design approach for building structures. Despite its far-reaching capability, the data quantity and quality may have limited the performance of such a data-driven network. This study proposes to enhance the objectiveness of training processes by innovatively introducing a surrogate model, Physics Estimator, that informs the generator by appraising the physical behavior of the generated design. Dual loss functions evaluated by a traditional data-driven discriminator and the Physics Estimator collaboratively foster the physics-enhanced GAN architecture. We further develop a structural mechanics model to train and optimize the inherent accuracy of the Physics Estimator. The comparative study suggests that the proposed physics-enhanced GAN can generate structural designs from architectural drawings and specified design conditions 44% better than a data-driven design method and 90 times faster than a competent engineer.",
  "year": 2022,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Image-to-image translation", "Layout synthesis", "Physics-informed generation"],
  "applications": ["Automated structural design", "Shear wall layout generation"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1002/eqe.3632",
  "code_url": "https://github.com/wenjie-liao/StructGAN-PHY",
  "thumbnail": "assets/img/models/structgan-phy.png"
},
{
  "id": "structgan-txt-txtimg2img",
  "title": "Intelligent generative structural design method for shear wall building based on “fused-text-image-to-image” generative adversarial networks",
  "authors": ["Wenjie Liao", "Yuli Huang", "Zhe Zheng", "Xinzheng Lu"],
  "abstract": "Like the way engineers designing buildings, competent generative design methods try to understand the prescriptive requirement in text and architectural sketches, apply engineering principles and develop the structural design. However, this requirement may be challenging to existing methods because they are not good at simultaneously taking text and image input and then generating designs. This study proposed an innovative design approach, TxtImg2Img, to overcome the difficulties. Based on generative adversarial networks architecture, the generator is proposed to encode, extract and fuse texts and images, and generate new design images; the discriminator is developed to judge real and fake images and texts. Consequently, TxtImg2Img is advantageous in extracting features from the multimodal text and image data, fusing the features using the Hadamard product, and generating designs to satisfy the text-image requirements after learning from a limited number of design samples. Specifically, TxtImg2Img can generate structural design images without distortion, and the corresponding structural design meets the mechanical requirements, after being trained by dozens of words and hundreds of image data. The case studies confirm performance improvement of up to 21% and that the proposed approach presents a promising breakthrough for intelligent construction.",
  "year": 2022,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Multimodal conditional generation", "Image-to-image translation", "Layout synthesis"],
  "applications": ["Automated structural design", "Shear wall layout generation"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.eswa.2022.118530",
  "code_url": "https://github.com/wenjie-liao/StructGAN-TXT-TxtImg2Img",
  "thumbnail": "assets/img/models/structgan-txt-txtimg2img.png"
},
{
  "id": "image2triplets",
  "title": "Image2Triplets: A computer vision-based explicit relationship extraction framework for updating construction activity knowledge graphs",
  "authors": ["Zaolin Pan", "Cheng Su", "Yichuan Deng", "Jack C. P. Cheng"],
  "abstract": "Knowledge graph (KG) is an effective tool for knowledge management, particularly in the architecture, engineering and construction (AEC) industry, where knowledge is fragmented and complicated. However, research on KG updates in the industry is scarce, with most current research focusing on text-based KG updates. Considering the superiority of visual data over textual data in terms of accuracy and timeliness, the potential of computer vision technology for explicit relationship extraction in KG updates is yet to be explored. This paper combines zero-shot human-object interaction detection techniques with general KGs to propose a novel framework called Image2Triplets that can extract explicit visual relationships from images to update the construction activity KG. Comprehensive experiments on the images of architectural decoration processes have been performed to validate the proposed framework. The results and insights will contribute new knowledge and evidence to human-object interaction detection, KG update and construction informatics from the theoretical perspective.",
  "modalities": ["Ground RGB"],
  "year": 2022, 
  "tasks": ["Human–object interaction detection"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.compind.2022.103610",
  "code_url": "https://github.com/CrossStyle/Image2Triplets",
  "thumbnail": "assets/img/models/image2triplets.png"
},
{
  "id": "mr-visual-warning",
  "title": "Real-time mixed reality-based visual warning for construction workforce safety",
  "authors": ["Shaoze Wu", "Lei Hou", "Guomin (Kevin) Zhang", "Haosen Chen"],
  "abstract": "Spatial locations of personnel, equipment, and materials are constantly changing as construction projects progress. The dynamic nature of the construction industry affects workers' performance of identifying hazards. Even though a great deal of effort has been made to improve construction safety, the construction industry still witnesses a high accident rate. In order to complement the existing body of knowledge relating to construction safety, this paper integrates Digital Twin (DT), Deep Learning (DL), and Mixed Reality (MR) technologies into a newly developed real-time visual warning system, which enables construction workers to proactively determine their safety status and avoid accidents. Next, system tests were conducted under three quasi-on-site scenarios, and the feasibility was proven in terms of synchronising construction activities over a large area and visually representing hazard information to its users. These evidenced merits of the development testing scenarios can improve workers' risk assessment accuracy, reinforce workers' safety behaviour, and provide a new perspective for construction safety managers to analyse construction safety status.",
  "year": 2022,
  "modalities": ["Video", "BIM Models"],
  "tasks": ["Object detection", "Object tracking"],
  "applications": ["Safety monitoring"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104252",
  "code_url": "https://github.com/Mercer-S/MR-visual-warning",
  "thumbnail": "assets/img/models/mr-visual-warning.png"
},
{
  "id": "synthetic-truss-bridges",
  "title": "Automated production of synthetic point clouds of truss bridges for semantic and instance segmentation using deep learning models",
  "authors": ["Daniel Lamas", "Andrés Justo", "Mario Soilán", "Belén Riveiro"],
  "abstract":"The cost of obtaining large volumes of bridge data with technologies like laser scanners hinders the training of deep learning models. To address this, this paper introduces a new method for creating synthetic point clouds of truss bridges and demonstrates the effectiveness of a deep learning approach for semantic and instance segmentation of these point clouds. The method generates point clouds by specifying the dimensions and components of the bridge, resulting in high variability in the generated dataset. A deep learning model is trained using the generated point clouds, which is an adapted version of JSNet. The accuracy of the results surpasses previous heuristic methods. The proposed methodology has significant implications for the development of automated inspection and monitoring systems for truss bridges. Furthermore, the success of the deep learning approach suggests its potential for semantic and instance segmentation of complex point clouds beyond truss bridges.",
  "year": 2024,
  "modalities": ["3D point cloud", "Synthetic"],
  "tasks": ["Point cloud generation"],
  "applications": ["Structural health monitoring"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.105176",
  "code_url": "https://github.com/GeoTechUVigo/synthetic_truss_bridges",
  "thumbnail": "assets/img/models/synthetic-truss-bridges.png"
},
{
  "id": "truss-bridge-pointcloud-segmentation",
  "title": "Instance and semantic segmentation of point clouds of large metallic truss bridges",
  "authors": ["Daniel Lamas", "Andrés Justo", "Mario Soilán", "Manuel Cabaleiro", "Belén Riveiro"],
  "abstract": "Several methods have been developed for the semantic segmentation of reinforced concrete bridges, however, there is a gap for truss bridges. Therefore, in this study a state-of-the-art methodology for the instance and semantic segmentation of point clouds of truss bridges for modelling purposes is presented, which, to the best of the authors' knowledge, is the first such methodology. This algorithm segments each truss element and classifies them as a chord, diagonal, vertical post, interior lateral brace, bottom lateral brace, or strut. The algorithm consists of a sequence of methods, including principal component analysis or clustering, that analyse each point and its neighbours in the point cloud. Case studies show that by adjusting only six manually measured parameters, the algorithm can automatically segment a truss bridge point cloud.",
  "year": 2023,
  "modalities": ["3D point cloud"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Structural health monitoring"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2023.104659",
  "code_url": "https://github.com/GeoTechUVigo/truss_bridge_pointcloud_segmentation",
  "thumbnail": "assets/img/models/truss-bridge-pointcloud-segmentation.gif"
},
{
  "id": "floorplangan",
  "title": "FloorplanGAN: Vector residential floorplan adversarial generation",
  "authors": ["Ziniu Luo", "Weixin Huang"],
  "abstract": "An architectural floorplan is a class of drawings that reflects the layout of rooms. The difference between a floorplan and a natural image and its dual features as both a vector graphic and a raster image makes it difficult to be generated by conventional deep neural generative models. We propose an adversarial generative framework that combines vector generation and raster discrimination for residential floorplan generation tasks. The floorplan is first generated in vector format with room areas as constraints and then discriminated in raster format visually using convolutional layers. A Differentiable Renderer connects the gap between the Vector Generator and Raster Discriminator. A self-attention mechanism is utilized to capture the interrelations of rooms in each floorplan. Experiments were conducted to demonstrate the feasibility of the proposed FloorplanGAN. In addition, we evaluated the effectiveness of generation based on diverse objective metrics and a user study. The code is available here: https://github.com/luozn15/FloorplanGAN.",
  "year": 2022,
  "modalities": ["Raster floorplan images"],
  "tasks": ["Graph-to-image translation"],
  "applications": ["Floorplan generation"],
  "license": "MIT",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104470",
  "code_url": "https://github.com/luozn15/FloorplanGAN",
  "thumbnail": "assets/img/models/floorplangan.png"
},
{
  "id": "cloud2bim",
  "title": "Open-source automatic pipeline for efficient conversion of large-scale point clouds to IFC format",
  "authors": ["Slávek Zbirovský", "Václav Nežerka"],
  "abstract": "Building Information Model (BIM) creation usually relies on laborious manual transformation of the unstructured point cloud data provided by laser scans or photogrammetry. This paper presents Cloud2BIM, an open-source software tool designed to automate the conversion of point clouds into BIM models compliant with the Industry Foundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for wall and slab segmentation, opening detection, and room zoning based on real wall surfaces, resulting in a comprehensive and fully automated workflow. Unlike existing tools, it avoids computationally- and calibration-intensive techniques such as RANSAC, supports non-orthogonal geometries, and provides unprecedented processing speed, achieving results up to seven times faster than fastest competing solutions. Systematic validation using benchmark datasets confirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for generating accurate BIM models, capable of converting extensive point cloud datasets for entire buildings into IFC format with minimal user input.",
  "year": 2025,
  "modalities": ["3D point cloud"],
  "tasks": ["Scan-to-BIM"],
  "applications": ["Site understanding"],
  "license": "GPL-3.0",
  "paper_url": "https://doi.org/10.1016/j.autcon.2025.106303",
  "code_url": "https://github.com/VaclavNezerka/Cloud2BIM",
  "thumbnail": "assets/img/models/cloud2bim.png"
},
{
  "id": "bfa-yolo",
  "title": "BFA-YOLO: A balanced multiscale object detection network for building façade elements detection",
  "authors": ["Yangguang Chen", "Tong Wang", "Guanzhou Chen", "Kun Zhu", "Xiaoliang Tan", "Jiaqi Wang", "Wenchao Guo", "Qing Wang", "Xiaolong Luo", "Xiaodong Zhang"],
  "abstract": "The detection of façade elements on buildings, such as doors, windows, balconies, air conditioning units, billboards, and glass curtain walls, is a critical step in automating the creation of Building Information Modeling (BIM). However, this field faces significant challenges, including the uneven distribution of façade elements, the presence of small objects, and substantial background noise, which hamper detection accuracy. To address these issues, we developed the BFA-YOLO model and the BFA-3D dataset in this study. The BFA-YOLO model is an advanced architecture designed specifically for analyzing multi-view images of façade elements. It integrates three novel components: the Feature Balanced Spindle Module (FBSM) that tackles the issue of uneven object distribution; the Target Dynamic Alignment Task Detection Head (TDATH) that enhances the detection of small objects; and the Position Memory Enhanced Self-Attention Mechanism (PMESA), aimed at reducing the impact of background noise. These elements collectively enable BFA-YOLO to effectively address each challenge, thereby improving model robustness and detection precision. The BFA-3D dataset offers multi-view images with precise annotations across a wide range of façade element categories. This dataset is developed to address the limitations present in existing façade detection datasets, which often feature a single perspective and insufficient category coverage. Through comparative analysis, BFA-YOLO demonstrated improvements of 1.8% and 2.9% in mAP50 on the BFA-3D dataset and the public Façade-WHU dataset, respectively, when compared to the baseline YOLOv8 model. These results highlight the superior performance of BFA-YOLO in façade element detection and the advancement of intelligent BIM technologies. The dataset and code are available at https://github.com/CVEO/BFA-YOLO.",
  "year": 2025,
  "modalities": ["Aerial RGB"],
  "tasks": ["Object detection"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.aei.2025.103289",
  "code_url": "https://github.com/CVEO/BFA-YOLO",
  "thumbnail": "assets/img/models/bfa-yolo.png"
},
{
  "id": "MLSTRUCT-FP_benchmarks",
  "title": "Large-scale multi-unit floor plan dataset for architectural plan analysis and recognition",
  "authors": ["Pablo N. Pizarro", "Nancy Hitschfeld", "Ivan Sipiran"],
  "abstract": "Among automatic floor plan analysis, data-driven methods have become increasingly popular in recent years because of their superior accuracy and generalizability compared to traditional approaches while processing rasterized floor plans. However, the scarcity of public raster datasets with various styles and sufficient quantity hinders the development of new models, as current ones only contain a single apartment or house, limiting the analysis of large-scale plans usually designed in architectural and structural offices. In order to address that issue, this paper presents a multi-unit floor plan dataset comprising 954 high-resolution images of residential buildings with annotated walls and slabs as polygons, enabling large-scale plan analysis. Additionally, this study implements an automatic wall vectorization method that uses a learning discriminative-based semantic segmentation U-Net model to retrieve wall objects, followed by a deep-learning model that predicts the segmented primitives, providing a baseline for future comparison of automatic wall segmentation results.",
  "year": 2023,
  "modalities": ["Rasterized CAD images"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Plan recognition"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1016/j.aei.2025.103289",
  "code_url": "https://github.com/MLSTRUCT/MLSTRUCT-FP_benchmarks",
  "thumbnail": "assets/img/models/MLSTRUCT-FP_benchmarks.png"
},
{
  "id": "tbbrdet",
  "title": "Deep learning approaches to building rooftop thermal bridge detection from aerial images",
    "authors": ["Zoe Mayer", "James Kahn", "Yu Hou", "Markus Götz", "Rebekka Volk", "Frank Schultmann"],
  "abstract": "Thermal bridges are weak points of building envelopes that can lead to energy losses, collection of moisture, and formation of mould in the building fabric. To detect thermal bridges of large building stocks, drones with thermographic cameras can be used. As the manual analysis of comprehensive image datasets is very time-consuming, we investigate deep learning approaches for its automation. For this, we focus on thermal bridges on building rooftops recorded in panorama drone images from our updated dataset of Thermal Bridges on Building Rooftops (TBBRv2), containing 926 images with 6,927 annotations. The images include RGB, thermal, and height information. We compare state-of-the-art models with and without pretraining from five different neural network architectures: MaskRCNN R50, Swin-T transformer, TridentNet, FSAF, and a MaskRCNN R18 baseline. We find promising results, especially for pretrained models, scoring an Average Recall above for detecting large thermal bridges with a pretrained Swin-T Transformer model.",
  "year": 2023,
  "modalities": ["Thermal images"],
  "tasks": ["Object detection"],
  "applications": ["Building energy management"],
  "license": "BSD 3-Clause",
  "paper_url": "https://doi.org/10.1016/j.autcon.2022.104690",
  "code_url": "https://github.com/Helmholtz-AI-Energy/TBBRDet",
  "thumbnail": "assets/img/models/TBBRDet.gif"
},
{
  "id": "OccFaçade",
  "title": "OccFaçade: Enabling precise building façade parsing in large urban scenes with occlusion",
    "authors": ["Yongjun Zhang", "Dongdong Yue", "Xinyi Liu", "Siyuan Zou", "Weiwei Fan", "Zihang Liu"],
  "abstract": "Building façade parsing is to recognize the building façade image into different categories of individuals including walls, doors, windows, balconies, etc. However, obstructions such as trees present a significant challenge to conducting façade parsing. In this paper, we designed OccFaçade to achieve high-precision parsing of occluded building façades in large urban scenes. OccFaçade primarily incorporates two modules, Multi-layer Dilated Convolution Module (MD-Module) and Multi-scale Row-Column Convolution Module (MRC-Module), to capture repeated texture in local and row-column directions. This aims to leverage repetitive textures to address occlusion challenges in building façade parsing. Besides, we introduce our building façade dataset MeshFaçade from the Mesh data generated by drone imagery to study the occlusion problem of missing textures. The experimental results demonstrate that OccFaçade achieves state-of-the-art performance with mIOU of 85.01%, 84.09%, 72.95%, and 88.83% on the ENPC2014 dataset, ECP dataset, RueMonge2014 dataset, and our MeshFaçade dataset, respectively. The code and data are available at https://github.com/yueyisui/OccFacade.",
  "year": 2024,
  "modalities": ["Ground RGB"],
  "tasks": ["Semantic segmentation"],
  "applications": ["Site understanding"],
  "license": "Unspecified",
  "paper_url": "https://doi.org/10.1080/01431161.2024.2391589",
  "code_url": "https://github.com/yueyisui/OccFacade",
  "thumbnail": "assets/img/models/OccFaçade.png"
},
{
    "id": "MultiWorker3DPose",
    "title": "A synthetic data-enhanced method for automated 3D pose recognition of construction workers",
    "authors": ["Yonglin Fu", "Weisheng Lu", "Zhiming Dong", "Yihai Fang"],
    "abstract": "Automated 3D pose recognition of construction workers is instrumental to analyzing their occupational safety and health, productivity and other jobsite behaviors. Existing studies in this field have been confined to high-quality training datasets collected from real-life construction jobsites, potentially triggering ethical, privacy, and cost concerns. Inspired by the success of synthetic data in other fields, this research proposes a synthetic data-enhanced method for automated 3D pose recognition of construction workers. It generates a synthetic dataset to supplement a real-life dataset for model training, presents a monocular vision-based model for recognizing multiple workers’ 3D poses, and then validates the model performance. Experiments verify that this model jointly trained with synthetic and real data outperforms a model trained on real data alone. The data enrichment approach explored in this study offers reliable data quality at less expense than real data-focused approaches. This research therefore lays a foundation for a series of studies to enhance workers’ occupational safety and health and productivity.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Pose estimation"],
	"applications": ["Site understanding"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.1016/j.eswa.2025.128768",
    "code_url": "https://github.com/fyongl6/MultiWorker3DPose",
    "thumbnail": "assets/img/models/MultiWorker3DPose.png"
  },
{
    "id": "SkeySpot",
    "title": " Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry",
    "authors": ["Dhruv Dosi", "Rohit Meena", "Param Rajpura", "Yogesh Kumar Meena"],
    "abstract": "Legacy floor plans, often preserved only as scanned documents, remain essential resources for architecture, urban planning, and facility management in the construction industry. However, the lack of machine-readable floor plans render large-scale interpretation both time-consuming and error-prone. Automated symbol spotting offers a scalable solution by enabling the identification of service key symbols directly from floor plans, supporting workflows such as cost estimation, infrastructure maintenance, and regulatory compliance. This work introduces a labelled Digitised Electrical Layout Plans (DELP) dataset comprising 45 scanned electrical layout plans annotated with 2,450 instances across 34 distinct service key classes. A systematic evaluation framework is proposed using pretrained object detection models for DELP dataset. Among the models benchmarked, YOLOv8 achieves the highest performance with a mean Average Precision (mAP) of 82.5%. Using YOLOv8, we develop SkeySpot, a lightweight, open-source toolkit for real-time detection, classification, and quantification of electrical symbols. SkeySpot produces structured, standardised outputs that can be scaled up for interoperable building information workflows, ultimately enabling compatibility across downstream applications and regulatory platforms. By lowering dependency on proprietary CAD systems and reducing manual annotation effort, this approach makes the digitisation of electrical layouts more accessible to small and medium-sized enterprises (SMEs) in the construction industry, while supporting broader goals of standardisation, interoperability, and sustainability in the built environment.",
    "year": 2025,
    "modalities": ["Rasterized CAD images"],
    "tasks": ["Object detection"],
	"applications": ["Plan recognition"],
    "license": "Unspecified",
    "paper_url": "https://doi.org/10.48550/arXiv.2508.10449",
    "code_url": "https://github.com/HAIx-Lab/SkeySpot",
    "thumbnail": "assets/img/models/SkeySpot.png"
  },
{
    "id": "Vitruvio",
    "title": "Vitruvio: Conditional variational autoencoder to generate building meshes via single perspective sketches",
    "authors": ["Alberto Tono1", "Heyaojing Huang", "Ashwin Agrawal", "Martin Fischer"],
    "abstract": "At the beginning of a project, architects convey design ideas via quick 2D diagrams, front views, floor plans, and sketches. Consequently, many stakeholders have difficulty visualizing the 3D representation of the building mass, leading to varied interpretations thus inhibiting a shared understanding of the design. To alleviate the challenge, this paper proposes a deep learning-based method, Vitruvio, for creating a 3D model from a single perspective sketch. This method allows designers to automatically generate 3D representations in real-time based on their initial sketches and thus communicate effectively and intuitively to the client. Vitruvio adapts the Occupancy Network to perform single view reconstruction (SVR), a technique for creating 3D representations from a single image. Vitruvio achieves: (1) an 18% increase in the reconstruction accuracy and (2) a 26% reduction in the inference time compared to the Occupancy Network on one thousand buildings provided by the New York municipality. This research investigates the effect that the building orientation has on the reconstruction quality, discovering that Vitruvio can capture fine-grain details in complex buildings when their native orientation is preserved during training, as opposed to the SVR's standard practice that aligns every building to its canonical pose. The code is available here https://github.com/CDInstitute/Vitruvio.",
    "year": 2024,
    "modalities": ["2D sketch", "3D mesh"],
    "tasks": ["3D reconstruction"],
	"applications": ["3D building mesh generation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105498",
    "code_url": "https://github.com/CDInstitute/Vitruvio",
    "thumbnail": "assets/img/models/Vitruvio.png"
  },
  {
    "id": "BCon",
    "title": "ControlNet-based domain adaptation for synthetic construction images via graphical simulation and generative AI",
    "authors": ["Sina Davari", "Daeho Kim", "Ali Tohidifar"],
    "abstract": "Data scarcity in construction hinders deep neural network training for computer vision applications. While synthetic data generators provide annotated images, they lack realism, creating a reality gap that leads to suboptimal real-world performance. This paper introduces BCon, a framework that integrates BlendCon, a construction data generation engine, with ControlNet, a generative architecture featuring conditioning controls, to enhance the realism and diversity of synthetic images while preserving annotations. Through hyperparameter tuning and post-processing, a dataset of 25,600 enhanced images is created. Quantitative evaluations demonstrate significant improvements in realism metrics: DreamSim (+9.5%), VIEScore (+114.3%), CLIPScore (+14.7%), and FID-5k (+22.6%), indicating closer alignment with real images. Moreover, YOLOv10 models trained on enhanced images achieve an AP50–95 of 0.66 on worker detection, outperforming those trained on original synthetic data by 7.9% and slightly surpassing models trained on equivalently sized real data. This framework offers cost-effective, high-quality dataset generation for visual AI applications in construction.",
    "year": 2025,
    "modalities": ["Synthetic"],
    "tasks": ["Image synthesis"],
	"applications": ["Site understanding"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106562",
    "code_url": "https://github.com/SinaDavari/bcon",
    "thumbnail": "assets/img/models/BCon.png",
	"added_date": "2025-10-5",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "DamageSegmentationXR",
    "title": "Integrating extended reality and AI-based damage segmentation for near real-time, traceable bridge inspections",
    "authors": ["B.G. Pantoja-Rosero", "S. Salamone"],
    "abstract": "This paper presents a framework for interactive and traceable visual bridge inspections by integrating AI-driven image processing models with extended reality. The framework addresses the subjectivity, labor intensity, and documentation challenges of traditional visual inspections. It employs YOLO11-seg models on a mixed reality device to perform multi-class damage instance detection and segmentation in near real-time, identifying cracks, spalling, rust, efflorescence, and exposed rebar. Unlike existing methods, this approach supports automated operation and local processing without cloud computing reliance. AI predictions are embedded in the 3D extended reality environment, enhancing efficiency and accuracy while enabling inspectors to visualize and interact with damage data, improving decision-making and traceability throughout the bridge life-cycle. Beyond model’s accuracy, inference time was evaluated to verify near real-time feasibility. Tests in three real-world scenarios demonstrated practical applicability. Future work will incorporate damage characterization and decision-support tools to advance digital inspection practices and foster efficient, resilient bridge monitoring.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection", "Semantic segmentation", "Extended reality"],
	"applications": ["Structural health monitoring"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106567",
    "code_url": "https://github.com/bgpantojar/DamageSegmentationXR",
    "thumbnail": "assets/img/models/DamageSegmentationXR.png",
	"added_date": "2025-10-12",
	"contributor": "ruoxinx",
	"contributor_url": "https://github.com/ruoxinx"
  },
  {
    "id": "client-to-bim",
    "title": "Transformer-based framework for mapping client requirements to BIM",
    "authors": ["Syed Haseeb Shah", "Saddiq Ur Rehman", "Inhan Kim", "Kyung-Eun Hwang"],
    "abstract": "Translating heterogeneous, client-authored textual requirements into constructible, information-rich models constitutes a primary impediment to digital transformation in early design phases. Legacy workflows demand high frequency client architect iteration, manual decoding of narrative requirements, and bespoke parametric modeling, introducing latency and inconsistency. This paper introduces an end-to-end automation pipeline that couples advanced Natural Language Processing (NLP) with Building Information Modeling (BIM) to dynamically interpret design intent from user inputs and instantiate corresponding BIM assemblies. A semantic translation layer maps parsed entities to a curated BIM model repository and propagates constraints into the authoring environment. On a multi project evaluation set the framework achieved 92 % mapping accuracy between client inputs and instantiated BIM elements. Embedding this capability enhances requirement traceability, clarifies intent for stakeholders, and enables scalable data driven design analytics. This contribution operationalizes AI assisted construction automation by unifying NLP and BIM within a single extensible workflow.",
    "year": 2025,
    "modalities": ["BIM"],
    "tasks": ["Text-to-BIM mapping"],
	"applications": ["Design brief automation"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106601",
    "code_url": "https://huggingface.co/ZappyKirby/client-to-bim",
    "thumbnail": "assets/img/models/client-to-bim.png",
	"added_date": "2025-10-15",
	"contributor": "Haseeb",
	"contributor_url": "https://github.com/QuantumNovice"
  }
]
