[
  {
    "id": "GDUT-HWD",
    "title": "Automatic detection of hardhats worn by construction personnel: A deep learning approach and benchmark dataset",
    "authors": ["Jixiu Wu", "Nian Cai", "Wenjie Chen", "Huiheng Wang", "Guotian Wang"],
    "abstract": "Hardhats play an essential role in protecting construction individuals from accidents. However, wearing hardhats is not strictly enforced among workers due to all kinds of reasons. To enhance construction sites safety, the majority of existing works monitor the presence and proper use of hardhats through multi-stage data processing, which come with limitations on adaption and generalizability. In this paper, a one-stage system based on convolutional neural network is proposed to automatically monitor whether construction personnel are wearing hardhats and identify the corresponding colors. To facilitate the study, this work constructs a new and publicly available hardhat wearing detection benchmark dataset, which consists of 3174 images covering various on-site conditions. Then, features from different layers with different scales are fused discriminately by the proposed reverse progressive attention to generate a new feature pyramid, which will be fed into the Single Shot Multibox Detector (SSD) to predict the final detection results. The proposed system is trained by an end-to-end scheme. The experimental results demonstrate that the proposed system is effective under all kinds of on-site conditions, which can achieve 83.89% mAP (mean average precision) with the input size 512×512.",
    "year": 2019,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Apache-2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2019.102894",
    "code_url": "https://github.com/wujixiu/helmet-detection",
    "thumbnail": "assets/img/models/GDUT-HWD.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Construction-Activity-Scenes",
    "title": "Manifesting construction activity scenes via image captioning",
    "authors": ["Huan Liu", "Guangbin Wang", "Ting Huang", "Ping He", "Martin Skitmore", "Xiaochun Luo"],
    "abstract": "This study proposed an automated method for manifesting construction activity scenes by image captioning – an approach rooted in computer vision and natural language generation. A linguistic description schema for manifesting the scenes is developed initially and two unique dedicated image captioning datasets are created for method validation. A general model architecture of image captioning is then instituted by combining an encoder-decoder framework with deep neural networks, followed by three experimental tests involving the selection of model learning strategies and performance evaluation metrics. It is demonstrated the method's performance is comparable with that of state-of-the-art computer vision methods in general. The paper concludes with a discussion of the feasibility of the practical application of the proposed approach at the current technical level.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103334",
    "code_url": "https://github.com/HannahHuanLIU/AEC-image-captioning",
    "thumbnail": "assets/img/models/Construction-Activity-Scenes.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Kenki-Posi",
    "title": "Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects",
    "authors": ["Runqiu Bao","Ren Komatsu","Renato Miyagusuku","Masaki Chino", "Atsushi Yamashita", "Hajime Asama"],
    "abstract": "At modern construction sites, utilizing GNSS (Global Navigation Satellite System) to measure the real-time location and orientation (i.e. pose) of construction machines and navigate them is very common. However, GNSS is not always available. Replacing GNSS with on-board cameras and visual simultaneous localization and mapping (visual SLAM) to navigate the machines is a cost-effective solution. Nevertheless, at construction sites, multiple construction machines will usually work together and side-by-side, causing large dynamic occlusions in the cameras' view. Standard visual SLAM cannot handle large dynamic occlusions well. In this work, we propose a motion segmentation method to efficiently extract static parts from crowded dynamic scenes to enable robust tracking of camera ego-motion. Our method utilizes semantic information combined with object-level geometric constraints to quickly detect the static parts of the scene. Then, we perform a two-step coarse-to-fine ego-motion tracking with reference to the static parts. This leads to a novel dynamic visual SLAM formation. We test our proposals through a real implementation based on ORB-SLAM2, and datasets we collected from real construction sites. The results show that when standard visual SLAM fails, our method can still retain accurate camera ego-motion tracking in real-time. Comparing to state-of-the-art dynamic visual SLAM methods, ours shows outstanding efficiency and competitive result trajectory accuracy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping/navigation"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1080/01691864.2020.1869586",
    "code_url": "https://github.com/RunqiuBao/kenki-positioning-vSLAM",
    "thumbnail": "assets/img/models/Kenki-Posi.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "CDE",
    "title": "Machine learning using synthetic images for detecting dust emissions on construction sites",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Automated dust monitoring in workplaces helps provide timely alerts to over-exposed workers and effective mitigation measures for proactive dust control. However, the cluttered nature of construction sites poses a practical challenge to obtain enough high-quality images in the real world. The study aims to establish a framework that overcomes the challenges of lacking sufficient imagery data (“data-hungry problem”) for training computer vision algorithms to monitor construction dust. This study develops a synthetic image generation method that incorporates virtual environments of construction dust for producing training samples. Three state-of-the-art object detection algorithms, including Faster-RCNN, you only look once (YOLO) and single shot detection (SSD), are trained using solely synthetic images. Finally, this research provides a comparative analysis of object detection algorithms for real-world dust monitoring regarding the accuracy and computational efficiency. This study creates a construction dust emission (CDE) dataset consisting of 3,860 synthetic dust images as the training dataset and 1,015 real-world images as the testing dataset. The YOLO-v3 model achieves the best performance with a 0.93 F1 score and 31.44 fps among all three object detection models. The experimental results indicate that training dust detection algorithms with only synthetic images can achieve acceptable performance on real-world images. This study provides insights into two questions: (1) how synthetic images could help train dust detection models to overcome data-hungry problems and (2) how well state-of-the-art deep learning algorithms can detect nonrigid construction dust.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Compliance monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1108/SASBE-04-2021-0066",
    "code_url": "https://github.com/ruoxinx/site-dust-detect",
    "thumbnail": "assets/img/models/CDE.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "CPPE",
    "title": "Pose guided anchoring for detecting proper use of personal protective equipment",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Ensuring proper use of personal protective equipment (PPE) is essential for improving workplace safety management. The authors present an extensible pose-guided anchoring framework aimed at multi-class PPE compliance detection. The overall approach harnesses a pose estimator to detect worker body parts as spatial anchors and guide the localization of part attention regions using body-knowledge-based rules considering workers' orientations and object scales. Specifically, “part attention regions” are local image patches expecting PPEs based on their inherent relationships with body parts, e.g., (head, hardhat) and (upper-body, vest). Finally, the shallow CNN-based classifiers can reliably recognize both PPE and non-PPE classes within their corresponding part attention regions. Quantitative evaluations tested on the developed construction personal protective equipment dataset (CPPE) show an overall 0.97 and 0.95 F1-score for hardhat and safety vest detection, respectively. Comparative studies with existing methods also demonstrate the higher detection accuracy and advantageous extensibility of the proposed strategy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2021.103828",
    "code_url": "https://github.com/ruoxinx/PPE-Detection-Pose",
    "thumbnail": "assets/img/models/CPPE.png",
	"added_date": "2025-09-26"
  },
   {
    "id": "ConSLAM",
    "title": "ConSLAM: Construction Dataset for SLAM",
    "authors": ["ConSLAM: Construction Dataset for SLAM"],
    "abstract": "This paper presents a data set collected periodically on a construction site. The data set aims to evaluate the performance of simultaneous localization and mapping (SLAM) algorithms used by mobile scanners or autonomous robots. It includes ground-truth scans of a construction site collected using a terrestrial laser scanner along with five sequences of spatially registered and time-synchronized images, lidar scans, and inertial data coming from our prototypical handheld scanner. We also recover the ground-truth trajectory of the mobile scanner by registering the sequential lidar scans to the ground-truth scans and show how to use a popular software package to measure the accuracy of SLAM algorithms against our trajectory automatically. To the best of our knowledge, this is the first publicly accessible data set consisting of periodically collected sequential data on a construction site.",
    "year": 2023,
    "modalities": ["Ground RGB, Point Cloud"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping/navigation"],
    "license": "Academic use only",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-5212",
    "code_url": "https://github.com/mac137/ConSLAM",
    "thumbnail": "assets/img/models/ConSLAM.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "CMA",
    "title": "Transformer-based deep learning model and video dataset for unsafe action identification in construction projects",
    "authors": ["Meng Yang", "Chengke Wu", "Yuanjun Guo", "Rui Jiang", "Feixiang Zhou", "Jianlin Zhang", "Zhile Yang"],
    "abstract": "A large proportion of construction accidents are caused by unintentional and unsafe actions and behaviors. It is of significant difficulties and ineffectiveness to monitor unsafe behaviors using conventional manual supervision due to the complex and dynamic working conditions on construction sites. Recently, surveillance videos and computer vision (CV) techniques have been increasingly adopted to automatically identify risky behaviors. However, the challenge remains that spatial and temporal features in video clips cannot be effectively captured and fused by current CV models. To address this challenge, this paper describes a deep learning model named Spatial Temporal Relation Transformer (STR-Transformer), where spatial and temporal features of work behaviors are simultaneously extracted in paralleling video streams and then fused by a specially designed module. To verify the effectiveness of the STR-Transformer, a customized dataset is developed, including seven categories of construction worker behaviors and 1595 video clips. In numerical experiments and case studies, the STR-Transformer achieves an average precision of 88.7%, 4.0% higher than the baseline model. The STR-Transformer enables more accurate and reliable automatic safety surveillance on construction projects, and is expected to reduce accident rates and management costs. Moreover, the performance of STR-Transformer relies on efficient feature integration, which may inspire future studies to identify, extract, and fuse richer features when applying CV-based deep learning models in construction management.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Action recognition"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104703",
    "code_url": "https://github.com/S1mpleyang/ConstructionActionRecognition",
    "thumbnail": "assets/img/models/CMA.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "VisualSiteDiary",
    "title": "VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals",
    "authors": ["Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Mani Golparvar-Fard"],
    "abstract": "This paper presents VisualSiteDiary, a Vision Transformer-based image captioning model which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. As a model for deciphering construction photologs, VisualSiteDiary incorporates pseudo-region features, utilizes high-level knowledge in pretraining, and fine-tunes for diverse captioning styles. To validate VisualSiteDiary, a new image captioning dataset, VSD, is presented. This dataset includes many realistic yet challenging cases commonly observed in commercial building projects. Experimental results using five different metrics demonstrate that VisualSiteDiary provides superior-quality captions compared to the state-of-the-art image captioning models. Excluding the task of object recognition, the presented model also outperformed mPLUG –the state-of-the-art visual-language model– in the image retrieval task by 0.6% in precision and 0.9% in recall, respectively. Detailed discussions illustrate practical examples on how VisualSiteDiary improves the process of creating daily construction reports, paving the way for future developments in the field.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105483",
    "code_url": "https://github.com/joonv2/VisualSiteDiary",
    "thumbnail": "assets/img/models/VisualSiteDiary.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "Small-Construction-Tools",
    "title": "Utilizing synthetic images to enhance the automated recognition of small-sized construction tools",
    "authors": ["Soeun Han", "Wonjun Park", "Kyumin Jeong", "Taehoon Hong", "Choongwan Koo"],
    "abstract": "Previous studies on vision-based classifiers often overlooked the need for detecting small-sized construction tools. Considering the substantial variations in these tools' size and shape, it is essential to train models using synthetic images that encompass diverse angles and distances. This study aimed to improve the performance of classifiers for small-sized construction tools by leveraging synthetic data. Three classifiers were proposed using YOLOv8 algorithm, varying in data composition: (i) ‘Real-4000’: 4000 authentic images; (ii) ‘Hybrid-4000’: 2000 authentic and 2000 synthetic images; (iii) ‘Hybrid-8000’: 4000 authentic and 4000 synthetic images. To assess practical applicability, a test dataset of 144 samples for each type was collected directly from construction sites. Results revealed that the ‘Hybrid-8000’ model, utilizing synthetic images, excelled at 94.8% of mAP_0.5. This represented a significant 15.2% improvement, affirming its practical applicability. These classifiers hold promise for enhancing safety and advancing real-time automation and robotics in construction.",
    "year": 2024,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105415",
    "code_url": "https://github.com/SenseableSpace/Detection-Small-Construction-Tools-Synthetic-Images",
    "thumbnail": "assets/img/models/Small-Construction-Tools.png",
	"added_date": "2025-09-27"
  }
]
