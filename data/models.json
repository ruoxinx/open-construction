[
  {
    "id": "GDUT-HWD",
    "title": "Automatic detection of hardhats worn by construction personnel: A deep learning approach and benchmark dataset",
    "authors": ["Jixiu Wu", "Nian Cai", "Wenjie Chen", "Huiheng Wang", "Guotian Wang"],
    "abstract": "Hardhats play an essential role in protecting construction individuals from accidents. However, wearing hardhats is not strictly enforced among workers due to all kinds of reasons. To enhance construction sites safety, the majority of existing works monitor the presence and proper use of hardhats through multi-stage data processing, which come with limitations on adaption and generalizability. In this paper, a one-stage system based on convolutional neural network is proposed to automatically monitor whether construction personnel are wearing hardhats and identify the corresponding colors. To facilitate the study, this work constructs a new and publicly available hardhat wearing detection benchmark dataset, which consists of 3174 images covering various on-site conditions. Then, features from different layers with different scales are fused discriminately by the proposed reverse progressive attention to generate a new feature pyramid, which will be fed into the Single Shot Multibox Detector (SSD) to predict the final detection results. The proposed system is trained by an end-to-end scheme. The experimental results demonstrate that the proposed system is effective under all kinds of on-site conditions, which can achieve 83.89% mAP (mean average precision) with the input size 512×512.",
    "year": 2019,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Apache-2.0",
    "paper_url": "https://doi.org/10.1016/j.autcon.2019.102894",
    "code_url": "https://github.com/wujixiu/helmet-detection",
    "thumbnail": "assets/img/models/GDUT-HWD.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Construction-Activity-Scenes",
    "title": "Manifesting construction activity scenes via image captioning",
    "authors": ["Huan Liu", "Guangbin Wang", "Ting Huang", "Ping He", "Martin Skitmore", "Xiaochun Luo"],
    "abstract": "This study proposed an automated method for manifesting construction activity scenes by image captioning – an approach rooted in computer vision and natural language generation. A linguistic description schema for manifesting the scenes is developed initially and two unique dedicated image captioning datasets are created for method validation. A general model architecture of image captioning is then instituted by combining an encoder-decoder framework with deep neural networks, followed by three experimental tests involving the selection of model learning strategies and performance evaluation metrics. It is demonstrated the method's performance is comparable with that of state-of-the-art computer vision methods in general. The paper concludes with a discussion of the feasibility of the practical application of the proposed approach at the current technical level.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103334",
    "code_url": "https://github.com/HannahHuanLIU/AEC-image-captioning",
    "thumbnail": "assets/img/models/Construction-Activity-Scenes.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Kenki-Posi",
    "title": "Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects",
    "authors": ["Runqiu Bao","Ren Komatsu","Renato Miyagusuku","Masaki Chino", "Atsushi Yamashita", "Hajime Asama"],
    "abstract": "At modern construction sites, utilizing GNSS (Global Navigation Satellite System) to measure the real-time location and orientation (i.e. pose) of construction machines and navigate them is very common. However, GNSS is not always available. Replacing GNSS with on-board cameras and visual simultaneous localization and mapping (visual SLAM) to navigate the machines is a cost-effective solution. Nevertheless, at construction sites, multiple construction machines will usually work together and side-by-side, causing large dynamic occlusions in the cameras' view. Standard visual SLAM cannot handle large dynamic occlusions well. In this work, we propose a motion segmentation method to efficiently extract static parts from crowded dynamic scenes to enable robust tracking of camera ego-motion. Our method utilizes semantic information combined with object-level geometric constraints to quickly detect the static parts of the scene. Then, we perform a two-step coarse-to-fine ego-motion tracking with reference to the static parts. This leads to a novel dynamic visual SLAM formation. We test our proposals through a real implementation based on ORB-SLAM2, and datasets we collected from real construction sites. The results show that when standard visual SLAM fails, our method can still retain accurate camera ego-motion tracking in real-time. Comparing to state-of-the-art dynamic visual SLAM methods, ours shows outstanding efficiency and competitive result trajectory accuracy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "GPL-3.0",
    "paper_url": "https://doi.org/10.1080/01691864.2020.1869586",
    "code_url": "https://github.com/RunqiuBao/kenki-positioning-vSLAM",
    "thumbnail": "assets/img/models/Kenki-Posi.gif",
	"added_date": "2025-09-26"
  },
  {
    "id": "CDE",
    "title": "Machine learning using synthetic images for detecting dust emissions on construction sites",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Automated dust monitoring in workplaces helps provide timely alerts to over-exposed workers and effective mitigation measures for proactive dust control. However, the cluttered nature of construction sites poses a practical challenge to obtain enough high-quality images in the real world. The study aims to establish a framework that overcomes the challenges of lacking sufficient imagery data (“data-hungry problem”) for training computer vision algorithms to monitor construction dust. This study develops a synthetic image generation method that incorporates virtual environments of construction dust for producing training samples. Three state-of-the-art object detection algorithms, including Faster-RCNN, you only look once (YOLO) and single shot detection (SSD), are trained using solely synthetic images. Finally, this research provides a comparative analysis of object detection algorithms for real-world dust monitoring regarding the accuracy and computational efficiency. This study creates a construction dust emission (CDE) dataset consisting of 3,860 synthetic dust images as the training dataset and 1,015 real-world images as the testing dataset. The YOLO-v3 model achieves the best performance with a 0.93 F1 score and 31.44 fps among all three object detection models. The experimental results indicate that training dust detection algorithms with only synthetic images can achieve acceptable performance on real-world images. This study provides insights into two questions: (1) how synthetic images could help train dust detection models to overcome data-hungry problems and (2) how well state-of-the-art deep learning algorithms can detect nonrigid construction dust.",
    "year": 2021,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Compliance monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1108/SASBE-04-2021-0066",
    "code_url": "https://github.com/ruoxinx/site-dust-detect",
    "thumbnail": "assets/img/models/CDE.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "CPPE",
    "title": "Pose guided anchoring for detecting proper use of personal protective equipment",
    "authors": ["Ruoxin Xiong", "Pingbo Tang"],
    "abstract": "Ensuring proper use of personal protective equipment (PPE) is essential for improving workplace safety management. The authors present an extensible pose-guided anchoring framework aimed at multi-class PPE compliance detection. The overall approach harnesses a pose estimator to detect worker body parts as spatial anchors and guide the localization of part attention regions using body-knowledge-based rules considering workers' orientations and object scales. Specifically, “part attention regions” are local image patches expecting PPEs based on their inherent relationships with body parts, e.g., (head, hardhat) and (upper-body, vest). Finally, the shallow CNN-based classifiers can reliably recognize both PPE and non-PPE classes within their corresponding part attention regions. Quantitative evaluations tested on the developed construction personal protective equipment dataset (CPPE) show an overall 0.97 and 0.95 F1-score for hardhat and safety vest detection, respectively. Comparative studies with existing methods also demonstrate the higher detection accuracy and advantageous extensibility of the proposed strategy.",
    "year": 2021,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2021.103828",
    "code_url": "https://github.com/ruoxinx/PPE-Detection-Pose",
    "thumbnail": "assets/img/models/CPPE.png",
	"added_date": "2025-09-26"
  },
   {
    "id": "ConSLAM",
    "title": "ConSLAM: Construction Dataset for SLAM",
    "authors": ["ConSLAM: Construction Dataset for SLAM"],
    "abstract": "This paper presents a data set collected periodically on a construction site. The data set aims to evaluate the performance of simultaneous localization and mapping (SLAM) algorithms used by mobile scanners or autonomous robots. It includes ground-truth scans of a construction site collected using a terrestrial laser scanner along with five sequences of spatially registered and time-synchronized images, lidar scans, and inertial data coming from our prototypical handheld scanner. We also recover the ground-truth trajectory of the mobile scanner by registering the sequential lidar scans to the ground-truth scans and show how to use a popular software package to measure the accuracy of SLAM algorithms against our trajectory automatically. To the best of our knowledge, this is the first publicly accessible data set consisting of periodically collected sequential data on a construction site.",
    "year": 2023,
    "modalities": ["Ground RGB, Point Cloud"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Academic Use Only",
    "paper_url": "https://doi.org/10.1061/JCCEE5.CPENG-5212",
    "code_url": "https://github.com/mac137/ConSLAM",
    "thumbnail": "assets/img/models/ConSLAM.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "CMA",
    "title": "Transformer-based deep learning model and video dataset for unsafe action identification in construction projects",
    "authors": ["Meng Yang", "Chengke Wu", "Yuanjun Guo", "Rui Jiang", "Feixiang Zhou", "Jianlin Zhang", "Zhile Yang"],
    "abstract": "A large proportion of construction accidents are caused by unintentional and unsafe actions and behaviors. It is of significant difficulties and ineffectiveness to monitor unsafe behaviors using conventional manual supervision due to the complex and dynamic working conditions on construction sites. Recently, surveillance videos and computer vision (CV) techniques have been increasingly adopted to automatically identify risky behaviors. However, the challenge remains that spatial and temporal features in video clips cannot be effectively captured and fused by current CV models. To address this challenge, this paper describes a deep learning model named Spatial Temporal Relation Transformer (STR-Transformer), where spatial and temporal features of work behaviors are simultaneously extracted in paralleling video streams and then fused by a specially designed module. To verify the effectiveness of the STR-Transformer, a customized dataset is developed, including seven categories of construction worker behaviors and 1595 video clips. In numerical experiments and case studies, the STR-Transformer achieves an average precision of 88.7%, 4.0% higher than the baseline model. The STR-Transformer enables more accurate and reliable automatic safety surveillance on construction projects, and is expected to reduce accident rates and management costs. Moreover, the performance of STR-Transformer relies on efficient feature integration, which may inspire future studies to identify, extract, and fuse richer features when applying CV-based deep learning models in construction management.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Action recognition"],
	"applications": ["Safety monitoring"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104703",
    "code_url": "https://github.com/S1mpleyang/ConstructionActionRecognition",
    "thumbnail": "assets/img/models/CMA.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "VisualSiteDiary",
    "title": "VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals",
    "authors": ["Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Mani Golparvar-Fard"],
    "abstract": "This paper presents VisualSiteDiary, a Vision Transformer-based image captioning model which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. As a model for deciphering construction photologs, VisualSiteDiary incorporates pseudo-region features, utilizes high-level knowledge in pretraining, and fine-tunes for diverse captioning styles. To validate VisualSiteDiary, a new image captioning dataset, VSD, is presented. This dataset includes many realistic yet challenging cases commonly observed in commercial building projects. Experimental results using five different metrics demonstrate that VisualSiteDiary provides superior-quality captions compared to the state-of-the-art image captioning models. Excluding the task of object recognition, the presented model also outperformed mPLUG –the state-of-the-art visual-language model– in the image retrieval task by 0.6% in precision and 0.9% in recall, respectively. Detailed discussions illustrate practical examples on how VisualSiteDiary improves the process of creating daily construction reports, paving the way for future developments in the field.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Image captioning"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105483",
    "code_url": "https://github.com/joonv2/VisualSiteDiary",
    "thumbnail": "assets/img/models/VisualSiteDiary.gif",
	"added_date": "2025-09-26"
  },
  {
    "id": "Small-Construction-Tools",
    "title": "Utilizing synthetic images to enhance the automated recognition of small-sized construction tools",
    "authors": ["Soeun Han", "Wonjun Park", "Kyumin Jeong", "Taehoon Hong", "Choongwan Koo"],
    "abstract": "Previous studies on vision-based classifiers often overlooked the need for detecting small-sized construction tools. Considering the substantial variations in these tools' size and shape, it is essential to train models using synthetic images that encompass diverse angles and distances. This study aimed to improve the performance of classifiers for small-sized construction tools by leveraging synthetic data. Three classifiers were proposed using YOLOv8 algorithm, varying in data composition: (i) ‘Real-4000’: 4000 authentic images; (ii) ‘Hybrid-4000’: 2000 authentic and 2000 synthetic images; (iii) ‘Hybrid-8000’: 4000 authentic and 4000 synthetic images. To assess practical applicability, a test dataset of 144 samples for each type was collected directly from construction sites. Results revealed that the ‘Hybrid-8000’ model, utilizing synthetic images, excelled at 94.8% of mAP_0.5. This represented a significant 15.2% improvement, affirming its practical applicability. These classifiers hold promise for enhancing safety and advancing real-time automation and robotics in construction.",
    "year": 2024,
    "modalities": ["Ground RGB (real and synthetic)"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105415",
    "code_url": "https://github.com/SenseableSpace/Detection-Small-Construction-Tools-Synthetic-Images",
    "thumbnail": "assets/img/models/Small-Construction-Tools.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "OSHA-safety",
    "title": "Deep Learning Enabled Computer Vision Model for Automated Safety Compliance in Construction Environments",
    "authors": ["Amr A. Mohy", "Hesham A. Bassioni", "Elbadr O. Elgendi", "Tarek M. Hassan"],
    "abstract": "Construction site safety demands proactive hazard detection, a challenge traditionally met with reactive measures that are often inadequate. This paper introduces a novel deep learning-based computer vision model designed for automated safety compliance monitoring, addressing critical limitations of existing approaches. The model utilizes a modified one-stage object detection algorithm, uniquely enhanced with Contextual Transformer Networks (CoTs), a Triplet Attention module, Activate or Not (ACON) activation functions, and Content-Aware Reassembly of Features (CARAFE) up-sampling, to significantly improve feature extraction, visual recognition, and contextual understanding in complex construction environments. To support this model development, a new OSHA-data-driven dataset of 55,594 images across 28 safety categories was developed. This dataset encompasses personal protective equipment (PPE), scaffolding, materials, hazards, and worker actions, ensuring comprehensive coverage of key safety domains. The Wise-Intersection over Union (IoU) loss function further refines bounding box regression, enhancing localization accuracy. Evaluations on both a benchmarking dataset and the newly developed dataset demonstrate the model's benchmark-surpassing performance (Precision: 0.89, mAP95: 0.45). This research offers a practically viable, data-driven solution for a critical industry challenge, moving towards a future of zero-accident construction sites.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Not specified",
    "paper_url": "http://dx.doi.org/10.36680/j.itcon.2025.057",
    "code_url": "https://github.com/amr21006/PhD_Safety_Management_Object_Detection",
    "thumbnail": "assets/img/models/OSHA-safety.png",
	"added_date": "2025-09-25"
  },
    {
    "id": "repetitive-action",
    "title": "Automatic repetitive action counting for construction worker ergonomic assessment",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Work-related musculoskeletal disorders are the primary cause of nonfatal occupational injuries in the construction industry. Accurate ergonomic assessment is essential to reduce the risk of work-related injuries. Repetitive work significantly contributes to musculoskeletal injuries, and various ergonomic evaluation methods have specific criteria for assessing repetitive actions. However, most existing methods for repetitive motions primarily rely on subjective and time-consuming manual observation. To accurately assess ergonomic risk, an automatic and precise method is required to count repetitive actions in construction work. This poses a challenge due to the unstructured nature of construction actions and their varying frequencies and cycles. This paper aims to overcome these challenges by identifying repetitive unstructured actions using posture self-similarity comparison and predicting construction actions' length using a transformer layer. Experimental results demonstrated that the proposed method achieved a 91.5 % accuracy in identifying repetitive actions. The research results will contribute to the promotion of accurate ergonomics evaluation of automation.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Pose estimation"],
	"applications": ["Ergonomic assessment"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105726",
    "code_url": "https://github.com/Chenxy875/Repetitive-Actions-Counting-Method-for-Construction-Workers-Ergonomic-Assessments",
    "thumbnail": "assets/img/models/repetitive-action.gif",
	"added_date": "2025-09-27"
  },
  {
    "id": "Pictor-v3",
    "title": "Deep Learning for Site Safety: Real-Time Detection of Personal Protective Equipment",
    "authors": ["Nipun D. Nath", "Amir H. Behzadan", "Stephanie G. Paal"],
    "abstract": "The leading causes of construction fatalities include traumatic brain injuries (resulted from fall and electrocution) and collisions (resulted from struck by objects). As a preventive step, the U.S. Occupational Safety and Health Administration (OSHA) requires that contractors enforce and monitor appropriate usage of personal protective equipment (PPE) of workers (e.g., hard hat and vest) at all times. This paper presents three deep learning (DL) models built on You-Only-Look-Once (YOLO) architecture to verify PPE compliance of workers; i.e., if a worker is wearing hard hat, vest, or both, from image/video in real-time. In the first approach, the algorithm detects workers, hats, and vests and then, a machine learning model (e.g., neural network and decision tree) verifies if each detected worker is properly wearing hat or vest. In the second approach, the algorithm simultaneously detects individual workers and verifies PPE compliance with a single convolutional neural network (CNN) framework. In the third approach, the algorithm first detects only the workers in the input image which are then cropped and classified by CNN-based classifiers (i.e., VGG-16, ResNet-50, and Xception) according to the presence of PPE attire. All models are trained on an in-house image dataset that is created using crowd-sourcing and web-mining. The dataset, named Pictor-v3, contains ~1,500 annotated images and ~4,700 instances of workers wearing various combinations of PPE components. It is found that the second approach achieves the best performance, i.e., 72.3% mean average precision (mAP), in real-world settings, and can process 11 frames per second (FPS) on a laptop computer which makes it suitable for real-time detection, as well as a good candidate for running on light-weight mobile devices. The closest alternative in terms of performance (67.93% mAP) is the third approach where VGG-16, ResNet-50, and Xception classifiers are assembled in a Bayesian framework. However, the first approach is the fastest among all and can process 13 FPS with 63.1% mAP. The crowed-sourced Pictor-v3 dataset and all trained models are publicly available to support the design and testing of other innovative applications for monitoring safety compliance, and advancing future research in automation in construction.",
    "year": 2020,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2020.103085",
    "code_url": "https://github.com/ciber-lab/pictor-ppe",
    "thumbnail": "assets/img/models/Pictor-v3.gif",
	"added_date": "2025-09-27"
  },
    {
    "id": "ABECIS",
    "title": "Developing a Free and Open-Source Semi-Automated Building Exterior Crack Inspection Software for Construction and Facility Managers",
    "authors": ["Nipun D. Nath", "Amir H. Behzadan", "Stephanie G. Paal"],
	"abstract":"Inspection of cracks is an important process for properly monitoring and maintaining a building. However, manual crack inspection is time-consuming, inconsistent, and dangerous (e.g., in tall buildings). Due to the development of open-source AI technologies, the increase in available Unmanned Aerial Vehicles (UAVs) and the availability of smartphone cameras, it has become possible to automate the building crack inspection process. This study presents the development of an easy-to-use, free and open-source Automated Building Exterior Crack Inspection Software (ABECIS) for construction and facility managers, using state-of-the-art segmentation algorithms to identify concrete cracks and generate a quantitative and qualitative report. ABECIS was tested using images collected from a UAV and smartphone cameras in real-world conditions and a controlled laboratory environment. From the raw output of the algorithm, the median Intersection over Unions (IoU) for the test experiments are (1) 0.686 for indoor crack detection experiment in a controlled lab environment using a commercial drone, (2) 0.186 for indoor crack detection at a construction site using a smartphone and (3) 0.958 for outdoor crack detection on university campus using a commercial drone. These IoU results can be improved significantly to over 0.8 when a human operator selectively removes the false positives. In general, ABECIS performs best for outdoor drone images, and combining the algorithm predictions with human verification/intervention offers very accurate crack detection results. The software is available publicly and can be downloaded for out-of-the-box use.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection","Semantic segmentation"],
	"applications": ["Quality control"],
    "license": "MIT",
    "paper_url": "https://doi.org/10.1109/ACCESS.2023.3296793",
    "code_url": "https://github.com/SMART-NYUAD/ABECIS",
    "thumbnail": "assets/img/models/ABECIS.png",
	"added_date": "2025-09-27"
  },
    {
    "id": "MGCAR",
    "title": "Multi-granular crew activity recognition for construction monitoring",
    "authors": ["Cheng Yun Tsai", "Mik Wanul Khosiin", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "The labor force is vital to construction projects, but traditional manual methods for productivity analysis are time-consuming and error-prone. Recent advancements in computer vision and deep learning offer automated solutions, yet most studies focus on low-level pose recognition, neglecting the collaborative dynamics of construction sites. This paper introduces a multi-granular crew activity recognition framework that identifies individual actions, groups collaborating workers, and links them to specific tasks. Using graph-based representations and self-attention mechanisms, the model integrates spatial and contextual information for accurate recognition. Experiments on a dataset covering rebar, formwork, and concrete operations show an overall F1 Score of 70.31%. Results highlight the importance of balancing visual features and spatial proximity for optimal performance. This framework offers an efficient solution for construction site monitoring and lays groundwork for future research on temporal modeling and human-object interaction analysis.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Crew activity recognition"],
	"applications": ["Productivity monitoring"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106428",
    "code_url": "https://github.com/JimmyTsai-Yun/MGCAR",
    "thumbnail": "assets/img/models/MGCAR.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "AID-AI-Infraction-Detection",
    "title": "Exploring a Multimodal Conversational Agent for Construction Site Safety: A Low-Code Approach to Hazard Detection and Compliance Assessment",
    "authors": ["Giancarlo de Marco", "Elias Niederwieser", "Dietmar Siegele"],
    "abstract": "This paper discusses the viability of using a low-code multimodal large language model agent with computer vision functionality to support occupational safety and health evaluations on construction sites. The central hypothesis aims to verify that these systems can provide reliable answers, as evaluated against a ground truth review, including the identification of high-risk dangers. A conversational agent was given the task of finding hazards and checking for national legislative compliance within a dataset of 100 real-world construction photos. The comparison of the agent’s results to the ground truth provides insight into current limitations. The primary issues identified were inconsistent taxonomies, inadequate causal reasoning, and insufficient contextual consideration, all of which adversely impacted performance—particularly when analyzing low-resolution images. The metrics supporting the conclusion synthesize that this tool is a valuable augmentation technology, enhancing safety evaluations while still requiring human supervision to ensure reliability.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Vision–language reasoning"],
	"applications": ["Safety monitoring"],
    "license": "Non-Commercial Use Only",
    "paper_url": "https://doi.org/10.3390/buildings15183352",
    "code_url": "https://github.com/fraunhofer-italia/AID-AI-Infraction-Detection",
    "thumbnail": "assets/img/models/AID-AI-Infraction-Detection.png",
	"added_date": "2025-09-26"
  },
    {
    "id": "ETHcavation",
    "title": "ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments",
    "authors": ["Lorenzo Terenzi", "Julian Nubert", "Pol Eyschen", "Pascal Roth", "Simin Fei", "Edo Jelavic", "Marco Hutter"],
	"abstract": "Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset and code for training and deployment are publicly available to support future research.",
    "year": 2025,
    "modalities": ["Ground RGB, 3D Point Cloud"],
    "tasks": ["Semantic segmentation, Object tracking"],
	"applications": ["Site mapping and navigation"],
    "license": "Not specified",
    "paper_url": "https://arxiv.org/abs/2410.04250",
    "code_url": "https://github.com/leggedrobotics/rsl_panoptic_mapping",
    "thumbnail": "assets/img/models/ETHcavation.gif",
	"added_date": "2025-09-28"
  },
  {
    "id": "Buildee",
    "title": "Buildee: A 3D Simulation Framework for Scene Exploration and Reconstruction with Understanding",
    "authors": ["Clementin Boittiaux", "Vincent Lepetit"],
    "abstract": "We introduce Buildee, a 3D simulation framework designed to benchmark scene exploration, 3D reconstruction, and semantic segmentation tasks in both static and dynamic environments. Built as a Python module on top of Blender, Buildee leverages its advanced rendering capabilities to generate realistic RGB, depth, and semantic data while enabling 2D / 3D point tracking and occlusion checking. Additionally, we provide a procedural generator for construction site environments and baseline methods for key computer vision tasks. Through Buildee, we establish a standardized platform for evaluating scene understanding algorithms in realistic settings. Our code is publicly available at https://github.com/clementinboittiaux/buildee.",
    "year": 2025,
    "modalities": ["Ground RGB, depth, synthetic"],
    "tasks": ["Scene generation"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://openreview.net/pdf?id=1LmsiOaMTy",
    "code_url": "https://github.com/clementinboittiaux/buildee",
    "thumbnail": "assets/img/models/Buildee.gif",
	"added_date": "2025-09-28"
  },
   {
    "id": "LOD3",
    "title": "Generating LOD3 building models from structure-from-motion and semantic segmentation",
    "authors": ["Bryan German Pantoja Rosero", "Radhakrishna Achanta", "Mateusz Kozinski", "Pascal Fua", "Fernando Perez-Cruz", "Katrin Beyer"],
	"abstract": "This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["3D reconstruction"],
	"applications": ["Site understanding"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.autcon.2022.104430",
    "code_url": "https://github.com/eesd-epfl/LOD3_buildings",
    "thumbnail": "assets/img/models/LOD3.png",
	"added_date": "2025-09-26"
  },
    {
    "id": "Open",
    "title": "Facilitating Construction Scene Understanding Knowledge Sharing and Reuse via Lifelong Site Object Detection",
    "authors": ["Ruoxin Xiong", "Yuansheng Zhu", "Yanyu Wang", "Pengkun Liu", "Pingbo Tang"],
	"abstract": "Automatically recognizing diverse construction resources (e.g., workers and equipment) from construction scenes supports efficient and intelligent workplace management. Previous studies have focused on identifying fixed object categories in specific contexts, but they have difficulties in accumulating existing knowledge while extending the model for handling additional classes in changing applications. This work proposes a novel lifelong construction resource detection framework for continuously learning from dynamic changing contexts without catastrophically forgetting previous knowledge. In particular, we contribute: (1) an OpenConstruction Dataset with 31 unique object categories, integrating three large datasets for validating lifelong object detection algorithms; (2) an OpenConstruction Taxonomy, unifying heterogeneous label space from various scenarios; and (3) an informativeness-based lifelong object detector that leverages very limited examples from previous learning tasks and adds new data progressively. We train and evaluate the proposed method on the OpenConstruction Dataset in sequential data streams and show mAP improvements on the overall task. Code is available at https://github.com/YUZ128pitt/OpenConstruction.",
    "year": 2022,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1007/978-3-031-25082-8_15",
    "code_url": "https://github.com/YUZ128pitt/OpenConstruction",
    "thumbnail": "assets/img/models/Open.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "Worker-Safety-Twin",
    "title": "Advancing construction site workforce safety monitoring through BIM and computer vision integration",
    "authors": ["Almo Senja Kulinan", "Minsoo Park", "Pa Pa Win Aung", "Gichun Cha", "Seunghee Park"],
    "abstract": "Ensuring a safe work environment is crucial for construction projects. It is essential that workforce monitoring is both efficient and non-intrusive to the ongoing construction activities. This paper introduces a method that integrates building information modeling (BIM) and computer vision to monitor workforce safety hazards at construction sites in real time. Despite the rising adoption of BIM and computer vision individually within the construction sector, the potential of their integrated application as a cohesive system for workforce safety monitoring remains unexplored. While BIM provides rich 3D semantic information about the construction site, computer vision captures real-time field data. The system was tested using a realistic construction simulation, and the accuracy of the position estimate was evaluated in a real-world interior environment, yielding a mean error distance (MED) of 13.2 cm. Overall, the findings have substantial significance for the construction industry to help minimize accidents and enhance overall worker safety.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.105227",
    "code_url": "https://github.com/almosenja/Worker-Safety-Twin",
    "thumbnail": "assets/img/models/Worker-Safety-Twin.gif",
	"added_date": "2025-09-28"
  },
  {
    "id": "compliance-checking",
    "title": "Automated rule-based safety inspection and compliance checking of temporary guardrail systems in construction",
    "authors": ["K.W. Johansen", "J. Teizer", "C. Schultz"],
    "abstract": "The construction industry records more hazards compared to any other sector. Protective equipment, such as guardrail systems, is essential for protecting workers from deadly falls but may quickly become incompliant after installation. Yet, many construction projects do not have the resources to dedicate personnel to perform the inspection as frequently as needed. Therefore, this paper proposes an automated rule-based inspection and compliance-checking system that can assist the responsible personnel in detecting faulty guardrails in live work environments. The classification approach utilizes safety design and mimics the steps of human guardrailing compliance assessment, which enforces simplicity and transparency, allowing the human domain expert to remain in control. Even under scarce data availability, this first-of-a-kind classification approach is reliable and scalable and successfully classifies 21 predefined and 9 validation scenarios of guardrail systems for fall protection.",
    "year": 2024,
    "modalities": ["Aerial RGB"],
    "tasks": ["Knowledge reasoning"],
	"applications": ["Safety monitoring"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105849",
    "code_url": "https://github.com/kakke14/Automated-inspection-and-compliance-checking",
    "thumbnail": "assets/img/models/compliance-checking.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "URRE-Net",
    "title": "An unsupervised low-light image enhancement method for improving V-SLAM localization in uneven low-light construction sites",
    "authors": ["Xinyu Chen", "Yantao Yu"],
    "abstract": "Construction robots have been increasingly adopted in construction projects to improve productivity, reduce risk, and speed up work cycles. Visual Simultaneous Localization and Mapping (V-SLAM) technology is widely used in construction robots due to its lightweight weight, cost-effectiveness, and provision of semantic information. On real construction sites, lighting conditions are often challenging due to factors such as uneven lighting intensity, inadequate exposure, or robots in backlit positions (e.g. roughcast houses without artificial lighting, underground car parks), which cause images captured under these conditions to exhibit low signal-to-noise ratio, uneven lighting, color distortion, noise, and other problems. These challenges make the extraction and matching of feature points in V-SLAM difficult, resulting in significant positioning errors or the inability to generate positioning trajectories for construction robotics. Although existing methods for enhancing low-light images can certainly improve image brightness, they still cannot effectively address the localization challenges brought about by low-light construction scenes with uneven illumination and noise. To address the interference of uneven and changing lighting conditions on V-SLAM localization in construction sites, this paper proposes the Unsupervised Reflectance Retinex and Noise model (URRN-Net) to enhance low-light construction images. By using a CNN model based on the Retinex theory to decompose the illumination characteristics of images, we achieve effective brightness restoration of uneven low-light images by utilizing unsupervised loss functions. URRE-Net can reduce the root mean square localization error by >65% compared to low-light images in the ORB-SLAM3 method, and the maximum error is reduced by >71% in on-site experiments. The proposed URRE-Net can be integrated with existing V-SLAM algorithms to provide more robust localization services for low-light construction site applications such as building operations (e.g., interior wall spraying robotic) or construction management tasks (e.g., automatic tunnel inspection).",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Simultaneous localization and mapping"],
	"applications": ["Site mapping and navigation"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105404",
    "code_url": "https://github.com/Chenxy875/URRE-Net",
    "thumbnail": "assets/img/models/URRE-Net.gif",
	"added_date": "2025-09-27"
  },
  {
    "id": "LMs_ConstructionSafety",
    "title": "Performance comparison of retrieval-augmented generation and fine-tuned large language models for construction safety management knowledge retrieval",
    "authors": ["Jungwon Lee", "Seungjun Ahn", "Daeho Kim", "Dongkyun Kim"],
    "abstract": "Construction safety standards are in unstructured formats like text and images, complicating their effective use in daily tasks. This paper compares the performance of Retrieval-Augmented Generation (RAG) and fine-tuned Large Language Model (LLM) for the construction safety knowledge retrieval. The RAG model was created by integrating GPT-4 with a knowledge graph derived from construction safety guidelines, while the fine-tuned LLM was fine-tuned using a question-answering dataset derived from the same guidelines. These models' performance is tested through case studies, using accident synopses as a query to generate preventive measurements. The responses were assessed using metrics, including cosine similarity, Euclidean distance, BLEU, and ROUGE scores. It was found that both models outperformed GPT-4, with the RAG model improving by 21.5 % and the fine-tuned LLM by 26 %. The findings highlight the relative strengths and weaknesses of the RAG and fine-tuned LLM approaches in terms of applicability and reliability for safety management.",
    "year": 2024,
    "modalities": ["Document"],
    "tasks": ["Knowledge retrieval"],
	"applications": ["Compliance checking"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105846",
    "code_url": "https://github.com/juuuungwon/LMs_ConstructionSafety",
    "thumbnail": "assets/img/models/LMs_ConstructionSafety.png",
	"added_date": "2025-09-27"
  },
  {
    "id": "ego",
    "title": "Egocentric camera-based method for detecting static hazardous objects on construction sites",
    "authors": ["Ziming Liu", "Jiuyi Xu", "Christine Wun Ki Suen", "Meida Chen", "Zhengbo Zou", "Yangming Shi"],
    "abstract": "The construction site is a hazardous workplace, accounting for more than 20 % of worker fatalities compared to other industries in the United States. Predominant causes of these fatalities are slips, trips, and falls (STFs). Therefore, identifying hazardous objects on construction sites that could lead to STFs is crucial for enhancing construction safety. Previous studies using fixed-position cameras often miss observations of obstructed or hidden objects. This paper proposes an alternative approach using safety helmets with lightweight wide-angle cameras and leveraging open-vocabulary object detection (OVOD) methods to identify hazardous objects on construction sites that could lead to STFs. In addition, an egocentric view dataset specifically for construction sites was created and released for benchmarking purposes. Research results indicated a 79.0 % weighted F1-score in classifying static hazardous objects on construction sites. This proposed system has the potential to enhance construction safety and provide a valuable dataset for future construction safety research.",
    "year": 2025,
    "modalities": ["Egocentric RGB"],
    "tasks": ["object detection"],
	"applications": ["Safety monitoring"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2025.106048",
    "code_url": "https://github.com/JiuyiX/Tripping-Hazard-Detection-based-on-OVOD",
    "thumbnail": "assets/img/models/ego.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "YOLO-FAS",
    "title": "YOLO-FAS: A lightweight model for detecting rebar intersections location and tying status",
    "authors": ["Hao Duan", "Mingming Yu", "Tengfeng Ai", "Mengmeng Zhu", "Haili Jiang", "Shuai Guo"],
    "abstract": "Current deep learning (DL) algorithms for detecting complex rebar mesh intersection points rely heavily on large amounts of training data to ensure recognition accuracy and generalization capabilities. However, deploying such algorithms on small mobile robots is challenging due to limited computational resources, making it difficult to achieve both real-time performance and high precision. To address this issue, this paper proposes a lightweight YOLO-FAS model specifically designed for rebar intersections detection on tying robots. This model is based on YOLOv5s and optimizes the FastNet structure by introducing the PGConv module, enhances feature fusion with the AFPN (Asymptotic feature pyramid network) module, and improves localization accuracy using the EIOU loss function. Experimental results demonstrate that the model's parameters and computational load are reduced by 60.6% and 62%, respectively. Furthermore, through BN (Batch Normalization) channel pruning and QAT (Quantization Aware Training), the model is compressed, and inference is accelerated using TensorRT. The improved YOLO-FAS model achieves a FPS (frames per second) increase of 84.1% in FP32 mode and 33.2% in INT8 mode. Finally, after real-world deployment testing, the system's average memory usage is reduced by 0.77GB, and the accuracy of recognition of intersection points reaches 98.21%, representing an improvement of 3.04% over YOLOv5s. The results indicate that this method effectively achieves model lightweighting while ensuring efficient and accurate detection of rebar intersection points, demonstrating robust performance and promising application prospects.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Site understanding"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.neucom.2025.129485",
    "code_url": "https://github.com/5204338/yolov5_node",
    "thumbnail": "assets/img/models/YOLO-FAS.gif",
	"added_date": "2025-09-26"
  },
  {
    "id": "MTKF",
    "title": "A unified object and keypoint detection framework for Personal Protective Equipment use",
    "authors": ["Bin Yang", "Hongru Xiao", "Binghan Zhang"],
    "abstract": "Accurately detecting whether workers wear Personal Protective Equipment (PPE) in real time plays an important role in safety management. Previous studies mainly used multiple models jointly or only object detection for wearing relationship judgments. This makes it difficult to provide real-time, accurate detection of security relationships. Therefore, this paper proposes safe-wearing detection rules and a novel multi-targets and keypoints detection framework (MTKF), which is capable of accomplishing multiple classes of targets and keypoints detection simultaneously in one-stage, to get more accurate results. In order to improve the performance in the PPE and worker keypoints detection in challenging construction scenes, the detection head transformation strategy, mix group shuffle attention (MGSA) module, and the improved dual and cross-class suppression algorithm (DC-NMS) are proposed. The experimental results are implemented on one established dataset (Joint dataset) and two public datasets (SHWD and COCO), which conduct a comprehensive evaluation in multiple dimensions. Compared to the baseline model, our method improves the mAP by 2.6%–7.1%, reduces the number of parameters by at least 70%, and is able to achieve an inference speed of 155 fps.",
    "year": 2024,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.dibe.2024.100559",
    "code_url": "https://github.com/TJSG337/MTKF",
    "thumbnail": "assets/img/models/MTKF.png",
	"added_date": "2025-09-26"
  },
  {
    "id": "SOC-YOLO",
    "title": "Small and overlapping worker detection at construction sites",
    "authors": ["Minsoo Park", "Dai Quoc Tran", "Jinyeong Bak", "Seunghee Park"],
    "abstract": "Although there has been study on worker detection using computer vision (CV) for the safety of construction sites, it is still challenging to identify employees who are obstructed or have poor vision. To solve these problems, we propose a method of small and overlapping target (worker) detection at a complex construction site named SOC-YOLO. The method is based on YOLOv5 and utilizes distance intersection over union (DIoU) non-maximum suppression (NMS), incorporating weighted triplet attention, expansion feature-level, and Soft-pool. Workers can be captured with overlap, particularly in large-scale construction sites, using the DIoU-based loss function, and NMS contributed to accuracy improvement. Next, we propose a weighted-triplet attention mechanism that can extract feature information from space more effectively and channel attention when learning object detection networks, using a simple average approach based on the same weight between the existing triplet attention. Next, we propose a model that adds additional predictive heads and residual connections to address the poor detection accuracy of workers photographed over long distances. A low-level feature map containing more information regarding small targets is used by extending the feature level. Finally, Softpool-spatial pyramid pooling fast (Softpool-SPPF) is proposed to solve the problem of inconsistent input image sizes. Softpool-SPPF performs an spatial pyramid pooling (SPP) function while preserving more functional information for accurate small target detection. Experiments were conducted using published worker detection datasets and handmade datasets, and the results showed increase from 81.26% to 84.63% average precision (AP) for small objects, from 67.52% to 73.88% mAP for minute objects, from 74.56% to77.57% for overlapping objects. The proposed method is expected to be useful for safety monitoring by applying it to the construction site worker tracking model.",
    "year": 2023,
    "modalities": ["Ground RGB"],
    "tasks": ["Object detection"],
	"applications": ["Safety monitoring"],
    "license": "GNU",
    "paper_url": "https://doi.org/10.1016/j.autcon.2023.104856",
    "code_url": "https://github.com/pms5343/SOC-YOLO",
    "thumbnail": "assets/img/models/SOC-YOLO.png",
	"added_date": "2025-09-25"
  },
  {
    "id": "SORD",
    "title": "Deep learning without human labeling for on-site rebar instance segmentation using synthetic BIM data and domain adaptation",
    "authors": ["Tsung-Wei Huang", "Yi-Hsiang Chen", "Jacob J. Lin", "Chuin-Shan Chen"],
    "abstract": "On-site rebar inspection is crucial for structural safety but remains labor-intensive and time-consuming. While deep learning presents a promising solution, existing research often relies on limited real-world labeled data. This paper introduces a framework to train a deep learning model for on-site rebar instance segmentation without human labeling. Synthetic data are generated from BIM models, creating a Synthetic On-site Rebar Dataset (SORD) with 25,287 labeled images. Domain adaptation is incorporated to bridge the gap between synthetic and real-world non-labeled data. This approach eliminates the need for human labeling. It significantly enhances model performance, achieving a threefold improvement in Average Precision (AP) metrics compared to models trained on limited real-world data. Additionally, the proposed method demonstrates superior performance across various on-site rebar images collected online, underscoring its generalizability and practical applications.",
    "year": 2025,
    "modalities": ["Ground RGB"],
    "tasks": ["Semantic segmentation"],
	"applications": ["Site understanding"],
    "license": "Not specified",
    "paper_url": "https://doi.org/10.1016/j.autcon.2024.105953",
    "code_url": "https://github.com/HuangBugWei/DA-MaskRCNN",
    "thumbnail": "assets/img/models/SORD.png",
	"added_date": "2025-09-26"
  }
]
