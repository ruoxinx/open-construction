<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenConstruction — Updates</title>
    <link>https://www.openconstruction.org/</link>
    <description>Newest datasets and models from OpenConstruction</description>
    <language>en-US</language>

  <item>
    <title><![CDATA[Model: Damage-augmented digital twins towards the automated inspection of buildings]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:dadt-buildings-model:2025-11-14</guid>
    <pubDate>Fri, 14 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Current procedures for the rapid inspection of buildings and infrastructure are subjective, time-consuming, and cumbersome to document, necessitating new technologies to automate the process and eliminate these shortcomings. Fortunately, recent developments in imaging devices and artificial intelligence, such as computer vision, provide the necessary tools for this, though they are not yet integrated into infrastructure applications. In this paper, we propose an end-to-end pipeline that generates damage-augmented digital twins for buildings at LOD3, including geometrical information as well as data pertaining to damage condition and its characterization. Our framework incorporates multiple-view images to (1) create a level of detail model, (2) segment damage information, and (3) characterize damage. The core of the method is the structure from motion, which is used to reconstruct the building scene, and machine-learning models that segment and characterize damage. In contrast to current practices, our method does not require manual intervention, generates lightweight models, and can be applied to a wide range of assets. The results generated with our pipeline represent a significant step towards an automated infrastructure damage assessment. We intend to expand our work in the future to include real-time applications and applications to other types of infrastructure. Codes and data sets are publicly available (https://github.com/eesd-epfl/DADT_buildings and https://doi.org/10.5281/zenodo.7767478).]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Dataset for generating LOD3 building models from SfM and semantic segmentation]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=Opening-Segmentation</link>
    <guid isPermaLink="false">datasets:Opening-Segmentation:2025-11-13</guid>
    <pubDate>Thu, 13 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: Generating LOD3 building models from structure-from-motion and semantic segmentation]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:LOD3:2025-11-13</guid>
    <pubDate>Thu, 13 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: Integrating extended reality and AI-based damage segmentation for near real-time, traceable bridge inspections]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:DamageSegmentationXR:2025-11-13</guid>
    <pubDate>Thu, 13 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[This paper presents a framework for interactive and traceable visual bridge inspections by integrating AI-driven image processing models with extended reality. The framework addresses the subjectivity, labor intensity, and documentation challenges of traditional visual inspections. It employs YOLO11-seg models on a mixed reality device to perform multi-class damage instance detection and segmentation in near real-time, identifying cracks, spalling, rust, efflorescence, and exposed rebar. Unlike existing methods, this approach supports automated operation and local processing without cloud computing reliance. AI predictions are embedded in the 3D extended reality environment, enhancing efficiency and accuracy while enabling inspectors to visualize and interact with damage data, improving decision-making and traceability throughout the bridge life-cycle. Beyond model’s accuracy, inference time was evaluated to verify near real-time feasibility. Tests in three real-world scenarios demonstrated practical applicability. Future work will incorporate damage characterization and decision-support tools to advance digital inspection practices and foster efficient, resilient bridge monitoring.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: CAMHighways: The Cambridge Highways Dataset]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=camhighways</link>
    <guid isPermaLink="false">datasets:camhighways:2025-11-11</guid>
    <pubDate>Tue, 11 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: TLSynth: A Novel Blender Add-On for Real-Time Point Cloud Generation from 3D Models]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:TLSynth:2025-11-11</guid>
    <pubDate>Tue, 11 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Point clouds are a crucial element in the process of scanning and reconstructing 3D environments, such as buildings or heritage sites. They allow for the creation of 3D models that can be used in a wide range of applications. In some cases, however, only the 3D model of an environment is available, and it is necessary to obtain point clouds with the same characteristics as those captured by a laser scanner. For instance, point clouds may be required for surveys, performance optimization, site scan planning, or validation of point cloud processing algorithms. This paper presents a new terrestrial laser scanner (TLS) simulator, designed as a Blender add-on, that produces synthetic point clouds from 3D models in real time. The simulator allows users to adjust a set of parameters to replicate real-world scanning conditions, such as noise generation, ensuring the synthetic point clouds closely mirror those produced by actual laser scanners. The target meshes may be derived from either a real-world scan or 3D designs created using design software. By replicating the spatial distributions and attributes of real laser scanner outputs and supporting real-time generation, the simulator serves as a valuable tool for scan planning and the development of synthetic point cloud repositories, advancing research and practical applications in 3D computer vision.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: CODE-ACCORD: Automated Compliance Checks for Construction, Renovation or Demolition Works (ACCORD)]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=code-accord</link>
    <guid isPermaLink="false">datasets:code-accord:2025-11-10</guid>
    <pubDate>Mon, 10 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: CubiCasa5K: A Dataset and an Improved Multi-Task Model for Floorplan Image Analysis]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=cubicasa5k</link>
    <guid isPermaLink="false">datasets:cubicasa5k:2025-11-10</guid>
    <pubDate>Mon, 10 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Concrete crack dataset for beam and column]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=concrete-crack-lab</link>
    <guid isPermaLink="false">datasets:concrete-crack-lab:2025-11-7</guid>
    <pubDate>Fri, 07 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: BIMNet: Dataset and benchmark for as-built BIM reconstruction from real-world point cloud]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=BIMNet</link>
    <guid isPermaLink="false">datasets:BIMNet:2025-11-6</guid>
    <pubDate>Thu, 06 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>
  </channel>
</rss>
