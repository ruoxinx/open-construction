<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenConstruction — Updates</title>
    <link>https://www.openconstruction.org/</link>
    <description>Newest datasets and models from OpenConstruction</description>
    <language>en-US</language>

  <item>
    <title><![CDATA[Model: Text2BIM: Generating Building Models Using a Large Language Model-Based Multiagent Framework]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:Text2BIM:2026-01-10</guid>
    <pubDate>Sat, 10 Jan 2026 00:00:00 GMT</pubDate>
    <description><![CDATA[The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (architecture, engineering, and construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multiagent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool’s APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that the proposed approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. The code is open source.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: BIMgent: Towards Autonomous Building Modeling via Computer-use Agents]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:BIMgent:2026-01-10</guid>
    <pubDate>Sat, 10 Jan 2026 00:00:00 GMT</pubDate>
    <description><![CDATA[Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios. Code available at: https://github.com/ZihanDDD/BIMgent]]></description>
  </item>

  <item>
    <title><![CDATA[Model: Predictive modeling: BIM command recommendation based on large-scale usage logs]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:BIM-Command-Recommendation:2026-01-10</guid>
    <pubDate>Sat, 10 Jan 2026 00:00:00 GMT</pubDate>
    <description><![CDATA[The adoption of Building Information Modeling (BIM) and model-based design within the Architecture, Engineering, and Construction (AEC) industry has been hindered by the perception that using BIM authoring tools demands more effort than conventional 2D drafting. To enhance design efficiency, this paper proposes a BIM command recommendation framework that predicts the optimal next actions in real-time based on users’ historical interactions. We propose a comprehensive filtering and enhancement method for large-scale raw BIM log data and introduce a novel command recommendation model. Our model builds upon the state-of-the-art Transformer backbones originally developed for large language models (LLMs), incorporating a custom feature fusion module, dedicated loss function, and targeted learning strategy. In a case study, the proposed method is applied to over 32 billion rows of real-world log data collected globally from the BIM authoring software Vectorworks. Experimental results demonstrate that our method can learn universal and generalizable modeling patterns from anonymous user interaction sequences across different countries, disciplines, and projects. When generating recommendations for the next command, our approach achieves a Recall@10 of approximately 84%. The code is available at: https://github.com/dcy0577/BIM-Command-Recommendation.git.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: VoxCity: Grid-based semantic 3D city model generation, and urban environment simulation]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:VoxCity:2026-01-07</guid>
    <pubDate>Wed, 07 Jan 2026 00:00:00 GMT</pubDate>
    <description><![CDATA[Three-dimensional urban environment simulation is a powerful tool for informed urban planning. However, the intensive manual effort required to prepare input 3D city models has hindered its widespread adoption. To address this challenge, we present VoxCity, an open-source Python package that provides a one-stop solution for grid-based 3D city model generation and urban environment simulation for cities worldwide. VoxCity’s ‘generator’ subpackage automatically downloads building heights, tree canopy heights, land cover, and terrain elevation within a specified target area, and voxelizes buildings, trees, land cover, and terrain to generate an integrated voxel city model. The ‘simulator’ subpackage enables users to conduct environmental simulations, including solar radiation and view index analyses. Users can export the generated models using several file formats compatible with external software, such as ENVI-met (INX), Blender, and Rhino (OBJ). We generated 3D city models for eight global cities, and demonstrated the calculation of solar irradiance, sky view index, and green view index. We also showcased microclimate simulation and 3D rendering visualization through ENVI-met and Rhino, respectively, through the file export function. Additionally, we reviewed openly available geospatial data to create guidelines to help users choose appropriate data sources depending on their target areas and purposes. VoxCity can significantly reduce the effort and time required for 3D city model preparation and promote the utilization of urban environment simulations. This contributes to more informed urban and architectural design that considers environmental impacts, and in turn, fosters sustainable and livable cities. VoxCity is released openly at https://github.com/kunifujiwara/VoxCity]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: AEC Objects in IFC-based BIM Dataset]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=AEC_Objects_in_IFC-based_BIM</link>
    <guid isPermaLink="false">datasets:AEC_Objects_in_IFC-based_BIM:2025-12-30</guid>
    <pubDate>Tue, 30 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Indoor FireRescue Rada (IFR) dataset]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=IFR_Dataset</link>
    <guid isPermaLink="false">datasets:IFR_Dataset:2025-12-29</guid>
    <pubDate>Mon, 29 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: DUSt3R: Geometric 3D Vision Made Easy]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:DUSt3R:2025-12-09</guid>
    <pubDate>Tue, 09 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: EnergyPlus-MCP: A model-context-protocol server for AI-driven building energy modeling]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:EnergyPlus-MCP:2025-12-08</guid>
    <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Traditional building energy modeling with the EnergyPlus building performance simulation engine requires domain expertise, programming skills, and intensive manual efforts limiting its effective adoption. This paper introduces EnergyPlus-MCP, the first open-source Model Context Protocol (MCP) server specifically designed for EnergyPlus simulation workflows, establishing a new foundational infrastructure for AI-driven building energy modeling. The MCP server implements a layered architecture with 35 specialized tools spanning model management, editing and analysis, HVAC and other systems configuration inspection, and simulation execution, enabling Large Language Models to interact with EnergyPlus through conversational interfaces. The server addresses critical workflow barriers by automating model validation, streamlining energy efficiency measures modification, and providing intelligent output management with interactive visualization. Through practical demonstrations using a multi-zone building retrofit analysis, we show how the EnergyPlus-MCP server significantly reduces manual efforts while maintaining full simulation rigor. By providing accessible natural language interfaces to sophisticated building energy analysis, this approach enables scalable deployment of simulation expertise across public and private organizations, educational institutions, and research teams, fundamentally transforming traditional building energy modeling practices.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: A KG-LLM-Enabled Inspection Work Packaging Approach in Module Prefabrication]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:KG-LLM-inspection-work-packaging:2025-12-05</guid>
    <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Quality inspection during the production of modular construction (MC) is critical to the success of MC projects because defects such as geometric errors can severely impact installation. Therefore, the effective organization and management of quality inspection is essential. Although work packaging has proven to be an effective method for project planning, automatically and accurately generating inspection work packages remains a challenge, particularly in mass production. To this end, this study aims to develop a knowledge graph-large language model (KG-LLM)-enabled inspection work packaging approach in MC production. First, a KG is constructed to represent inspection knowledge, encompassing inspection tasks, target MC products, inspectors, inspection tools, inspection frequencies, and corresponding production processes, which can enrich LLM in product2inspection mapping and can help identify features of inspection tasks for inspection2package clustering. Second, a prompt-based product2inspection mapping approach leveraging LLM is developed. Next, inspection2package clustering is completed through hierarchical agglomerative clustering, with parameters optimized by particle swarm optimization (PSO). Finally, the effectiveness of the proposed approach is validated through controlled experiments on an MC case project. This study demonstrates how leveraging KG and LLMs can enhance collaboration in quality inspection during module prefabrication and highlights the potential of LLMs in the construction domain.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: GlobalBuildingAtlas]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=GlobalBuildingAtlas</link>
    <guid isPermaLink="false">datasets:GlobalBuildingAtlas:2025-12-4</guid>
    <pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>
  </channel>
</rss>
