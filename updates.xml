<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenConstruction — Updates</title>
    <link>https://www.openconstruction.org/</link>
    <description>Newest datasets and models from OpenConstruction</description>
    <language>en-US</language>

  <item>
    <title><![CDATA[Model: A KG-LLM-Enabled Inspection Work Packaging Approach in Module Prefabrication]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:KG-LLM-inspection-work-packaging:2025-12-05</guid>
    <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Quality inspection during the production of modular construction (MC) is critical to the success of MC projects because defects such as geometric errors can severely impact installation. Therefore, the effective organization and management of quality inspection is essential. Although work packaging has proven to be an effective method for project planning, automatically and accurately generating inspection work packages remains a challenge, particularly in mass production. To this end, this study aims to develop a knowledge graph-large language model (KG-LLM)-enabled inspection work packaging approach in MC production. First, a KG is constructed to represent inspection knowledge, encompassing inspection tasks, target MC products, inspectors, inspection tools, inspection frequencies, and corresponding production processes, which can enrich LLM in product2inspection mapping and can help identify features of inspection tasks for inspection2package clustering. Second, a prompt-based product2inspection mapping approach leveraging LLM is developed. Next, inspection2package clustering is completed through hierarchical agglomerative clustering, with parameters optimized by particle swarm optimization (PSO). Finally, the effectiveness of the proposed approach is validated through controlled experiments on an MC case project. This study demonstrates how leveraging KG and LLMs can enhance collaboration in quality inspection during module prefabrication and highlights the potential of LLMs in the construction domain.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: GlobalBuildingAtlas]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=GlobalBuildingAtlas</link>
    <guid isPermaLink="false">datasets:GlobalBuildingAtlas:2025-12-4</guid>
    <pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Global dataset of critical infrastructure]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=CISI</link>
    <guid isPermaLink="false">datasets:CISI:2025-11-28</guid>
    <pubDate>Fri, 28 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: VideoCAD: Learning Long-Horizon 3D CAD UI Interactions from Video]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=VideoCAD</link>
    <guid isPermaLink="false">datasets:VideoCAD:2025-11-26</guid>
    <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Construction Management Systems (CMS) Domain Corpora]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=cms-domain-corpora</link>
    <guid isPermaLink="false">datasets:cms-domain-corpora:2025-11-26</guid>
    <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: VideoCADFormer: A Model for Learning Long-Horizon 3D CAD UI Interactions from Video]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:VideoCAD:2025-11-26</guid>
    <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt to model UI interactions for precision engineering tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order-of-magnitude increase in complexity for real-world engineering UI tasks, with time horizons up to 20x longer than those in other datasets. We show two important downstream applications of VideoCAD: (1) learning UI interactions from professional 3D CAD tools for precision tasks and (2) a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models (LLMs) on spatial reasoning and video understanding. To learn the UI interactions, we propose VideoCADFormer, a state-of-the-art model for learning CAD interactions directly from video, which outperforms existing behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Fire-ART: Firefighting asset recognition dataset]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=Fire-ART</link>
    <guid isPermaLink="false">datasets:Fire-ART:2025-11-22</guid>
    <pubDate>Sat, 22 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: VTT-ConIoT: IMU Dataset for Activity Recognition of Construction Workers]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=vtt-coniot</link>
    <guid isPermaLink="false">datasets:vtt-coniot:2025-11-18</guid>
    <pubDate>Tue, 18 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Dataset: Dataset for generation of LOD4 models for buildings]]></title>
    <link>https://www.openconstruction.org/datasets/detail.html?id=LOD4-Building-Image-Modeling</link>
    <guid isPermaLink="false">datasets:LOD4-Building-Image-Modeling:2025-11-17</guid>
    <pubDate>Mon, 17 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[New dataset added to OpenConstruction.]]></description>
  </item>

  <item>
    <title><![CDATA[Model: Automated image-based generation of finite element models for masonry buildings]]></title>
    <link>https://www.openconstruction.org/models.html</link>
    <guid isPermaLink="false">models:FEM_buildings:2025-11-17</guid>
    <pubDate>Mon, 17 Nov 2025 00:00:00 GMT</pubDate>
    <description><![CDATA[To predict the response of masonry buildings to various types of loads, engineers use finite element models, specifically solid-element and macro-element models. For predicting masonry responses to seismic events in particular, equivalent frame models—a subcategory of macro-element models—are a common choice because of their low computational cost. However, an existing bottleneck in modeling pipelines is generating the geometry of the model, which is currently a slow and laborious process that is done manually using computer-aided design tools. In this paper, we address this by automating the modelling process using recent advancements in computer vision and machine learning. We present an image-based end-to-end pipeline that automatically generates finite element meshes for solid-element and equivalent-frame models of the outer walls of free-standing historical masonry buildings. As the input, our framework requires RGB images of the buildings that are processed using structure-from-motion algorithms, which create 3D geometries, and convolutional neural networks, which segment the openings and their corners. These layers are then combined to generate level of detail models. We tested our pipeline on structures with irregular surface geometries and opening layouts. While generating the solid element mesh from the level of detail model is straightforward, generating equivalent frame models required algorithms for segmenting the façade and the meshing. Experts in the field analyzed the generated equivalent frame models and determined them to be useful for numerical modeling. These finite element geometries will be invaluable for future predictions of the seismic response of damaged and undamaged buildings. The codes and dataset are publicly available for future studies and benchmarking (https://github.com/eesd-epfl/FEM_buildings and https://doi.org/10.5281/zenodo.8094306).]]></description>
  </item>
  </channel>
</rss>
